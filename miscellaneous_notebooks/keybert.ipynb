{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keybert\n",
      "  Downloading keybert-0.8.5-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from keybert) (1.24.4)\n",
      "Requirement already satisfied: rich>=10.4.0 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from keybert) (13.9.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from keybert) (1.5.2)\n",
      "Requirement already satisfied: sentence-transformers>=0.3.8 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from keybert) (3.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from rich>=10.4.0->keybert) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from rich>=10.4.0->keybert) (2.18.0)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from rich>=10.4.0->keybert) (4.11.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from scikit-learn>=0.22.2->keybert) (3.5.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (4.46.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (2.5.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (0.24.6)\n",
      "Requirement already satisfied: Pillow in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from sentence-transformers>=0.3.8->keybert) (10.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2.32.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from tqdm->sentence-transformers>=0.3.8->keybert) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.20.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.3.8->keybert) (0.4.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.3.8->keybert) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers>=0.3.8->keybert) (2024.8.30)\n",
      "Downloading keybert-0.8.5-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: keybert\n",
      "Successfully installed keybert-0.8.5\n"
     ]
    }
   ],
   "source": [
    "#!pip install keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "from gensim.models import LdaModel\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "import pyLDAvis.gensim_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus combined successfully as a list of strings.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def combine_text_files_to_list(input_directory):\n",
    "\n",
    "    txt_files = [os.path.join(input_directory, file) for file in os.listdir(input_directory) if file.endswith(\".txt\")]\n",
    "    corpus = []\n",
    "\n",
    "    for txt_file in txt_files:\n",
    "        \n",
    "        try:\n",
    "            # Read the entire file as a string and add the string to the corpus\n",
    "            with open(txt_file, 'r', encoding='utf-8') as file:\n",
    "                file_content = file.read()  \n",
    "                corpus.append(file_content)  \n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while reading {txt_file}: {e}\")\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "modules = combine_text_files_to_list(\"../Dataset/Parsed_Slides\")\n",
    "print(\"Corpus combined successfully as a list of strings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "print(len(modules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_list_from_csv(path):\n",
    "    corpus = []\n",
    "    with open(path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            columns = line.split(',')   \n",
    "            # Columns 1 and 2 contain the company name and the job title, both guaranteed to not include commas, and both separated by a comma. \n",
    "            # We are not analyzing this information, so we can safely discard the first two columns.\n",
    "            # The third \"column\" contains the job description, but it may contain commas, so we use \",\".join() to concatenate all the columns after the second one.\n",
    "            # csv.reader()'s quotechar parameter does not seem to work for whatever reason, and this just seemed faster. \n",
    "            description = \",\".join(columns[2:]).strip('\"')      # strip('\"') to remove leading and trailing quotes\n",
    "            corpus.append(description)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_descriptions = create_list_from_csv('jobs.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262\n"
     ]
    }
   ],
   "source": [
    "print(len(job_descriptions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyBERT()   # Here we use the default embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('computers represent data', 0.6239), ('data memory', 0.6137), ('data computer encoded', 0.5903), ('technologies storing data', 0.5883), ('data computer', 0.5762)]\n",
      "[('analysis basics rstudio', 0.5192), ('write tests function', 0.4577), ('expect_error fahr_to_celsius', 0.4547), ('function make tests', 0.4541), ('error expect_error fahr_to_celsius', 0.4537)]\n",
      "[('structure traversal trees', 0.7225), ('data structure traversal', 0.705), ('tree data structure', 0.7004), ('data structures looked', 0.6892), ('data structures', 0.6858)]\n",
      "[('uc python testing', 0.6879), ('unittest python standard', 0.6413), ('unittest python', 0.6354), ('python testing', 0.6267), ('ubc python testing', 0.6235)]\n",
      "[('web scraping vs', 0.6499), ('web scrapers web', 0.6407), ('scraping explain web', 0.6383), ('web data', 0.636), ('web scraping understand', 0.6294)]\n",
      "[('sql declarative language', 0.6833), ('sql standard language', 0.6753), ('sql query languages', 0.666), ('language sql standard', 0.6473), ('sql standard ddl', 0.6414)]\n",
      "[('distributed version control', 0.703), ('development distributed git', 0.6789), ('distributed git', 0.6782), ('version control fundamentals', 0.6759), ('version control systems', 0.674)]\n",
      "[('missing values pandas', 0.61), ('pandas missing values', 0.6018), ('data pandas', 0.5818), ('data pandas ubco', 0.5481), ('methods pandas', 0.5448)]\n",
      "[('treatments randomized', 0.6319), ('randomize patients', 0.6243), ('experiments properly randomize', 0.6201), ('randomize properly experiments', 0.6109), ('randomization experimental', 0.6087)]\n",
      "[('interactive dash table', 0.5825), ('dash data tables', 0.571), ('dash tables', 0.5669), ('create dash tables', 0.5582), ('dash table components', 0.5477)]\n",
      "[('communication understanding rhetorical', 0.6145), ('writing diagnostic rhetorical', 0.5473), ('understanding rhetorical situation', 0.5389), ('communication writing assignments', 0.5381), ('rhetorical situation ubco', 0.5246)]\n",
      "[('definitions privacy', 0.7974), ('privacy rights', 0.7758), ('right privacy constitutes', 0.7655), ('privacy right privacy', 0.7573), ('privacy general definition', 0.7533)]\n",
      "[('linearity weight vs', 0.5395), ('moving linearity weight', 0.5365), ('vs height regression', 0.5166), ('linearity weight', 0.506), ('linearity weight bo', 0.502)]\n",
      "[('decision trees carts', 0.6363), ('decisions trees carts', 0.6163), ('motivation decision trees', 0.5904), ('trees carts partition', 0.5633), ('motivating example classificationx2', 0.5607)]\n",
      "[('supervised learning 2023w2', 0.5232), ('classified supervised', 0.5158), ('figure classification', 0.4977), ('classification discriminant analysis', 0.4847), ('supervised learning support', 0.4827)]\n",
      "[('negative matrix factorization', 0.6514), ('matrix factorization ubco', 0.5935), ('matrix factorization nmf', 0.5743), ('non negative pca', 0.5573), ('factor analysis pca', 0.5534)]\n",
      "[('marginal densities propane', 0.4954), ('densities propane example', 0.4556), ('interactions propane gas', 0.4334), ('densities propane', 0.4293), ('expectations sums example', 0.4227)]\n",
      "[('uses markov chains', 0.6889), ('movement markov chain', 0.6643), ('markov chains', 0.661), ('markov chain useful', 0.6542), ('markov chain simulation', 0.6497)]\n",
      "[('beta binomial bayesian', 0.6124), ('theta model binomial', 0.5972), ('likelihood posterior dθ', 0.5887), ('beta binomial posterior', 0.5783), ('binomial model posterior', 0.5766)]\n",
      "[('models use splines', 0.5581), ('glms generalized linear', 0.5552), ('regression splines', 0.5514), ('model generalized linear', 0.5482), ('generalized linear model', 0.5464)]\n",
      "[('descent direction computational', 0.544), ('gradient descent algorithms', 0.53), ('descent direction gradient', 0.5284), ('direction gradient descent', 0.5234), ('gradients algorithms', 0.5214)]\n",
      "[('networks lenet cnn', 0.7286), ('lenet cnn', 0.7269), ('lenet cnn used', 0.6946), ('cnn trained imagenet', 0.6682), ('training cnns', 0.6535)]\n",
      "[('cassandra data management', 0.7068), ('interacting cassandra databases', 0.7065), ('structure cassandra', 0.6813), ('design cassandra', 0.68), ('cassandra ubc concepts', 0.674)]\n"
     ]
    }
   ],
   "source": [
    "for module in modules:\n",
    "    keywords = model.extract_keywords(module, keyphrase_ngram_range=(1, 3), stop_words='english')\n",
    "    print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'databases', 'applying', 'microsoft', 'etls', 'developer', 'ideas', 'hdfs', 'mentor', 'insights', 'phd', 'agent', 'database', 'collaborate', 'engineering', 'supervised', 'debian', 'biologics', 'hospitality', 'linux', 'architecture', 'ml', 'visualizations', 'marketing', 'technologies', 'dashboards', 'pyspark', 'analyses', 'stakeholder', 'responsibilities', 'enterprise', 'sap', 'pipelines', 'gcp', 'crimes', 'gpu', 'administrative', 'healthcare', 'cuda', '5g', 'developing', 'schemas', 'advancements', 'vertexai', 'cloud', 'llms', 'technologist', 'data', 'databricks', 'affirm', 'arima', 'workflow', 'skills', 'forecasting', 'programming', 'gaming', 'collaborators', 'learning', 'internship', 'interns', 'software', 'bottlenecks', 'automated', 'sports', 'generative', 'acquisition', 'pricing', 'iot', 'tensorflow', 'productionizing', 'scrum', 'benchsci', 'communications', 'clio', 'amd', 'chatbots', 'researcher', 'prerequisite', 'development', 'spark', 'qualification', 'project', 'managing', 'mlflow', 'industry', 'protein', 'deployment', 'boosting', 'conversational', 'consultants', 'monitoring', 'innovate', 'backend', 'snow', 'mathematics', 'products', 'job', 'mlops', 'datavault', 'redis', 'bigquery', 'datascience', 'architectures', 'autonomy', 'schema', 'predict', 'cloudera', 'modelops', 'backtesting', 'intellectual', 'leveraging', 'laundering', 'technological', 'graduate', 'anddataanalytics', 'build', 'sqlserver', 'stakeholders', 'frameworks', 'excel', 'market', 'qualifications', 'chatbot', 'dbt', 'antibody', 'collaborating', 'mining', 'ai', 'devops', 'business', 'medeloop', 'nlp', 'mapreduce', 'optimize', 'hiveql', 'sql', 'crime', 'infrastructure', 'pipeline', 'features', 'mentorship', 'oracle', 'scalable', 'git', 'emr', 'feature', 'skillsets', 'fraud', 'sparksql', 'internships', 'recommender', 'genai', 'postgresql', 'pursuing', 'financial', 'microservice', 'gitlab', 'resume', 'academic', 'competencies', 'epson', 'reports', 'roadmap', 'embodied', 'cresta', 'snowflake', 'publications', 'catalog', 'wireless', 'documentation', 'tech', 'modeling', 'exploring', 'training', 'developers', 'bi', 'mentoring', 'automation', 'learningtechniques', 'ec2', 'experiences', 'agile', 'architectural', 'roles', 'django', 'loan', 'bioinformatics', 'azure', 'models', 'manulife', 'lending', 'fico', 'speech', 'terraform', 'programs', 'cv', 'cloudformation', 'gdpr', 'tableau', 'analyst', 'preparingdatato', 'ondeck', 'doctoral', 'onboarding', 'asr', 'thri5', 'programmer', 'hadoop', 'productionalize', 'azureml', 'datasets', 'analyze', 'clients', 'vault', 'attributes', 'openai', 'instrumentation', 'tiktok', 'tts', 'ecobee', 'virtualized', 'vision', 'processing', 'mbb', 'consulting', 'cybersecurity', 'aiml', 'etl', 'hive', 'agents', 'analyzing', 'skilled', 'requirements', 'aws', 'experience', 'driver', 'bachelors', 'degree', 'interdisciplinary', 'dataflow', 'projects', 'capabilities', 'github', 'services', 'advertising', 'serverless', 'modelling', 'tools', 'experiencemaster', 'leak', 'rdbms', '3d', 'coinbase', 'robotics', 'expertise', 'kubernetes', 'virtualization', 'reporting', 'scala', 'deploying', 'transactional', 'proficiency', 'workflows', 'progeniter', 'dynamodb', 'knowledge', 'engineer', 'analytics', 'trained', 'proficient', 'strategies', 'footage', 'lake', 'role', 'icc', 'engineers', 'python', 'entrepreneurial', 'sales', 'masters', 'develop', 'automate', 'certification', 'businesses', 'consumer', 'roadmaps', 'nosql', 'analysts'}\n"
     ]
    }
   ],
   "source": [
    "jobs_dataset_keywords = set()\n",
    "\n",
    "for desc in job_descriptions:\n",
    "    job_keywords = []\n",
    "    job_keywords_tuple = model.extract_keywords(desc)\n",
    "    for job_keyword in job_keywords_tuple:\n",
    "        job_keywords.append(job_keyword[0])\n",
    "\n",
    "    job_keywords = set(job_keywords)\n",
    "    jobs_dataset_keywords = jobs_dataset_keywords.union(job_keywords)\n",
    "\n",
    "print(jobs_dataset_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "287\n",
      "{'3d',\n",
      " '5g',\n",
      " 'academic',\n",
      " 'acquisition',\n",
      " 'administrative',\n",
      " 'advancements',\n",
      " 'advertising',\n",
      " 'affirm',\n",
      " 'agent',\n",
      " 'agents',\n",
      " 'agile',\n",
      " 'ai',\n",
      " 'aiml',\n",
      " 'amd',\n",
      " 'analyses',\n",
      " 'analyst',\n",
      " 'analysts',\n",
      " 'analytics',\n",
      " 'analyze',\n",
      " 'analyzing',\n",
      " 'anddataanalytics',\n",
      " 'antibody',\n",
      " 'applying',\n",
      " 'architectural',\n",
      " 'architecture',\n",
      " 'architectures',\n",
      " 'arima',\n",
      " 'asr',\n",
      " 'attributes',\n",
      " 'automate',\n",
      " 'automated',\n",
      " 'automation',\n",
      " 'autonomy',\n",
      " 'aws',\n",
      " 'azure',\n",
      " 'azureml',\n",
      " 'bachelors',\n",
      " 'backend',\n",
      " 'backtesting',\n",
      " 'benchsci',\n",
      " 'bi',\n",
      " 'bigquery',\n",
      " 'bioinformatics',\n",
      " 'biologics',\n",
      " 'boosting',\n",
      " 'bottlenecks',\n",
      " 'build',\n",
      " 'business',\n",
      " 'businesses',\n",
      " 'capabilities',\n",
      " 'catalog',\n",
      " 'certification',\n",
      " 'chatbot',\n",
      " 'chatbots',\n",
      " 'clients',\n",
      " 'clio',\n",
      " 'cloud',\n",
      " 'cloudera',\n",
      " 'cloudformation',\n",
      " 'coinbase',\n",
      " 'collaborate',\n",
      " 'collaborating',\n",
      " 'collaborators',\n",
      " 'communications',\n",
      " 'competencies',\n",
      " 'consultants',\n",
      " 'consulting',\n",
      " 'consumer',\n",
      " 'conversational',\n",
      " 'cresta',\n",
      " 'crime',\n",
      " 'crimes',\n",
      " 'cuda',\n",
      " 'cv',\n",
      " 'cybersecurity',\n",
      " 'dashboards',\n",
      " 'data',\n",
      " 'database',\n",
      " 'databases',\n",
      " 'databricks',\n",
      " 'dataflow',\n",
      " 'datascience',\n",
      " 'datasets',\n",
      " 'datavault',\n",
      " 'dbt',\n",
      " 'debian',\n",
      " 'degree',\n",
      " 'deploying',\n",
      " 'deployment',\n",
      " 'develop',\n",
      " 'developer',\n",
      " 'developers',\n",
      " 'developing',\n",
      " 'development',\n",
      " 'devops',\n",
      " 'django',\n",
      " 'doctoral',\n",
      " 'documentation',\n",
      " 'driver',\n",
      " 'dynamodb',\n",
      " 'ec2',\n",
      " 'ecobee',\n",
      " 'embodied',\n",
      " 'emr',\n",
      " 'engineer',\n",
      " 'engineering',\n",
      " 'engineers',\n",
      " 'enterprise',\n",
      " 'entrepreneurial',\n",
      " 'epson',\n",
      " 'etl',\n",
      " 'etls',\n",
      " 'excel',\n",
      " 'experience',\n",
      " 'experiencemaster',\n",
      " 'experiences',\n",
      " 'expertise',\n",
      " 'exploring',\n",
      " 'feature',\n",
      " 'features',\n",
      " 'fico',\n",
      " 'financial',\n",
      " 'footage',\n",
      " 'forecasting',\n",
      " 'frameworks',\n",
      " 'fraud',\n",
      " 'gaming',\n",
      " 'gcp',\n",
      " 'gdpr',\n",
      " 'genai',\n",
      " 'generative',\n",
      " 'git',\n",
      " 'github',\n",
      " 'gitlab',\n",
      " 'gpu',\n",
      " 'graduate',\n",
      " 'hadoop',\n",
      " 'hdfs',\n",
      " 'healthcare',\n",
      " 'hive',\n",
      " 'hiveql',\n",
      " 'hospitality',\n",
      " 'icc',\n",
      " 'ideas',\n",
      " 'industry',\n",
      " 'infrastructure',\n",
      " 'innovate',\n",
      " 'insights',\n",
      " 'instrumentation',\n",
      " 'intellectual',\n",
      " 'interdisciplinary',\n",
      " 'interns',\n",
      " 'internship',\n",
      " 'internships',\n",
      " 'iot',\n",
      " 'job',\n",
      " 'knowledge',\n",
      " 'kubernetes',\n",
      " 'lake',\n",
      " 'laundering',\n",
      " 'leak',\n",
      " 'learning',\n",
      " 'learningtechniques',\n",
      " 'lending',\n",
      " 'leveraging',\n",
      " 'linux',\n",
      " 'llms',\n",
      " 'loan',\n",
      " 'managing',\n",
      " 'manulife',\n",
      " 'mapreduce',\n",
      " 'market',\n",
      " 'marketing',\n",
      " 'masters',\n",
      " 'mathematics',\n",
      " 'mbb',\n",
      " 'medeloop',\n",
      " 'mentor',\n",
      " 'mentoring',\n",
      " 'mentorship',\n",
      " 'microservice',\n",
      " 'microsoft',\n",
      " 'mining',\n",
      " 'ml',\n",
      " 'mlflow',\n",
      " 'mlops',\n",
      " 'modeling',\n",
      " 'modelling',\n",
      " 'modelops',\n",
      " 'models',\n",
      " 'monitoring',\n",
      " 'nlp',\n",
      " 'nosql',\n",
      " 'onboarding',\n",
      " 'ondeck',\n",
      " 'openai',\n",
      " 'optimize',\n",
      " 'oracle',\n",
      " 'phd',\n",
      " 'pipeline',\n",
      " 'pipelines',\n",
      " 'postgresql',\n",
      " 'predict',\n",
      " 'preparingdatato',\n",
      " 'prerequisite',\n",
      " 'pricing',\n",
      " 'processing',\n",
      " 'productionalize',\n",
      " 'productionizing',\n",
      " 'products',\n",
      " 'proficiency',\n",
      " 'proficient',\n",
      " 'progeniter',\n",
      " 'programmer',\n",
      " 'programming',\n",
      " 'programs',\n",
      " 'project',\n",
      " 'projects',\n",
      " 'protein',\n",
      " 'publications',\n",
      " 'pursuing',\n",
      " 'pyspark',\n",
      " 'python',\n",
      " 'qualification',\n",
      " 'qualifications',\n",
      " 'rdbms',\n",
      " 'recommender',\n",
      " 'redis',\n",
      " 'reporting',\n",
      " 'reports',\n",
      " 'requirements',\n",
      " 'researcher',\n",
      " 'responsibilities',\n",
      " 'resume',\n",
      " 'roadmap',\n",
      " 'roadmaps',\n",
      " 'robotics',\n",
      " 'role',\n",
      " 'roles',\n",
      " 'sales',\n",
      " 'sap',\n",
      " 'scala',\n",
      " 'scalable',\n",
      " 'schema',\n",
      " 'schemas',\n",
      " 'scrum',\n",
      " 'serverless',\n",
      " 'services',\n",
      " 'skilled',\n",
      " 'skills',\n",
      " 'skillsets',\n",
      " 'snow',\n",
      " 'snowflake',\n",
      " 'software',\n",
      " 'spark',\n",
      " 'sparksql',\n",
      " 'speech',\n",
      " 'sports',\n",
      " 'sql',\n",
      " 'sqlserver',\n",
      " 'stakeholder',\n",
      " 'stakeholders',\n",
      " 'strategies',\n",
      " 'supervised',\n",
      " 'tableau',\n",
      " 'tech',\n",
      " 'technological',\n",
      " 'technologies',\n",
      " 'technologist',\n",
      " 'tensorflow',\n",
      " 'terraform',\n",
      " 'thri5',\n",
      " 'tiktok',\n",
      " 'tools',\n",
      " 'trained',\n",
      " 'training',\n",
      " 'transactional',\n",
      " 'tts',\n",
      " 'vault',\n",
      " 'vertexai',\n",
      " 'virtualization',\n",
      " 'virtualized',\n",
      " 'vision',\n",
      " 'visualizations',\n",
      " 'wireless',\n",
      " 'workflow',\n",
      " 'workflows'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "print(len(jobs_dataset_keywords))\n",
    "pprint(jobs_dataset_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_keywords = [\"agile\", \"architecture\", \"arima\", \"aws\", \"azure\", \"bachelors\", \"bigquery\", \"chatbots\", \"cloud\", \"collaborate\", \"dashboards\", \"database\", \"databricks\", \"cuda\", \"deploying\", \"development\", \"devops\", \"documentation\", \"ec2\", \"entrepreneurial\", \"etl\", \"excel\", \"experience\", \"forecasting\", \"gcp\", \"generative\", \"git\", \"gpu\", \"hadoop\", \"hive\", \"hiveql\", \"infrastructure\", \"insights\", \"kubernetes\", \"learning\", \"linux\", \"llms\", \"managing\", \"mapreduce\", \"masters\", \"mathematics\", \"mining\", \"ml\", \"mlops\", \"models\", \"modeling\", \"nlp\", \"nosql\", \"optimize\", \"phd\", \"pipelines\", \"postgresql\", \"predict\", \"programming\", \"python\" ,\"pyspark\", \"rdbms\", \"redis\", \"reports\", \"scala\", \"scalable\", \"schema\", \"scrum\", \"snowflake\", \"software\", \"spark\", \"sql\", \"sqlserver\", \"stakeholders\", \"supervised\", \"tableau\", \"tensorflow\", \"terraform\", \"training\", \"virtualization\", \"workflow\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    }
   ],
   "source": [
    "print(len(important_keywords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try finding these keywords in the lectures corpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dir-st",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
