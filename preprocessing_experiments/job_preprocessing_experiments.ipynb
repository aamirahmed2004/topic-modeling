{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to tokenize the text; i.e. create a dictionary mapping words to integers. The dictionary can be used to create a term-document matrix.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.phrases import ENGLISH_CONNECTOR_WORDS\n",
    "\n",
    "import spacy\n",
    "from textacy import extract\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "def create_list_from_csv(path):\n",
    "    corpus = []\n",
    "    with open(path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            columns = line.split(',')   \n",
    "            # Columns 1 and 2 contain the company name and the job title, both guaranteed to not include commas, and both separated by a comma. \n",
    "            # We are not analyzing this information, so we can safely discard the first two columns.\n",
    "            # The third \"column\" contains the job description, but it may contain commas, so we use \",\".join() to concatenate all the columns after the second one.\n",
    "            # csv.reader()'s quotechar parameter does not seem to work for whatever reason, and this just seemed faster. \n",
    "            description = \",\".join(columns[2:]).strip('\"')      # strip('\"') to remove leading and trailing quotes\n",
    "            corpus.append(description)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_list_from_csv('../jobs.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262\n",
      "2051\n",
      " \"What You'll Do Apply statistical and machine learning techniques to process and analyze unstructured textual data Develop and finetune machine learning models for tasks such as entity recognition, c\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(len(data[-1]))\n",
    "print(data[-1][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])  \n",
    "\n",
    "# Add custom stop words \n",
    "nlp.Defaults.stop_words |= {\"experience\", \"preferred\", \"skill\", \"yelp\", \"strong\", \"work\", \"solutions\", \"drive\", \"insights\", \"use\", \"needs\", \"responsibilities\", \"do\", \"particularly\", \"related\", \"leak\", \"radio\", }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_without_ngrams(doc):\n",
    "\n",
    "    spacy_doc = nlp(doc.lower())\n",
    "\n",
    "    # Remove stopwords, punctuation, and numeric tokens\n",
    "    tokens = [\n",
    "        token.text \n",
    "        for token in spacy_doc \n",
    "        if not token.is_stop and not token.is_punct and not token.is_digit and token.is_alpha       # Keep only words that are not stop words\n",
    "            and token.text not in [\"_\", \"+\", \"=\", \"\\n\",\"-\",\"*\",\"<\",\">\"]                             # Remove special characters       \n",
    "            and not len(token.text) == 1                                                            # Remove single character words\n",
    "            # and token.pos_ in [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]                                        # Keep only nouns, adjectives, verbs, and adverbs\n",
    "    ]    \n",
    "                                                                           \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    tokens = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        sent_tokens = []\n",
    "        for token in doc: \n",
    "            if \"_\" in token.text:\n",
    "                sent_tokens.append(token.text)\n",
    "            else:\n",
    "                if token.pos_ in allowed_postags:\n",
    "                    sent_tokens.append(token.lemma_)\n",
    "                    \n",
    "        sent_tokens = [token.replace(\"datum\", \"data\") for token in sent_tokens]\n",
    "        tokens.append(sent_tokens)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = [clean_without_ngrams(doc) for doc in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phrases(data_words, min_count=10, threshold=20) \n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = Phraser(bigram)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "data_words_bigrams = make_bigrams(data_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized = lemmatize(data_words_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the cleaned corpus: 36738\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for doc in data_lemmatized:\n",
    "    sum += len(doc)\n",
    "\n",
    "print(f\"Total number of words in the cleaned corpus: {sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262\n",
      "['data', 'collection', 'cleaning', 'assist', 'collection', 'organization', 'data', 'data', 'clean', 'preprocessing', 'activity', 'ensure', 'data', 'accuracy', 'data', 'analysis', 'utilize', 'statistical', 'method', 'tool', 'familiarity', 'data', 'analysis', 'tool', 'power_bi', 'excel', 'attention_detail', 'analytical', 'skill', 'good', 'communication_skills', 'ability', 'collaboratively', 'team', 'eagerness', 'learn', 'adapt', 'new', 'technology', 'technique']\n"
     ]
    }
   ],
   "source": [
    "print(len(data_lemmatized))\n",
    "print(data_lemmatized[0][:20] + data_lemmatized[0][-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = []\n",
    "\n",
    "for data_lemmatized_doc in data_lemmatized:    \n",
    "    for word in data_lemmatized_doc:\n",
    "        if \"_\" in word:\n",
    "            bigrams.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ngrams: 3008\n",
      "Number of unique ngrams: 101\n",
      "{'tensorflow_pytorch', 'industry_trends', 'open_source', 'product_managers', 'state_art', 'interpersonal_skills', 'apache_spark', 'communication_skills', 'deep_learning', 'comprehensive_documentation', 'continuous_integration', 'verbal_communication', 'best_practices', 'functional_teams', 'relational_databases', 'etl_elt', 'generative_ai', 'end_end', 'bachelor_master', 'updated_latest', 'computer_science', 'excellent_communication', 'high_quality', 'collect_clean', 'cloud_platforms', 'attention_detail', 'large_scale', 'qualifications_bachelor', 'collaborate_cross', 'stay_date', 'problem_solving', 'google_cloud', 'develop_maintain', 'decision_making', 'production_environments', 'excellent_written', 'non_technical', 'natural_language', 'solving_skills', 'mathematics_physics', 'apache_airflow', 'fine_tuning', 'gcp_services', 'docker_kubernetes', 'required_qualifications', 'degree_computer', 'solve_complex', 'cloud_composer', 'programming_language', 'computer_vision', 'power_bi', 'sql_nosql', 'pandas_numpy', 'date_latest', 'proven_track', 'feature_cases', 'fast_paced', 'minimum_years', 'distributed_systems', 'team_members', 'quantitative_field', 'scalability_reliability', 'etl_processes', 'trends_patterns', 'programming_languages', 'statistics_mathematics', 'solid_understanding', 'production_grade', 'machine_learning', 'real_world', 'cloud_based', 'scikit_learn', 'ci_cd', 'real_time', 'version_control', 'platforms_aws', 'cutting_edge', 'latest_advancements', 'artificial_intelligence', 'excellent_problem', 'fine_tune', 'tools_like', 'stay_updated', 'written_verbal', 'track_record', 'design_implementation', 'vertex_ai', 'team_player', 'pytorch_tensorflow', 'time_series', 'master_degree', 'bachelor_degree', 'ability_independently', 'code_reviews', 'large_datasets', 'knowledge_sharing', 'design_implement', 'model_training', 'designing_implementing', 'nosql_databases', 'cross_functional'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of ngrams: {len(bigrams)}\")\n",
    "unique_bigrams = set(bigrams)\n",
    "print(f\"Number of unique ngrams: {len(unique_bigrams)}\")\n",
    "print(unique_bigrams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dir-st",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
