{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Used to tokenize the text; i.e. create a dictionary mapping words to integers. The dictionary can be used to create a term-document matrix.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models.phrases import ENGLISH_CONNECTOR_WORDS\n",
    "\n",
    "import spacy\n",
    "\n",
    "from textacy import extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For topic visualizations \n",
    "import pyLDAvis.gensim_models as gensim_vis\n",
    "import pyLDAvis\n",
    "# For enabling HTML widget in Jupyter notebook\n",
    "from pyLDAvis import enable_notebook\n",
    "\n",
    "enable_notebook(local=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_list_from_csv(path):\n",
    "    corpus = []\n",
    "    with open(path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            columns = line.split(',')   \n",
    "            # Columns 1 and 2 contain the company name and the job title, both guaranteed to not include commas, and both separated by a comma. \n",
    "            # We are not analyzing this information, so we can safely discard the first two columns.\n",
    "            # The third \"column\" contains the job description, but it may contain commas, so we use \",\".join() to concatenate all the columns after the second one.\n",
    "            # csv.reader()'s quotechar parameter does not seem to work for whatever reason, and this just seemed faster. \n",
    "            description = \",\".join(columns[2:]).strip('\"')      # strip('\"') to remove leading and trailing quotes\n",
    "            corpus.append(description)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = create_list_from_csv('jobs.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "1812\n",
      " \"What You'll Do Analyze, design, develop, test, review, document and troubleshoot data pipeline / ELT solutions against multiple structured and unstructured data sources. Support our team of analysts\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))\n",
    "print(len(corpus[-1]))\n",
    "print(corpus[-1][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 178\n",
      "Number of words: 45\n",
      "Number of words: 180\n",
      "Number of words: 180\n",
      "Number of words: 284\n",
      "Number of words: 284\n",
      "Number of words: 284\n",
      "Number of words: 284\n",
      "Number of words: 284\n",
      "Number of words: 284\n",
      "Number of words: 329\n",
      "Number of words: 198\n",
      "Number of words: 142\n",
      "Number of words: 134\n",
      "Number of words: 134\n",
      "Number of words: 180\n",
      "Number of words: 291\n",
      "Number of words: 291\n",
      "Number of words: 134\n",
      "Number of words: 134\n",
      "Number of words: 230\n",
      "Number of words: 552\n",
      "Number of words: 391\n",
      "Number of words: 118\n",
      "Number of words: 275\n",
      "Number of words: 293\n",
      "Number of words: 355\n",
      "Number of words: 307\n",
      "Number of words: 366\n",
      "Number of words: 209\n",
      "Number of words: 666\n",
      "Number of words: 191\n",
      "Number of words: 448\n",
      "Number of words: 139\n",
      "Number of words: 244\n",
      "Number of words: 216\n",
      "Number of words: 142\n",
      "Number of words: 51\n",
      "Number of words: 109\n",
      "Number of words: 172\n",
      "Number of words: 116\n",
      "Number of words: 151\n",
      "Number of words: 115\n",
      "Number of words: 136\n",
      "Number of words: 271\n",
      "Number of words: 177\n",
      "Number of words: 165\n",
      "Number of words: 177\n",
      "Number of words: 186\n",
      "Number of words: 211\n",
      "Number of words: 218\n",
      "Number of words: 305\n",
      "Number of words: 330\n",
      "Number of words: 199\n",
      "Number of words: 230\n",
      "Number of words: 196\n",
      "Number of words: 320\n",
      "Number of words: 343\n",
      "Number of words: 260\n",
      "Number of words: 356\n",
      "Number of words: 169\n",
      "Number of words: 148\n",
      "Number of words: 271\n",
      "Number of words: 396\n",
      "Number of words: 288\n",
      "Number of words: 74\n",
      "Number of words: 270\n",
      "Number of words: 225\n",
      "Number of words: 395\n",
      "Number of words: 305\n",
      "Number of words: 202\n",
      "Number of words: 230\n",
      "Number of words: 88\n",
      "Number of words: 277\n",
      "Number of words: 298\n",
      "Number of words: 147\n",
      "Number of words: 105\n",
      "Number of words: 230\n",
      "Number of words: 462\n",
      "Number of words: 227\n",
      "Number of words: 136\n",
      "Number of words: 162\n",
      "Number of words: 309\n",
      "Number of words: 269\n",
      "Number of words: 89\n",
      "Number of words: 89\n",
      "Number of words: 162\n",
      "Number of words: 381\n",
      "Number of words: 205\n",
      "Number of words: 245\n",
      "Number of words: 283\n",
      "Number of words: 428\n",
      "Number of words: 170\n",
      "Number of words: 153\n",
      "Number of words: 164\n",
      "Number of words: 157\n",
      "Number of words: 226\n",
      "Number of words: 248\n",
      "Total number of words in the corpus: 22873\n",
      "Mean: 233.4\n",
      "Standard deviation: 105.24\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "doc_lengths = []\n",
    "for doc in corpus:\n",
    "    sum += len(doc.split())\n",
    "    print(\"Number of words:\", len(doc.split()))\n",
    "    doc_lengths.append(len(doc.split()))\n",
    "    \n",
    "print(f\"Total number of words in the corpus: {sum}\")\n",
    "print(\"Mean:\", round(np.mean(doc_lengths),2))\n",
    "print(\"Standard deviation:\", round(np.std(doc_lengths),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_with_spacy(doc, lemmatize = False):\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])   # Disable the parser and named entity recognition since we only need the tokenization, lemmatization, and POS tagging\n",
    "    # Add custom stop words that don't add anything to each topic.\n",
    "    nlp.Defaults.stop_words |= {\"experience\", \"preferred\", \"skill\", \"yelp\", \"strong\", \"work\", \"solutions\", \"drive\", \"insights\", \"use\", \"needs\", \"responsibilities\", \"do\", \"particularly\", \"related\", \"leak\", \"radio\", }\n",
    "    \n",
    "    spacy_doc = nlp(doc.lower())\n",
    "\n",
    "    ngrams = [\n",
    "        ngram.text.replace(\" \", \"_\")    # ngrams are separated by spaces, so we replace them with underscores\n",
    "        for ngram in extract.ngrams(spacy_doc, n = 2, min_freq = 4, filter_punct = True, filter_nums = True, exclude_pos=[\"ORG\", \"DATE\", \"X\"]) \n",
    "        if not ngram.text.__contains__(\"=\") \n",
    "            and not ngram.text.__contains__(\"@\") \n",
    "            and not ngram.text.__contains__(\"$\")\n",
    "    ]\n",
    "    \n",
    "    allowed_pos_tags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]\n",
    "    tokens = []\n",
    "\n",
    "    if lemmatize:\n",
    "        # Remove stopwords, punctuation, and numeric tokens\n",
    "        tokens = [\n",
    "            token.lemma_ \n",
    "            for token in spacy_doc \n",
    "            if not token.is_stop and not token.is_punct and not token.is_digit and token.is_alpha       # Keep only words that are not stop words\n",
    "                and token.text not in [\"_\", \"+\", \"=\", \"\\n\",\"-\",\"*\",\"<\",\">\"]                             # Remove special characters     \n",
    "                and not len(token.text) == 1                                                            # Remove single character words\n",
    "                and token.pos_ in allowed_pos_tags                                                      # Keep only words that are nouns, adjectives, verbs, and adverbs\n",
    "        ]          \n",
    "\n",
    "        tokens = [token.replace(\"datum\", \"data\") for token in tokens]                                   # Replace \"datum\" (lemma of data) with \"data\" for clarity\n",
    "    \n",
    "    else:\n",
    "        tokens = [\n",
    "            token.text \n",
    "            for token in spacy_doc \n",
    "            if not token.is_stop and not token.is_punct and not token.is_digit and token.is_alpha       # Keep only words that are not stop words\n",
    "                and token.text not in [\"_\", \"+\", \"=\", \"\\n\",\"-\",\"*\",\"<\",\">\"]                             # Remove special characters     \n",
    "                and not len(token.text) == 1                                                            # Remove single character words\n",
    "                and token.pos_ in allowed_pos_tags                                                      # Keep only words that are nouns, adjectives, verbs, and adverbs\n",
    "        ]                                                                                   \n",
    "    \n",
    "    return tokens + ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating non-lemmatized corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the cleaned corpus: 14290\n"
     ]
    }
   ],
   "source": [
    "corpus_with_bigrams = [clean_with_spacy(doc) for doc in corpus]\n",
    "sum = 0\n",
    "for doc in corpus_with_bigrams:\n",
    "    sum += len(doc)\n",
    "\n",
    "print(f\"Total number of words in the cleaned corpus: {sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'collection', 'cleaning', 'assist', 'collection', 'organization', 'data', 'data', 'cleaning', 'preprocessing', 'activities', 'ensure', 'data', 'accuracy', 'data', 'analysis', 'utilize', 'statistical', 'methods', 'tools', 'assist', 'analysis', 'datasets', 'gsp', 'team', 'members', 'identify', 'trends', 'patterns', 'data', 'data', 'visualization', 'support', 'creation', 'visualizations', 'reports', 'communicating', 'data', 'findings', 'collaboration', 'collaborate', 'team', 'members', 'understand', 'data', 'requirements', 'provide', 'support', 'delivering', 'analytical', 'learn', 'experienced', 'team', 'members', 'actively', 'seek', 'guidance', 'generation', 'learn', 'summarize', 'communicatedatainsights', 'clear', 'understandable', 'learning', 'actively', 'participate', 'training', 'development', 'opportunities', 'enhance', 'skills', 'job', 'qualifications', 'bachelor', 'degree', 'relevant', 'field', 'mathematics', 'computer', 'science', 'equivalent', 'basic', 'understanding', 'data', 'analysis', 'concepts', 'methodologies', 'familiarity', 'data', 'analysis', 'tools', 'power', 'excel', 'attention', 'detail', 'analytical', 'skills', 'good', 'communication', 'skills', 'ability', 'collaboratively', 'team', 'eagerness', 'learn', 'adapt', 'new', 'technologies', 'techniques']\n"
     ]
    }
   ],
   "source": [
    "print(corpus_with_bigrams[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 2), (3, 1), (4, 1), (5, 4), (6, 2), (7, 2), (8, 1), (9, 1), (10, 1), (11, 2), (12, 1), (13, 1), (14, 1), (15, 1), (16, 2), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 11), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 3), (45, 1), (46, 1), (47, 3), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 3), (65, 1), (66, 1), (67, 2), (68, 4), (69, 1), (70, 1), (71, 2), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 1), (79, 1)]\n"
     ]
    }
   ],
   "source": [
    "dictionary = Dictionary(corpus_with_bigrams)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in corpus_with_bigrams]\n",
    "print(doc_term_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating lemmatized corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the cleaned corpus: 14290\n"
     ]
    }
   ],
   "source": [
    "lem_corpus_with_bigrams = [clean_with_spacy(doc, lemmatize=True) for doc in corpus]\n",
    "sum = 0\n",
    "for doc in lem_corpus_with_bigrams:\n",
    "    sum += len(doc)\n",
    "\n",
    "print(f\"Total number of words in the cleaned corpus: {sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'collection', 'cleaning', 'assist', 'collection', 'organization', 'data', 'data', 'cleaning', 'preprocesse', 'activity', 'ensure', 'data', 'accuracy', 'data', 'analysis', 'utilize', 'statistical', 'method', 'tool', 'assist', 'analysis', 'dataset', 'gsp', 'team', 'member', 'identify', 'trend', 'pattern', 'data', 'data', 'visualization', 'support', 'creation', 'visualization', 'report', 'communicate', 'data', 'finding', 'collaboration', 'collaborate', 'team', 'member', 'understand', 'data', 'requirement', 'provide', 'support', 'deliver', 'analytical', 'learn', 'experienced', 'team', 'member', 'actively', 'seek', 'guidance', 'generation', 'learn', 'summarize', 'communicatedatainsight', 'clear', 'understandable', 'learning', 'actively', 'participate', 'training', 'development', 'opportunity', 'enhance', 'skill', 'job', 'qualification', 'bachelor', 'degree', 'relevant', 'field', 'mathematic', 'computer', 'science', 'equivalent', 'basic', 'understanding', 'data', 'analysis', 'concept', 'methodology', 'familiarity', 'data', 'analysis', 'tool', 'power', 'excel', 'attention', 'detail', 'analytical', 'skill', 'good', 'communication', 'skill', 'ability', 'collaboratively', 'team', 'eagerness', 'learn', 'adapt', 'new', 'technology', 'technique']\n"
     ]
    }
   ],
   "source": [
    "print(lem_corpus_with_bigrams[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 2), (3, 1), (4, 1), (5, 4), (6, 2), (7, 2), (8, 1), (9, 1), (10, 1), (11, 2), (12, 1), (13, 1), (14, 1), (15, 1), (16, 2), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 11), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 3), (45, 1), (46, 1), (47, 3), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 3), (65, 1), (66, 1), (67, 2), (68, 4), (69, 1), (70, 1), (71, 2), (72, 1), (73, 1), (74, 1), (75, 1), (76, 1), (77, 1), (78, 2)]\n"
     ]
    }
   ],
   "source": [
    "lem_dictionary = Dictionary(lem_corpus_with_bigrams)\n",
    "lem_doc_term_matrix = [lem_dictionary.doc2bow(doc) for doc in lem_corpus_with_bigrams]\n",
    "print(lem_doc_term_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling with non-lemmatized corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 8\n",
    "PATH_TO_MODEL = f\"Entry_Jobs_NoLemma_LDA_{NUM_TOPICS}_topics\"\n",
    "lda_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.010*\"algorithms\" + 0.010*\"analysis\" + 0.009*\"sentiment\" + 0.009*\"text\" + 0.009*\"quality\" + 0.007*\"team\" + 0.007*\"knowledge\" + 0.007*\"design\" + 0.006*\"best\" + 0.006*\"contribute\"'),\n",
       " (1,\n",
       "  '0.023*\"learning\" + 0.020*\"machine\" + 0.019*\"models\" + 0.016*\"ai\" + 0.010*\"research\" + 0.009*\"skills\" + 0.008*\"machine_learning\" + 0.008*\"engineering\" + 0.008*\"systems\" + 0.008*\"performance\"'),\n",
       " (2,\n",
       "  '0.014*\"mining\" + 0.011*\"data_mining\" + 0.011*\"programs\" + 0.009*\"op\" + 0.009*\"co\" + 0.009*\"school\" + 0.006*\"idea\" + 0.005*\"create\" + 0.004*\"new\" + 0.004*\"game\"'),\n",
       " (3,\n",
       "  '0.086*\"data\" + 0.016*\"skills\" + 0.013*\"analysis\" + 0.011*\"tools\" + 0.008*\"science\" + 0.008*\"processes\" + 0.008*\"team\" + 0.008*\"business\" + 0.008*\"analytics\" + 0.008*\"development\"'),\n",
       " (4,\n",
       "  '0.064*\"data\" + 0.011*\"skills\" + 0.010*\"business\" + 0.010*\"learning\" + 0.009*\"machine\" + 0.008*\"models\" + 0.008*\"teams\" + 0.008*\"pipelines\" + 0.008*\"tools\" + 0.007*\"systems\"'),\n",
       " (5,\n",
       "  '0.022*\"data\" + 0.014*\"ability\" + 0.009*\"spark\" + 0.009*\"complex\" + 0.008*\"understand\" + 0.008*\"design\" + 0.007*\"database\" + 0.007*\"processes\" + 0.005*\"sales\" + 0.005*\"production\"'),\n",
       " (6,\n",
       "  '0.018*\"learning\" + 0.017*\"data\" + 0.014*\"business\" + 0.013*\"machine\" + 0.013*\"knowledge\" + 0.012*\"skills\" + 0.011*\"machine_learning\" + 0.009*\"models\" + 0.009*\"research\" + 0.009*\"development\"'),\n",
       " (7,\n",
       "  '0.012*\"ai\" + 0.010*\"learning\" + 0.008*\"real\" + 0.008*\"asset\" + 0.005*\"data\" + 0.005*\"models\" + 0.005*\"team\" + 0.005*\"research\" + 0.005*\"applications\" + 0.005*\"degree\"')]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model = LdaModel(doc_term_matrix, num_topics = NUM_TOPICS, id2word = dictionary, alpha = 'auto', eta = 'auto', passes = 50, random_state=448)\n",
    "lda_model.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "lda_model.save(datapath(PATH_TO_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared = gensim_vis.prepare(lda_model, doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, f'Jobs_LDA_NoLemma_{NUM_TOPICS}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.009*\"research\" + 0.008*\"learning\" + 0.008*\"knowledge\" + 0.006*\"team\" + 0.006*\"development\" + 0.006*\"design\" + 0.006*\"skills\" + 0.005*\"algorithms\" + 0.005*\"code\" + 0.005*\"systems\"'),\n",
       " (1,\n",
       "  '0.061*\"data\" + 0.012*\"skills\" + 0.011*\"learning\" + 0.009*\"machine\" + 0.009*\"models\" + 0.008*\"tools\" + 0.008*\"business\" + 0.008*\"analysis\" + 0.006*\"science\" + 0.006*\"team\"'),\n",
       " (2,\n",
       "  '0.029*\"data\" + 0.013*\"business\" + 0.009*\"skills\" + 0.007*\"partners\" + 0.006*\"design\" + 0.005*\"ability\" + 0.005*\"support\" + 0.005*\"different\" + 0.005*\"processes\" + 0.004*\"teams\"')]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model = LdaModel(doc_term_matrix, num_topics = NUM_TOPICS, id2word = dictionary, alpha = 'auto', eta = 'auto', passes = 50, random_state=448)\n",
    "lda_model.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "lda_model.save(datapath(PATH_TO_MODEL))\n",
    "\n",
    "LDAvis_prepared = gensim_vis.prepare(lda_model, doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, f'Jobs_LDA_NoLemma_{NUM_TOPICS}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.016*\"research\" + 0.012*\"analysis\" + 0.010*\"algorithms\" + 0.010*\"sentiment\" + 0.009*\"team\" + 0.009*\"text\" + 0.009*\"design\" + 0.008*\"contribute\" + 0.008*\"interns\" + 0.008*\"research_interns\"'),\n",
       " (1,\n",
       "  '0.011*\"learning\" + 0.011*\"machine\" + 0.009*\"systems\" + 0.009*\"software\" + 0.008*\"infrastructure\" + 0.008*\"apply\" + 0.007*\"performance\" + 0.007*\"design\" + 0.007*\"quality\" + 0.006*\"skills\"'),\n",
       " (2,\n",
       "  '0.000*\"data\" + 0.000*\"learning\" + 0.000*\"ability\" + 0.000*\"models\" + 0.000*\"skills\" + 0.000*\"development\" + 0.000*\"machine\" + 0.000*\"research\" + 0.000*\"understanding\" + 0.000*\"knowledge\"'),\n",
       " (3,\n",
       "  '0.094*\"data\" + 0.017*\"skills\" + 0.013*\"analysis\" + 0.012*\"tools\" + 0.010*\"science\" + 0.010*\"processes\" + 0.009*\"ability\" + 0.009*\"business\" + 0.009*\"team\" + 0.009*\"analytics\"'),\n",
       " (4,\n",
       "  '0.038*\"data\" + 0.018*\"learning\" + 0.017*\"machine\" + 0.016*\"models\" + 0.009*\"systems\" + 0.009*\"business\" + 0.009*\"ai\" + 0.008*\"aws\" + 0.008*\"skills\" + 0.008*\"pipelines\"'),\n",
       " (5,\n",
       "  '0.035*\"data\" + 0.009*\"complex\" + 0.009*\"skills\" + 0.008*\"engineering\" + 0.008*\"design\" + 0.008*\"spark\" + 0.008*\"models\" + 0.007*\"datasets\" + 0.007*\"python\" + 0.007*\"business\"'),\n",
       " (6,\n",
       "  '0.019*\"business\" + 0.018*\"data\" + 0.011*\"knowledge\" + 0.009*\"design\" + 0.008*\"development\" + 0.008*\"understanding\" + 0.008*\"learning\" + 0.007*\"team\" + 0.007*\"skills\" + 0.007*\"systems\"'),\n",
       " (7,\n",
       "  '0.023*\"ai\" + 0.014*\"learning\" + 0.013*\"models\" + 0.010*\"real\" + 0.009*\"training\" + 0.009*\"applications\" + 0.008*\"vision\" + 0.008*\"world\" + 0.007*\"mlops\" + 0.007*\"workflows\"'),\n",
       " (8,\n",
       "  '0.063*\"data\" + 0.018*\"reports\" + 0.015*\"skills\" + 0.010*\"teams\" + 0.009*\"including\" + 0.009*\"client\" + 0.008*\"analyze\" + 0.008*\"analysis\" + 0.007*\"manage\" + 0.007*\"ability\"'),\n",
       " (9,\n",
       "  '0.030*\"learning\" + 0.024*\"machine\" + 0.024*\"machine_learning\" + 0.019*\"models\" + 0.016*\"research\" + 0.013*\"data\" + 0.011*\"skills\" + 0.009*\"engineering\" + 0.008*\"computer\" + 0.007*\"deep\"')]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model = LdaModel(doc_term_matrix, num_topics = NUM_TOPICS, id2word = dictionary, alpha = 'auto', eta = 'auto', passes = 50, random_state=448)\n",
    "lda_model.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "lda_model.save(datapath(PATH_TO_MODEL))\n",
    "\n",
    "LDAvis_prepared = gensim_vis.prepare(lda_model, doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, f'Jobs_LDA_NoLemma_{NUM_TOPICS}.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effect of number of passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.012*\"research\" + 0.010*\"analysis\" + 0.008*\"algorithms\" + 0.007*\"team\" + 0.007*\"quality\" + 0.007*\"sentiment\" + 0.007*\"software\" + 0.007*\"contribute\" + 0.006*\"systems\" + 0.006*\"collaborate\"'),\n",
       " (1,\n",
       "  '0.017*\"learning\" + 0.014*\"machine\" + 0.012*\"ai\" + 0.012*\"models\" + 0.009*\"skills\" + 0.007*\"performance\" + 0.007*\"systems\" + 0.006*\"computer\" + 0.006*\"infrastructure\" + 0.006*\"training\"'),\n",
       " (2,\n",
       "  '0.000*\"data\" + 0.000*\"learning\" + 0.000*\"models\" + 0.000*\"skills\" + 0.000*\"research\" + 0.000*\"machine\" + 0.000*\"engineering\" + 0.000*\"development\" + 0.000*\"ability\" + 0.000*\"technologies\"'),\n",
       " (3,\n",
       "  '0.091*\"data\" + 0.016*\"skills\" + 0.013*\"analysis\" + 0.011*\"tools\" + 0.009*\"team\" + 0.009*\"ability\" + 0.008*\"processes\" + 0.008*\"science\" + 0.008*\"analytical\" + 0.007*\"reports\"'),\n",
       " (4,\n",
       "  '0.050*\"data\" + 0.018*\"learning\" + 0.017*\"machine\" + 0.015*\"models\" + 0.010*\"pipelines\" + 0.009*\"skills\" + 0.009*\"business\" + 0.009*\"systems\" + 0.008*\"engineering\" + 0.008*\"aws\"'),\n",
       " (5,\n",
       "  '0.037*\"data\" + 0.011*\"skills\" + 0.009*\"ability\" + 0.008*\"business\" + 0.007*\"datasets\" + 0.007*\"analysis\" + 0.006*\"models\" + 0.006*\"complex\" + 0.006*\"design\" + 0.006*\"reports\"'),\n",
       " (6,\n",
       "  '0.016*\"data\" + 0.014*\"learning\" + 0.013*\"business\" + 0.009*\"knowledge\" + 0.009*\"skills\" + 0.009*\"team\" + 0.008*\"understanding\" + 0.007*\"development\" + 0.007*\"design\" + 0.006*\"code\"')]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model = LdaModel(doc_term_matrix, num_topics = 7, id2word = dictionary, alpha = 'auto', eta = 'auto', passes = 25, random_state=448)\n",
    "lda_model.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.011*\"research\" + 0.009*\"analysis\" + 0.008*\"algorithms\" + 0.008*\"team\" + 0.007*\"quality\" + 0.007*\"sentiment\" + 0.007*\"software\" + 0.007*\"contribute\" + 0.006*\"collaborate\" + 0.006*\"systems\"'),\n",
       " (1,\n",
       "  '0.016*\"learning\" + 0.013*\"machine\" + 0.011*\"ai\" + 0.010*\"models\" + 0.009*\"skills\" + 0.007*\"systems\" + 0.007*\"performance\" + 0.006*\"infrastructure\" + 0.006*\"computer\" + 0.005*\"training\"'),\n",
       " (2,\n",
       "  '0.000*\"data\" + 0.000*\"learning\" + 0.000*\"models\" + 0.000*\"research\" + 0.000*\"skills\" + 0.000*\"machine\" + 0.000*\"engineering\" + 0.000*\"development\" + 0.000*\"ability\" + 0.000*\"technologies\"'),\n",
       " (3,\n",
       "  '0.093*\"data\" + 0.017*\"skills\" + 0.013*\"analysis\" + 0.011*\"tools\" + 0.009*\"team\" + 0.009*\"science\" + 0.009*\"ability\" + 0.009*\"processes\" + 0.007*\"analytical\" + 0.007*\"business\"'),\n",
       " (4,\n",
       "  '0.043*\"data\" + 0.020*\"learning\" + 0.019*\"machine\" + 0.016*\"models\" + 0.010*\"pipelines\" + 0.009*\"systems\" + 0.009*\"business\" + 0.009*\"engineering\" + 0.008*\"aws\" + 0.008*\"skills\"'),\n",
       " (5,\n",
       "  '0.035*\"data\" + 0.010*\"skills\" + 0.009*\"ability\" + 0.008*\"business\" + 0.007*\"datasets\" + 0.007*\"analysis\" + 0.006*\"models\" + 0.006*\"complex\" + 0.006*\"reports\" + 0.006*\"statistical\"'),\n",
       " (6,\n",
       "  '0.016*\"data\" + 0.013*\"learning\" + 0.013*\"business\" + 0.009*\"knowledge\" + 0.009*\"team\" + 0.009*\"skills\" + 0.008*\"understanding\" + 0.007*\"development\" + 0.007*\"design\" + 0.007*\"code\"')]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model = LdaModel(doc_term_matrix, num_topics = 7, id2word = dictionary, alpha = 'auto', eta = 'auto', passes = 50, random_state=448)\n",
    "lda_model.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared = gensim_vis.prepare(lda_model, doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, f'Jobs_LDA_NoLemma_7_Auto_50_passes.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just out of curiosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.010*data + 0.005*aws + 0.004*systems + 0.004*analytics + 0.004*machine + 0.004*learning + 0.004*building + 0.004*product + 0.003*manageable + 0.003*enabling'),\n",
       " (1,\n",
       "  '0.005*learning + 0.004*machine + 0.004*machine_learning + 0.004*data + 0.003*models + 0.003*image + 0.002*large + 0.002*energy + 0.002*engineering + 0.002*model'),\n",
       " (2,\n",
       "  '0.014*data + 0.003*environment + 0.003*present + 0.003*skills + 0.002*positions + 0.002*ability + 0.002*codepipeline + 0.002*colleagues + 0.002*participate + 0.002*thrive'),\n",
       " (3,\n",
       "  '0.005*data + 0.004*requirements + 0.003*production + 0.003*company + 0.003*failure + 0.003*clauses + 0.003*machine_learning + 0.003*platforms + 0.002*codebase + 0.002*masterpieces'),\n",
       " (4,\n",
       "  '0.006*data + 0.004*business + 0.003*systems + 0.003*good + 0.003*learning + 0.003*team + 0.003*overcome + 0.003*standardize + 0.002*workflows + 0.002*ems'),\n",
       " (5,\n",
       "  '0.006*data + 0.003*cross + 0.003*methods + 0.003*reports + 0.003*uncovering + 0.003*skills + 0.003*insurance + 0.003*come + 0.002*trends + 0.002*flat'),\n",
       " (6,\n",
       "  '0.005*data + 0.004*learning + 0.003*machine + 0.003*gaussian + 0.002*skills + 0.002*concerned + 0.002*successful + 0.002*frame + 0.002*english + 0.002*platform'),\n",
       " (7,\n",
       "  '0.006*data + 0.004*avenue + 0.003*elt + 0.003*parallel + 0.003*amounts + 0.003*loading + 0.003*ones + 0.003*meet + 0.002*site + 0.002*combination'),\n",
       " (8,\n",
       "  '0.012*data + 0.005*field + 0.003*moderate_level + 0.003*meaningful + 0.003*sources + 0.002*engineering + 0.002*troubleshooting + 0.002*believe + 0.002*analytics + 0.002*databases'),\n",
       " (9,\n",
       "  '0.010*data + 0.004*learn + 0.003*data_governance + 0.003*skills + 0.003*team + 0.003*opportunities + 0.003*different + 0.003*members + 0.003*schema + 0.003*analysis'),\n",
       " (10,\n",
       "  '0.004*data + 0.004*learning + 0.003*lead + 0.003*models + 0.003*ensure + 0.003*undergoing + 0.003*machine + 0.003*model + 0.003*paced + 0.003*enhances'),\n",
       " (11,\n",
       "  '0.007*data + 0.004*business + 0.003*analytical + 0.003*research + 0.003*developers + 0.003*producing + 0.003*python + 0.002*comprehensive + 0.002*articulation + 0.002*users'),\n",
       " (12,\n",
       "  '0.003*timetable + 0.003*corporate + 0.003*modelsand + 0.003*deep + 0.003*finance + 0.003*representations + 0.002*equivalent + 0.002*seismic + 0.002*meeting + 0.002*contain'),\n",
       " (13,\n",
       "  '0.006*data + 0.003*negotiate + 0.003*business + 0.003*involves + 0.003*fluency + 0.003*agents + 0.003*data_governance + 0.003*tools + 0.003*constantly + 0.002*budget'),\n",
       " (14,\n",
       "  '0.007*data + 0.004*common + 0.004*automating + 0.004*segmentation + 0.003*meet + 0.003*platforms + 0.003*validation + 0.003*foundational + 0.003*pipelines + 0.003*maintain')]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import HdpModel\n",
    "# from pprint import pprint\n",
    "\n",
    "hdp_model = HdpModel(doc_term_matrix, id2word = dictionary)\n",
    "hdp_model.optimal_ordering()\n",
    "hdp_model.show_topics(num_topics = 15, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, beta = hdp_model.hdp_to_lda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150,)\n",
      "(150, 2190)\n"
     ]
    }
   ],
   "source": [
    "print(alpha.shape)\n",
    "print(beta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling with lemmatized corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 8\n",
    "PATH_TO_MODEL = f\"Entry_Jobs_LDA_{NUM_TOPICS}_topics\"\n",
    "lda_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.008*\"machine\" + 0.008*\"record\" + 0.008*\"public\" + 0.008*\"track\" + 0.008*\"cloud\" + 0.005*\"open\" + 0.005*\"kubernete\" + 0.005*\"application\" + 0.005*\"distribute\" + 0.005*\"idea\"'),\n",
       " (1,\n",
       "  '0.021*\"model\" + 0.011*\"data\" + 0.011*\"code\" + 0.010*\"vision\" + 0.010*\"framework\" + 0.010*\"language\" + 0.009*\"knowledge\" + 0.008*\"proficiency\" + 0.008*\"research\" + 0.008*\"team\"'),\n",
       " (2,\n",
       "  '0.018*\"research\" + 0.014*\"data\" + 0.013*\"ability\" + 0.008*\"candidate\" + 0.008*\"skill\" + 0.007*\"system\" + 0.007*\"research_interns\" + 0.007*\"intern\" + 0.007*\"layer\" + 0.006*\"innovative\"'),\n",
       " (3,\n",
       "  '0.036*\"model\" + 0.024*\"machine\" + 0.022*\"learning\" + 0.018*\"ai\" + 0.016*\"data\" + 0.013*\"machine_learning\" + 0.011*\"learn\" + 0.010*\"design\" + 0.010*\"team\" + 0.010*\"engineering\"'),\n",
       " (4,\n",
       "  '0.021*\"data\" + 0.014*\"application\" + 0.011*\"include\" + 0.010*\"learn\" + 0.010*\"real\" + 0.010*\"code\" + 0.009*\"feature\" + 0.009*\"cloud\" + 0.009*\"skill\" + 0.009*\"analysis\"'),\n",
       " (5,\n",
       "  '0.018*\"data\" + 0.013*\"skill\" + 0.011*\"product\" + 0.010*\"problem\" + 0.010*\"computer\" + 0.009*\"degree\" + 0.009*\"science\" + 0.009*\"model\" + 0.008*\"code\" + 0.008*\"write\"'),\n",
       " (6,\n",
       "  '0.078*\"data\" + 0.013*\"skill\" + 0.011*\"team\" + 0.010*\"business\" + 0.010*\"analysis\" + 0.009*\"tool\" + 0.009*\"process\" + 0.009*\"analytic\" + 0.007*\"ability\" + 0.007*\"develop\"'),\n",
       " (7,\n",
       "  '0.034*\"business\" + 0.023*\"data\" + 0.020*\"team\" + 0.016*\"support\" + 0.015*\"good\" + 0.014*\"stakeholder\" + 0.014*\"partner\" + 0.014*\"different\" + 0.014*\"skill\" + 0.011*\"initiative\"')]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model = LdaModel(lem_doc_term_matrix, num_topics = NUM_TOPICS, id2word = lem_dictionary, alpha = 'auto', eta = 'auto', passes = 50, random_state=448)\n",
    "lda_model.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "lda_model.save(datapath(PATH_TO_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1811 is out of bounds for axis 1 with size 1811",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[220], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m LDAvis_prepared \u001b[38;5;241m=\u001b[39m \u001b[43mgensim_vis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlda_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlem_doc_term_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpcoa\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m pyLDAvis\u001b[38;5;241m.\u001b[39mdisplay(LDAvis_prepared)\n\u001b[0;32m      3\u001b[0m pyLDAvis\u001b[38;5;241m.\u001b[39msave_html(LDAvis_prepared, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJobs_LDA_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_TOPICS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages\\pyLDAvis\\gensim_models.py:122\u001b[0m, in \u001b[0;36mprepare\u001b[1;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare\u001b[39m(topic_model, corpus, dictionary, doc_topic_dist\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     78\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Transforms the Gensim TopicModel and related corpus and dictionary into\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;03m    the data structures needed for the visualization.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03m    See `pyLDAvis.prepare` for **kwargs.\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     opts \u001b[38;5;241m=\u001b[39m fp\u001b[38;5;241m.\u001b[39mmerge(\u001b[43m_extract_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_topic_dist\u001b[49m\u001b[43m)\u001b[49m, kwargs)\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pyLDAvis\u001b[38;5;241m.\u001b[39mprepare(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopts)\n",
      "File \u001b[1;32mc:\\Users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages\\pyLDAvis\\gensim_models.py:69\u001b[0m, in \u001b[0;36m_extract_data\u001b[1;34m(topic_model, corpus, dictionary, doc_topic_dists)\u001b[0m\n\u001b[0;32m     67\u001b[0m     topic \u001b[38;5;241m=\u001b[39m topic_model\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mget_lambda()\n\u001b[0;32m     68\u001b[0m topic \u001b[38;5;241m=\u001b[39m topic \u001b[38;5;241m/\u001b[39m topic\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[:, \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m---> 69\u001b[0m topic_term_dists \u001b[38;5;241m=\u001b[39m \u001b[43mtopic\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfnames_argsort\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m topic_term_dists\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m doc_topic_dists\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopic_term_dists\u001b[39m\u001b[38;5;124m'\u001b[39m: topic_term_dists, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc_topic_dists\u001b[39m\u001b[38;5;124m'\u001b[39m: doc_topic_dists,\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc_lengths\u001b[39m\u001b[38;5;124m'\u001b[39m: doc_lengths, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvocab\u001b[39m\u001b[38;5;124m'\u001b[39m: vocab, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mterm_frequency\u001b[39m\u001b[38;5;124m'\u001b[39m: term_freqs}\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1811 is out of bounds for axis 1 with size 1811"
     ]
    }
   ],
   "source": [
    "LDAvis_prepared = gensim_vis.prepare(lda_model, lem_doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, f'Jobs_LDA_{NUM_TOPICS}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.014*\"learning\" + 0.010*\"machine\" + 0.010*\"models\" + 0.009*\"data\" + 0.009*\"research\" + 0.008*\"knowledge\" + 0.006*\"design\" + 0.006*\"skills\" + 0.005*\"systems\" + 0.005*\"development\"'),\n",
       " (1,\n",
       "  '0.065*\"data\" + 0.011*\"skills\" + 0.011*\"business\" + 0.008*\"tools\" + 0.008*\"learning\" + 0.007*\"analysis\" + 0.006*\"analytics\" + 0.006*\"machine\" + 0.006*\"science\" + 0.006*\"teams\"'),\n",
       " (2,\n",
       "  '0.038*\"data\" + 0.014*\"skills\" + 0.011*\"learning\" + 0.008*\"machine\" + 0.008*\"team\" + 0.008*\"models\" + 0.006*\"tools\" + 0.006*\"analysis\" + 0.006*\"ability\" + 0.006*\"computer\"')]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model = LdaModel(lem_doc_term_matrix, num_topics = NUM_TOPICS, id2word = lem_dictionary, alpha = 'auto', eta = 'auto', passes = 50, random_state=448)\n",
    "lda_model.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared = gensim_vis.prepare(lda_model, lem_doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, f'Jobs_LDA_{NUM_TOPICS}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.013*\"research\" + 0.010*\"analysis\" + 0.009*\"contribute\" + 0.009*\"algorithms\" + 0.009*\"systems\" + 0.008*\"data\" + 0.008*\"collaborate\" + 0.008*\"sentiment\" + 0.008*\"text\" + 0.008*\"infrastructure\"'),\n",
       " (1,\n",
       "  '0.056*\"data\" + 0.015*\"clients\" + 0.014*\"client\" + 0.014*\"reports\" + 0.011*\"teams\" + 0.009*\"account_teams\" + 0.009*\"account\" + 0.008*\"findings\" + 0.007*\"tools\" + 0.006*\"analyze\"'),\n",
       " (2,\n",
       "  '0.021*\"ability\" + 0.010*\"sql\" + 0.010*\"processes\" + 0.008*\"scripts\" + 0.008*\"excel\" + 0.008*\"database\" + 0.008*\"understand\" + 0.007*\"data\" + 0.006*\"escalation\" + 0.006*\"formulas\"'),\n",
       " (3,\n",
       "  '0.031*\"data\" + 0.029*\"learning\" + 0.025*\"machine\" + 0.017*\"models\" + 0.014*\"systems\" + 0.011*\"machine_learning\" + 0.011*\"aws\" + 0.008*\"building\" + 0.008*\"product\" + 0.007*\"analytics\"'),\n",
       " (4,\n",
       "  '0.069*\"data\" + 0.015*\"skills\" + 0.014*\"business\" + 0.009*\"team\" + 0.009*\"science\" + 0.008*\"tools\" + 0.008*\"learning\" + 0.007*\"models\" + 0.007*\"development\" + 0.007*\"analysis\"'),\n",
       " (5,\n",
       "  '0.015*\"knowledge\" + 0.011*\"understanding\" + 0.010*\"skills\" + 0.010*\"software\" + 0.008*\"code\" + 0.008*\"learning\" + 0.007*\"project\" + 0.007*\"degree\" + 0.007*\"quality\" + 0.007*\"high\"'),\n",
       " (6,\n",
       "  '0.016*\"learning\" + 0.014*\"models\" + 0.012*\"language\" + 0.011*\"machine\" + 0.010*\"skills\" + 0.010*\"frameworks\" + 0.009*\"machine_learning\" + 0.008*\"data\" + 0.008*\"tools\" + 0.008*\"team\"'),\n",
       " (7,\n",
       "  '0.033*\"data\" + 0.017*\"reports\" + 0.014*\"trends\" + 0.010*\"datasets\" + 0.009*\"analysis\" + 0.009*\"visualizations\" + 0.009*\"analytical\" + 0.009*\"skills\" + 0.008*\"analyze\" + 0.008*\"extract\"'),\n",
       " (8,\n",
       "  '0.072*\"data\" + 0.013*\"processes\" + 0.012*\"tools\" + 0.011*\"science\" + 0.010*\"ability\" + 0.010*\"skills\" + 0.009*\"analysis\" + 0.009*\"analyze\" + 0.009*\"trends\" + 0.009*\"pipelines\"'),\n",
       " (9,\n",
       "  '0.022*\"data\" + 0.017*\"analysis\" + 0.009*\"knowledge\" + 0.009*\"technical\" + 0.009*\"system\" + 0.009*\"software\" + 0.008*\"tools\" + 0.008*\"leak\" + 0.008*\"skills\" + 0.007*\"analytics\"')]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model = LdaModel(lem_doc_term_matrix, num_topics = NUM_TOPICS, id2word = lem_dictionary, alpha = 'auto', eta = 'auto', passes = 50, random_state=448)\n",
    "lda_model.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared = gensim_vis.prepare(lda_model, lem_doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, f'Jobs_LDA_{NUM_TOPICS}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Done2!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dir-st",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
