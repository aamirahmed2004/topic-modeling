{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Used to tokenize the text; i.e. create a dictionary mapping words to integers. The dictionary can be used to create a term-document matrix.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models.phrases import ENGLISH_CONNECTOR_WORDS\n",
    "\n",
    "import spacy\n",
    "\n",
    "from textacy import extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For topic visualizations \n",
    "import pyLDAvis.gensim_models as gensim_vis\n",
    "import pyLDAvis\n",
    "# For enabling HTML widget in Jupyter notebook\n",
    "from pyLDAvis import enable_notebook\n",
    "\n",
    "enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_list_from_csv(path):\n",
    "    corpus = []\n",
    "    with open(path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            columns = line.split(',')   \n",
    "            # Columns 1 and 2 contain the company name and the job title, both guaranteed to not include commas, and both separated by a comma. \n",
    "            # We are not analyzing this information, so we can safely discard the first two columns.\n",
    "            # The third \"column\" contains the job description, but it may contain commas, so we use \",\".join() to concatenate all the columns after the second one.\n",
    "            # csv.reader()'s quotechar parameter does not seem to work for whatever reason, and this just seemed faster. \n",
    "            description = \",\".join(columns[2:]).strip('\"')      # strip('\"') to remove leading and trailing quotes\n",
    "            corpus.append(description)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings_list = create_list_from_csv('jobs.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262\n",
      "2051\n",
      " \"What You'll Do Apply statistical and machine learning techniques to process and analyze unstructured textual data Develop and finetune machine learning models for tasks such as entity recognition, c\n"
     ]
    }
   ],
   "source": [
    "print(len(strings_list))\n",
    "print(len(strings_list[-1]))\n",
    "print(strings_list[-1][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 178\n",
      "Number of words: 45\n",
      "Number of words: 180\n",
      "Number of words: 180\n",
      "Number of words: 284\n",
      "Number of words: 284\n",
      "Number of words: 284\n",
      "Number of words: 284\n",
      "Number of words: 284\n",
      "Number of words: 284\n",
      "Number of words: 329\n",
      "Number of words: 198\n",
      "Number of words: 142\n",
      "Number of words: 134\n",
      "Number of words: 134\n",
      "Number of words: 180\n",
      "Number of words: 291\n",
      "Number of words: 291\n",
      "Number of words: 134\n",
      "Number of words: 134\n",
      "Number of words: 230\n",
      "Number of words: 552\n",
      "Number of words: 391\n",
      "Number of words: 118\n",
      "Number of words: 275\n",
      "Number of words: 293\n",
      "Number of words: 355\n",
      "Number of words: 307\n",
      "Number of words: 366\n",
      "Number of words: 209\n",
      "Number of words: 666\n",
      "Number of words: 191\n",
      "Number of words: 448\n",
      "Number of words: 139\n",
      "Number of words: 216\n",
      "Number of words: 142\n",
      "Number of words: 51\n",
      "Number of words: 109\n",
      "Number of words: 172\n",
      "Number of words: 116\n",
      "Number of words: 151\n",
      "Number of words: 115\n",
      "Number of words: 136\n",
      "Number of words: 271\n",
      "Number of words: 177\n",
      "Number of words: 165\n",
      "Number of words: 177\n",
      "Number of words: 186\n",
      "Number of words: 211\n",
      "Number of words: 218\n",
      "Number of words: 305\n",
      "Number of words: 330\n",
      "Number of words: 199\n",
      "Number of words: 230\n",
      "Number of words: 196\n",
      "Number of words: 320\n",
      "Number of words: 343\n",
      "Number of words: 260\n",
      "Number of words: 356\n",
      "Number of words: 169\n",
      "Number of words: 148\n",
      "Number of words: 271\n",
      "Number of words: 396\n",
      "Number of words: 288\n",
      "Number of words: 74\n",
      "Number of words: 270\n",
      "Number of words: 225\n",
      "Number of words: 305\n",
      "Number of words: 202\n",
      "Number of words: 230\n",
      "Number of words: 88\n",
      "Number of words: 277\n",
      "Number of words: 298\n",
      "Number of words: 147\n",
      "Number of words: 105\n",
      "Number of words: 230\n",
      "Number of words: 462\n",
      "Number of words: 227\n",
      "Number of words: 136\n",
      "Number of words: 162\n",
      "Number of words: 309\n",
      "Number of words: 269\n",
      "Number of words: 89\n",
      "Number of words: 89\n",
      "Number of words: 162\n",
      "Number of words: 381\n",
      "Number of words: 205\n",
      "Number of words: 245\n",
      "Number of words: 283\n",
      "Number of words: 428\n",
      "Number of words: 170\n",
      "Number of words: 153\n",
      "Number of words: 164\n",
      "Number of words: 157\n",
      "Number of words: 226\n",
      "Number of words: 248\n",
      "Number of words: 144\n",
      "Number of words: 178\n",
      "Number of words: 317\n",
      "Number of words: 67\n",
      "Number of words: 249\n",
      "Number of words: 121\n",
      "Number of words: 156\n",
      "Number of words: 141\n",
      "Number of words: 181\n",
      "Number of words: 202\n",
      "Number of words: 212\n",
      "Number of words: 309\n",
      "Number of words: 142\n",
      "Number of words: 234\n",
      "Number of words: 284\n",
      "Number of words: 334\n",
      "Number of words: 219\n",
      "Number of words: 50\n",
      "Number of words: 169\n",
      "Number of words: 345\n",
      "Number of words: 281\n",
      "Number of words: 252\n",
      "Number of words: 196\n",
      "Number of words: 851\n",
      "Number of words: 409\n",
      "Number of words: 547\n",
      "Number of words: 176\n",
      "Number of words: 126\n",
      "Number of words: 307\n",
      "Number of words: 45\n",
      "Number of words: 45\n",
      "Number of words: 406\n",
      "Number of words: 301\n",
      "Number of words: 302\n",
      "Number of words: 250\n",
      "Number of words: 250\n",
      "Number of words: 250\n",
      "Number of words: 149\n",
      "Number of words: 129\n",
      "Number of words: 269\n",
      "Number of words: 395\n",
      "Number of words: 335\n",
      "Number of words: 303\n",
      "Number of words: 445\n",
      "Number of words: 330\n",
      "Number of words: 175\n",
      "Number of words: 241\n",
      "Number of words: 312\n",
      "Number of words: 312\n",
      "Number of words: 223\n",
      "Number of words: 342\n",
      "Number of words: 313\n",
      "Number of words: 224\n",
      "Number of words: 224\n",
      "Number of words: 224\n",
      "Number of words: 359\n",
      "Number of words: 297\n",
      "Number of words: 228\n",
      "Number of words: 224\n",
      "Number of words: 123\n",
      "Number of words: 167\n",
      "Number of words: 78\n",
      "Number of words: 228\n",
      "Number of words: 279\n",
      "Number of words: 402\n",
      "Number of words: 139\n",
      "Number of words: 320\n",
      "Number of words: 320\n",
      "Number of words: 544\n",
      "Number of words: 95\n",
      "Number of words: 320\n",
      "Number of words: 76\n",
      "Number of words: 182\n",
      "Number of words: 173\n",
      "Number of words: 125\n",
      "Number of words: 412\n",
      "Number of words: 412\n",
      "Number of words: 376\n",
      "Number of words: 376\n",
      "Number of words: 73\n",
      "Number of words: 385\n",
      "Number of words: 385\n",
      "Number of words: 255\n",
      "Number of words: 239\n",
      "Number of words: 194\n",
      "Number of words: 251\n",
      "Number of words: 184\n",
      "Number of words: 155\n",
      "Number of words: 244\n",
      "Number of words: 244\n",
      "Number of words: 213\n",
      "Number of words: 99\n",
      "Number of words: 214\n",
      "Number of words: 136\n",
      "Number of words: 222\n",
      "Number of words: 95\n",
      "Number of words: 116\n",
      "Number of words: 229\n",
      "Number of words: 151\n",
      "Number of words: 602\n",
      "Number of words: 231\n",
      "Number of words: 437\n",
      "Number of words: 124\n",
      "Number of words: 317\n",
      "Number of words: 286\n",
      "Number of words: 227\n",
      "Number of words: 225\n",
      "Number of words: 217\n",
      "Number of words: 284\n",
      "Number of words: 486\n",
      "Number of words: 255\n",
      "Number of words: 293\n",
      "Number of words: 312\n",
      "Number of words: 115\n",
      "Number of words: 419\n",
      "Number of words: 147\n",
      "Number of words: 288\n",
      "Number of words: 208\n",
      "Number of words: 218\n",
      "Number of words: 131\n",
      "Number of words: 307\n",
      "Number of words: 289\n",
      "Number of words: 170\n",
      "Number of words: 136\n",
      "Number of words: 447\n",
      "Number of words: 149\n",
      "Number of words: 192\n",
      "Number of words: 300\n",
      "Number of words: 280\n",
      "Number of words: 290\n",
      "Number of words: 123\n",
      "Number of words: 323\n",
      "Number of words: 228\n",
      "Number of words: 327\n",
      "Number of words: 260\n",
      "Number of words: 329\n",
      "Number of words: 328\n",
      "Number of words: 224\n",
      "Number of words: 150\n",
      "Number of words: 211\n",
      "Number of words: 211\n",
      "Number of words: 211\n",
      "Number of words: 120\n",
      "Number of words: 377\n",
      "Number of words: 314\n",
      "Number of words: 150\n",
      "Number of words: 464\n",
      "Number of words: 274\n",
      "Number of words: 234\n",
      "Number of words: 235\n",
      "Number of words: 159\n",
      "Number of words: 164\n",
      "Number of words: 477\n",
      "Number of words: 670\n",
      "Number of words: 246\n",
      "Number of words: 113\n",
      "Number of words: 194\n",
      "Number of words: 208\n",
      "Number of words: 193\n",
      "Number of words: 193\n",
      "Number of words: 306\n",
      "Number of words: 256\n",
      "Number of words: 166\n",
      "Number of words: 309\n",
      "Number of words: 340\n",
      "Number of words: 293\n",
      "Total number of words in the corpus: 64328\n",
      "Mean: 245.53\n",
      "Standard deviation: 114.06\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "doc_lengths = []\n",
    "for desc_string in strings_list:\n",
    "    sum += len(desc_string.split())\n",
    "    print(\"Number of words:\", len(desc_string.split()))\n",
    "    doc_lengths.append(len(desc_string.split()))\n",
    "    \n",
    "print(f\"Total number of words in the corpus: {sum}\")\n",
    "print(\"Mean:\", round(np.mean(doc_lengths),2))\n",
    "print(\"Standard deviation:\", round(np.std(doc_lengths),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "nlp.Defaults.stop_words |= {\"experience\", \"preferred\", \"skill\", \"yelp\", \"strong\", \"work\", \"solutions\", \"drive\", \"insights\", \"use\", \"needs\", \"responsibilities\", \"do\", \"particularly\", \"related\", \"leak\", \"radio\", \"understand\", \"apply\", \"self\", \"like\", \"work\", \"qualifications\", \"do\", \"bring\", \"include\", \"problem\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Old function, kept here because of how stupid it is\n",
    "\n",
    "def clean_with_spacy(doc, lemmatize = False):\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])   # Disable the parser and named entity recognition since we only need the tokenization, lemmatization, and POS tagging\n",
    "    # Add custom stop words that don't add anything to each topic.\n",
    "    nlp.Defaults.stop_words |= {\"experience\", \"preferred\", \"skill\", \"yelp\", \"strong\", \"work\", \"solutions\", \"drive\", \"insights\", \"use\", \"needs\", \"responsibilities\", \"do\", \"particularly\", \"related\", \"leak\", \"radio\", }\n",
    "    \n",
    "    spacy_doc = nlp(doc.lower())\n",
    "\n",
    "    ngrams = [\n",
    "        ngram.text.replace(\" \", \"_\")    # ngrams are separated by spaces, so we replace them with underscores\n",
    "        for ngram in extract.ngrams(spacy_doc, n = 2, min_freq = 4, filter_punct = True, filter_nums = True, exclude_pos=[\"ORG\", \"DATE\", \"X\"]) \n",
    "        if not ngram.text.__contains__(\"=\") \n",
    "            and not ngram.text.__contains__(\"@\") \n",
    "            and not ngram.text.__contains__(\"$\")\n",
    "    ]\n",
    "    \n",
    "    allowed_pos_tags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]\n",
    "    tokens = []\n",
    "\n",
    "    if lemmatize:\n",
    "        # Remove stopwords, punctuation, and numeric tokens\n",
    "        tokens = [\n",
    "            token.lemma_ \n",
    "            for token in spacy_doc \n",
    "            if not token.is_stop and not token.is_punct and not token.is_digit and token.is_alpha       # Keep only words that are not stop words\n",
    "                and token.text not in [\"_\", \"+\", \"=\", \"\\n\",\"-\",\"*\",\"<\",\">\"]                             # Remove special characters     \n",
    "                and not len(token.text) == 1                                                            # Remove single character words\n",
    "                and token.pos_ in allowed_pos_tags                                                      # Keep only words that are nouns, adjectives, verbs, and adverbs\n",
    "        ]          \n",
    "\n",
    "        tokens = [token.replace(\"datum\", \"data\") for token in tokens]                                   # Replace \"datum\" (lemma of data) with \"data\" for clarity\n",
    "    \n",
    "    else:\n",
    "        tokens = [\n",
    "            token.text \n",
    "            for token in spacy_doc \n",
    "            if not token.is_stop and not token.is_punct and not token.is_digit and token.is_alpha       # Keep only words that are not stop words\n",
    "                and token.text not in [\"_\", \"+\", \"=\", \"\\n\",\"-\",\"*\",\"<\",\">\"]                             # Remove special characters     \n",
    "                and not len(token.text) == 1                                                            # Remove single character words\n",
    "                and token.pos_ in allowed_pos_tags                                                      # Keep only words that are nouns, adjectives, verbs, and adverbs\n",
    "        ]                                                                                   \n",
    "    \n",
    "    return tokens + ngrams\n",
    "\"\"\"\n",
    "\n",
    "def clean_without_ngrams(doc):\n",
    "\n",
    "    spacy_doc = nlp(doc.lower())\n",
    "\n",
    "    # Remove stopwords, punctuation, and numeric tokens\n",
    "    tokens = [\n",
    "        token.text \n",
    "        for token in spacy_doc \n",
    "        if not token.is_stop and not token.is_punct and not token.is_digit and token.is_alpha       # Keep only words that are not stop words\n",
    "            and token.text not in [\"_\", \"+\", \"=\", \"\\n\",\"-\",\"*\",\"<\",\">\"]                             # Remove special characters       \n",
    "            and not len(token.text) == 1                                                            # Remove single character words\n",
    "    ]                                                                         \n",
    "    return tokens\n",
    "\n",
    "def lemmatize(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    tokens = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        sent_tokens = []\n",
    "        for token in doc: \n",
    "            if \"_\" in token.text:\n",
    "                sent_tokens.append(token.text)\n",
    "            else:\n",
    "                if token.pos_ in allowed_postags:\n",
    "                    sent_tokens.append(token.lemma_)\n",
    "                    \n",
    "        sent_tokens = [token.replace(\"datum\", \"data\") for token in sent_tokens]\n",
    "        tokens.append(sent_tokens)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating non-lemmatized corpus (only for testing, no longer used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_with_bigrams = [clean_with_spacy(doc) for doc in corpus]\n",
    "# sum = 0\n",
    "# for doc in corpus_with_bigrams:\n",
    "#     sum += len(doc)\n",
    "\n",
    "# print(f\"Total number of words in the cleaned corpus: {sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(corpus_with_bigrams[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary = Dictionary(corpus_with_bigrams)\n",
    "# doc_term_matrix = [dictionary.doc2bow(doc) for doc in corpus_with_bigrams]\n",
    "# print(doc_term_matrix[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating lemmatized corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the cleaned corpus: 36345\n"
     ]
    }
   ],
   "source": [
    "bag_of_words_list = [clean_without_ngrams(doc) for doc in strings_list]\n",
    "\n",
    "bigram = Phrases(bag_of_words_list, min_count=10, threshold=20) \n",
    "bigram_mod = Phraser(bigram)    # For speed\n",
    "\n",
    "# Add bigrams\n",
    "bag_of_words_list = [bigram_mod[doc] for doc in bag_of_words_list]\n",
    "\n",
    "# Lemmatize the words, exluding bigrams\n",
    "bag_of_words_list = lemmatize(bag_of_words_list)\n",
    "\n",
    "sum = 0\n",
    "for doc in bag_of_words_list:\n",
    "    sum += len(doc)\n",
    "\n",
    "print(f\"Total number of words in the cleaned corpus: {sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'collection', 'cleaning', 'assist', 'collection', 'organization', 'data', 'data', 'clean', 'preprocessing', 'activity', 'ensure', 'data', 'accuracy', 'data', 'analysis', 'utilize', 'statistical', 'method', 'tool', 'assist', 'analysis', 'dataset', 'gsp', 'team_members', 'identify', 'trends_patterns', 'data', 'data', 'visualization', 'support', 'creation', 'visualization', 'report', 'communicate', 'data', 'finding', 'collaboration', 'collaborate', 'team_members', 'data', 'requirement', 'provide', 'support', 'deliver', 'analytical', 'learn', 'experienced', 'team_members', 'actively', 'seek', 'guidance', 'generation', 'learn', 'summarize', 'communicatedatainsight', 'clear', 'understandable', 'learning', 'actively', 'participate', 'training', 'development', 'opportunity', 'enhance', 'skill', 'job', 'bachelor_degree', 'relevant', 'field', 'mathematic', 'computer_science', 'equivalent', 'basic', 'understanding', 'data', 'analysis', 'concept', 'methodology', 'familiarity', 'data', 'analysis', 'tool', 'power_bi', 'excel', 'attention_detail', 'analytical', 'skill', 'good', 'communication_skills', 'ability', 'collaboratively', 'team', 'eagerness', 'learn', 'adapt', 'new', 'technology', 'technique']\n"
     ]
    }
   ],
   "source": [
    "print(bag_of_words_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 2), (3, 1), (4, 1), (5, 4), (6, 2), (7, 2), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 2), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 11), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 3), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 2), (60, 1), (61, 1), (62, 2), (63, 1), (64, 3), (65, 1), (66, 1), (67, 2), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 2)]\n"
     ]
    }
   ],
   "source": [
    "dictionary = Dictionary(bag_of_words_list)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in bag_of_words_list]\n",
    "print(doc_term_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2764\n"
     ]
    }
   ],
   "source": [
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just out of curiosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.022*development + 0.009*team + 0.009*software + 0.009*participate + 0.008*engineering + 0.008*data + 0.008*system + 0.008*experience + 0.008*year + 0.007*good'),\n",
       " (1,\n",
       "  '0.017*data + 0.015*model + 0.012*machine_learning + 0.007*degree + 0.007*language + 0.006*team + 0.005*ability + 0.005*processing + 0.005*high + 0.004*natural_language'),\n",
       " (2,\n",
       "  '0.013*technical + 0.011*ai + 0.009*develop + 0.009*design + 0.009*year + 0.009*model + 0.008*test + 0.007*machine_learning + 0.007*data + 0.007*engineer'),\n",
       " (3,\n",
       "  '0.016*model + 0.014*machine_learning + 0.011*data + 0.007*language + 0.007*degree + 0.006*team + 0.005*ability + 0.005*high + 0.005*nlp + 0.005*processing'),\n",
       " (4,\n",
       "  '0.006*data + 0.003*business + 0.003*develop + 0.003*mapreduce + 0.003*participate + 0.003*class + 0.002*technology + 0.002*structured + 0.002*cassandra + 0.002*public'),\n",
       " (5,\n",
       "  '0.006*data + 0.003*tensorflow + 0.003*confidential + 0.002*process + 0.002*apache_spark + 0.002*pipeline + 0.002*technology + 0.002*bonus + 0.002*journey + 0.002*continent'),\n",
       " (6,\n",
       "  '0.007*data + 0.004*model + 0.004*affirm + 0.003*business + 0.003*documentation + 0.002*write + 0.002*incorporate + 0.002*machine_learning + 0.002*pipeline + 0.002*sustain'),\n",
       " (7,\n",
       "  '0.004*data + 0.003*excited + 0.003*prerequisite + 0.002*measurable + 0.002*suite + 0.002*governance + 0.002*clone + 0.002*augment + 0.002*manner + 0.002*sourced'),\n",
       " (8,\n",
       "  '0.004*data + 0.003*administration + 0.002*rapidly + 0.002*team + 0.002*functional_teams + 0.002*wireless + 0.002*department + 0.002*unafraid + 0.002*fluently + 0.002*custom'),\n",
       " (9,\n",
       "  '0.004*data + 0.003*hugging + 0.003*real_world + 0.003*scale + 0.002*timeserie + 0.002*pro + 0.002*owner + 0.002*accredit + 0.002*daily + 0.002*comfort'),\n",
       " (10,\n",
       "  '0.003*solve + 0.003*output + 0.002*combine + 0.002*protect + 0.002*idea + 0.002*labor + 0.002*data + 0.002*sustained + 0.002*apprise + 0.002*demos'),\n",
       " (11,\n",
       "  '0.003*listen + 0.003*ecobee + 0.003*data + 0.002*solver + 0.002*concurrent + 0.002*refining + 0.002*luigi + 0.002*personalized + 0.002*technical + 0.002*ability_independently'),\n",
       " (12,\n",
       "  '0.006*data + 0.003*evolving + 0.003*versioning + 0.003*onboarding + 0.003*iteratively + 0.002*administration + 0.002*ifrs + 0.002*member + 0.002*update + 0.002*currently'),\n",
       " (13,\n",
       "  '0.002*distribution + 0.002*incremental + 0.002*data + 0.002*codepipeline + 0.002*structured + 0.002*external + 0.002*framework + 0.002*fast + 0.002*experienced + 0.002*prescriptive'),\n",
       " (14,\n",
       "  '0.006*data + 0.002*prospect + 0.002*optimization + 0.002*unstructure + 0.002*programming_language + 0.002*experience + 0.002*superior + 0.002*performance + 0.002*flink + 0.002*writing')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import HdpModel\n",
    "# from pprint import pprint\n",
    "\n",
    "hdp_model = HdpModel(doc_term_matrix, id2word = dictionary)\n",
    "hdp_model.optimal_ordering()\n",
    "hdp_model.show_topics(num_topics = 15, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, beta = hdp_model.hdp_to_lda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150,)\n",
      "(150, 2764)\n"
     ]
    }
   ],
   "source": [
    "print(alpha.shape)\n",
    "print(beta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling with lemmatized corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 3\n",
    "PATH_TO_MODEL = f\"Bigrams_All_Jobs_LDA_{NUM_TOPICS}_topics\"\n",
    "lda_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_3 = LdaModel(doc_term_matrix, num_topics = NUM_TOPICS, id2word = dictionary, alpha = 'auto', eta = 'auto', passes = 50, random_state=448)\n",
    "lda_model_3.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "lda_model_3.save(datapath('Jobs_Chosen_3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared = gensim_vis.prepare(lda_model_3, doc_term_matrix, dictionary, mds='pcoa')\n",
    "# pyLDAvis.display(LDAvis_prepared)\n",
    "# pyLDAvis.save_html(LDAvis_prepared, f'Jobs_LDA_{NUM_TOPICS}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.082*\"data\" + 0.016*\"tool\" + 0.012*\"report\" + 0.011*\"skill\" + 0.011*\"analysis\" + 0.010*\"ability\" + 0.009*\"pipeline\" + 0.009*\"analytical\" + 0.007*\"optimize\" + 0.007*\"process\"'),\n",
       " (1,\n",
       "  '0.065*\"data\" + 0.013*\"model\" + 0.010*\"business\" + 0.010*\"team\" + 0.009*\"pipeline\" + 0.008*\"skill\" + 0.008*\"ensure\" + 0.008*\"system\" + 0.008*\"engineering\" + 0.008*\"tool\"'),\n",
       " (2,\n",
       "  '0.031*\"model\" + 0.017*\"ai\" + 0.012*\"data\" + 0.011*\"year\" + 0.009*\"machine_learning\" + 0.008*\"ml\" + 0.008*\"work\" + 0.007*\"deploy\" + 0.007*\"skill\" + 0.007*\"code\"'),\n",
       " (3,\n",
       "  '0.026*\"model\" + 0.025*\"machine_learning\" + 0.013*\"ai\" + 0.013*\"research\" + 0.011*\"team\" + 0.010*\"develop\" + 0.010*\"engineering\" + 0.008*\"product\" + 0.008*\"system\" + 0.008*\"data\"')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model = LdaModel(doc_term_matrix, num_topics = NUM_TOPICS, id2word = dictionary, alpha = 'auto', eta = 'auto', passes = 50, random_state=448)\n",
    "lda_model.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared = gensim_vis.prepare(lda_model, doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, f'Jobs_LDA_{NUM_TOPICS}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.071*\"data\" + 0.014*\"tool\" + 0.013*\"pipeline\" + 0.010*\"optimize\" + 0.010*\"performance\" + 0.009*\"query\" + 0.009*\"governance\" + 0.009*\"best_practices\" + 0.008*\"engineer\" + 0.008*\"gcp\"'),\n",
       " (1,\n",
       "  '0.074*\"data\" + 0.013*\"pipeline\" + 0.009*\"ensure\" + 0.008*\"engineering\" + 0.008*\"business\" + 0.008*\"analytic\" + 0.007*\"system\" + 0.007*\"azure\" + 0.006*\"tool\" + 0.006*\"support\"'),\n",
       " (2,\n",
       "  '0.030*\"model\" + 0.023*\"ai\" + 0.013*\"machine_learning\" + 0.011*\"year\" + 0.008*\"algorithm\" + 0.007*\"ml\" + 0.007*\"code\" + 0.007*\"deploy\" + 0.006*\"system\" + 0.006*\"deep_learning\"'),\n",
       " (3,\n",
       "  '0.032*\"model\" + 0.024*\"machine_learning\" + 0.012*\"research\" + 0.010*\"engineering\" + 0.010*\"ai\" + 0.009*\"product\" + 0.006*\"team\" + 0.006*\"technical\" + 0.006*\"develop\" + 0.006*\"deep_learning\"'),\n",
       " (4,\n",
       "  '0.048*\"data\" + 0.018*\"analysis\" + 0.015*\"business\" + 0.012*\"report\" + 0.011*\"ability\" + 0.010*\"tool\" + 0.009*\"statistical\" + 0.009*\"visualization\" + 0.008*\"skill\" + 0.008*\"client\"')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model_5 = LdaModel(doc_term_matrix, num_topics = NUM_TOPICS, id2word = dictionary, alpha = 'auto', eta = 'auto', passes = 50, random_state=448)\n",
    "lda_model_5.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_5.save(datapath('Jobs_Chosen_5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared = gensim_vis.prepare(lda_model_5, doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, f'Jobs_LDA_{NUM_TOPICS}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words = []\n",
    "for i in range(1,NUM_TOPICS+1): \n",
    "    topic = {}   \n",
    "    topic[i] = LDAvis_prepared.sorted_terms(topic = i, _lambda = 0.67)[:30][\"Term\"].tolist()\n",
    "    topic_words.append(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: ['data', 'pipeline', 'ensure', 'analytic', 'azure', 'engineering', 'business', 'system', 'support', 'process', 'maintain', 'tool', 'database', 'design', 'science', 'service', 'technology', 'etl', 'manage', 'processing', 'project', 'engineer', 'skill', 'include', 'aw', 'management', 'scientist', 'architecture', 'technical', 'performance']}\n",
      "{2: ['model', 'machine_learning', 'research', 'engineering', 'product', 'ai', 'deep_learning', 'impact', 'llm', 'develop', 'problem', 'team', 'technical', 'fraud', 'algorithm', 'phd', 'development', 'framework', 'include', 'technique', 'high', 'ml', 'proficiency', 'domain', 'new', 'degree', 'production', 'platform', 'language', 'risk']}\n",
      "{3: ['model', 'ai', 'year', 'machine_learning', 'algorithm', 'ml', 'deploy', 'code', 'deep_learning', 'training', 'feature', 'image', 'mlop', 'deployment', 'infrastructure', 'industrial', 'technique', 'project', 'experiment', 'system', 'llm', 'improve', 'agent', 'design', 'development', 'real_world', 'generative_ai', 'solve', 'performance', 'integrate']}\n",
      "{4: ['data', 'analysis', 'report', 'business', 'statistical', 'visualization', 'ability', 'finding', 'analyze', 'client', 'analytical', 'tool', 'skill', 'stakeholder', 'excel', 'identify', 'power_bi', 'different', 'attention_detail', 'team_members', 'team', 'actionable', 'communicate', 'assist', 'able', 'dataset', 'provide', 'perform', 'trend', 'support']}\n",
      "{5: ['data', 'governance', 'tool', 'cloud_composer', 'query', 'gcp_services', 'pipeline', 'gcp', 'cost', 'etl_elt', 'optimize', 'performance', 'best_practices', 'retailer', 'engineer', 'schema', 'integration', 'modeling', 'function', 'work', 'pub', 'sub', 'security', 'large_datasets', 'storage', 'report', 'quality', 'dataflow', 'reduce', 'share']}\n"
     ]
    }
   ],
   "source": [
    "for i in range(NUM_TOPICS):\n",
    "    print(topic_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.049*\"data\" + 0.015*\"report\" + 0.008*\"warehouse\" + 0.007*\"snowflake\" + 0.007*\"stakeholder\" + 0.007*\"pipeline\" + 0.007*\"visualization\" + 0.006*\"tool\" + 0.006*\"create\" + 0.006*\"work\"'),\n",
       " (1,\n",
       "  '0.062*\"data\" + 0.014*\"business\" + 0.010*\"system\" + 0.009*\"team\" + 0.008*\"skill\" + 0.008*\"analytic\" + 0.007*\"analysis\" + 0.007*\"support\" + 0.007*\"design\" + 0.006*\"develop\"'),\n",
       " (2,\n",
       "  '0.024*\"model\" + 0.020*\"ai\" + 0.010*\"machine_learning\" + 0.009*\"design\" + 0.009*\"team\" + 0.009*\"large\" + 0.008*\"develop\" + 0.008*\"language\" + 0.008*\"project\" + 0.008*\"include\"'),\n",
       " (3,\n",
       "  '0.024*\"model\" + 0.020*\"machine_learning\" + 0.019*\"research\" + 0.016*\"ai\" + 0.010*\"development\" + 0.010*\"engineering\" + 0.009*\"system\" + 0.007*\"software\" + 0.007*\"team\" + 0.007*\"deep_learning\"'),\n",
       " (4,\n",
       "  '0.071*\"data\" + 0.015*\"skill\" + 0.015*\"report\" + 0.014*\"analysis\" + 0.012*\"team\" + 0.011*\"ability\" + 0.010*\"analyze\" + 0.010*\"analytical\" + 0.008*\"client\" + 0.007*\"tool\"'),\n",
       " (5,\n",
       "  '0.054*\"data\" + 0.029*\"model\" + 0.017*\"machine_learning\" + 0.013*\"pipeline\" + 0.010*\"engineering\" + 0.009*\"tool\" + 0.009*\"team\" + 0.009*\"performance\" + 0.009*\"engineer\" + 0.008*\"skill\"')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model = LdaModel(doc_term_matrix, num_topics = NUM_TOPICS, id2word = dictionary, alpha = 'auto', eta = 'auto', passes = 50, random_state=448)\n",
    "lda_model.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared = gensim_vis.prepare(lda_model, doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, f'Jobs_LDA_{NUM_TOPICS}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.042*\"data\" + 0.010*\"report\" + 0.009*\"warehouse\" + 0.009*\"snowflake\" + 0.007*\"tool\" + 0.006*\"trend\" + 0.006*\"intern\" + 0.006*\"create\" + 0.005*\"visualization\" + 0.005*\"develop\"'),\n",
       " (1,\n",
       "  '0.067*\"data\" + 0.011*\"business\" + 0.010*\"team\" + 0.010*\"skill\" + 0.008*\"analysis\" + 0.008*\"ability\" + 0.008*\"tool\" + 0.007*\"analytic\" + 0.007*\"design\" + 0.007*\"system\"'),\n",
       " (2,\n",
       "  '0.022*\"model\" + 0.020*\"ai\" + 0.011*\"large\" + 0.010*\"data\" + 0.010*\"knowledge\" + 0.008*\"aw\" + 0.008*\"llm\" + 0.008*\"include\" + 0.008*\"skill\" + 0.007*\"year\"'),\n",
       " (3,\n",
       "  '0.024*\"model\" + 0.022*\"machine_learning\" + 0.019*\"ai\" + 0.017*\"research\" + 0.011*\"system\" + 0.010*\"development\" + 0.009*\"team\" + 0.009*\"engineering\" + 0.009*\"deep_learning\" + 0.008*\"develop\"'),\n",
       " (4,\n",
       "  '0.058*\"data\" + 0.014*\"report\" + 0.014*\"team\" + 0.012*\"client\" + 0.011*\"skill\" + 0.011*\"business\" + 0.010*\"analysis\" + 0.009*\"support\" + 0.009*\"analytical\" + 0.007*\"different\"'),\n",
       " (5,\n",
       "  '0.053*\"data\" + 0.030*\"model\" + 0.017*\"machine_learning\" + 0.014*\"pipeline\" + 0.010*\"engineering\" + 0.009*\"tool\" + 0.009*\"team\" + 0.009*\"engineer\" + 0.009*\"performance\" + 0.008*\"develop\"'),\n",
       " (6,\n",
       "  '0.041*\"data\" + 0.013*\"system\" + 0.013*\"user\" + 0.012*\"pipeline\" + 0.011*\"machine_learning\" + 0.011*\"business\" + 0.011*\"analytic\" + 0.010*\"contribute\" + 0.009*\"engineer\" + 0.009*\"good\"')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model = LdaModel(doc_term_matrix, num_topics = NUM_TOPICS, id2word = dictionary, alpha = 'auto', eta = 'auto', passes = 50, random_state=448)\n",
    "lda_model.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared = gensim_vis.prepare(lda_model, doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, f'Jobs_LDA_{NUM_TOPICS}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.047*\"data\" + 0.017*\"report\" + 0.011*\"analytical\" + 0.008*\"tool\" + 0.007*\"dashboard\" + 0.007*\"visualization\" + 0.007*\"extract\" + 0.007*\"key\" + 0.007*\"channel\" + 0.007*\"warehouse\"'),\n",
       " (1,\n",
       "  '0.063*\"data\" + 0.012*\"business\" + 0.010*\"team\" + 0.008*\"design\" + 0.008*\"skill\" + 0.008*\"analytic\" + 0.008*\"analysis\" + 0.008*\"system\" + 0.007*\"development\" + 0.007*\"tool\"'),\n",
       " (2,\n",
       "  '0.018*\"model\" + 0.018*\"ai\" + 0.012*\"large\" + 0.012*\"data\" + 0.011*\"year\" + 0.011*\"infrastructure\" + 0.010*\"system\" + 0.010*\"llm\" + 0.008*\"machine_learning\" + 0.008*\"develop\"'),\n",
       " (3,\n",
       "  '0.024*\"ai\" + 0.023*\"model\" + 0.017*\"machine_learning\" + 0.016*\"development\" + 0.016*\"research\" + 0.012*\"team\" + 0.011*\"llm\" + 0.010*\"high\" + 0.009*\"deep_learning\" + 0.008*\"system\"'),\n",
       " (4,\n",
       "  '0.087*\"data\" + 0.019*\"report\" + 0.019*\"skill\" + 0.017*\"analysis\" + 0.013*\"ability\" + 0.012*\"team\" + 0.011*\"analyze\" + 0.010*\"analytical\" + 0.010*\"support\" + 0.010*\"process\"'),\n",
       " (5,\n",
       "  '0.053*\"data\" + 0.030*\"model\" + 0.018*\"machine_learning\" + 0.014*\"pipeline\" + 0.010*\"engineering\" + 0.010*\"team\" + 0.009*\"performance\" + 0.009*\"tool\" + 0.009*\"engineer\" + 0.008*\"skill\"'),\n",
       " (6,\n",
       "  '0.038*\"data\" + 0.016*\"system\" + 0.016*\"user\" + 0.014*\"machine_learning\" + 0.013*\"business\" + 0.013*\"analytic\" + 0.012*\"contribute\" + 0.011*\"content\" + 0.010*\"personalize\" + 0.010*\"infrastructure\"'),\n",
       " (7,\n",
       "  '0.026*\"data\" + 0.023*\"research\" + 0.023*\"model\" + 0.020*\"machine_learning\" + 0.010*\"knowledge\" + 0.009*\"aw\" + 0.008*\"engineering\" + 0.008*\"develop\" + 0.007*\"pipeline\" + 0.007*\"visual\"'),\n",
       " (8,\n",
       "  '0.036*\"model\" + 0.020*\"ai\" + 0.014*\"machine_learning\" + 0.010*\"ml\" + 0.009*\"deep_learning\" + 0.007*\"build\" + 0.007*\"new\" + 0.007*\"mining\" + 0.007*\"include\" + 0.006*\"year\"'),\n",
       " (9,\n",
       "  '0.022*\"technical\" + 0.014*\"ensure\" + 0.013*\"data\" + 0.013*\"team\" + 0.013*\"engineering\" + 0.012*\"system\" + 0.011*\"domain\" + 0.011*\"impact\" + 0.010*\"ecobee\" + 0.009*\"significant\"')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model = LdaModel(doc_term_matrix, num_topics = NUM_TOPICS, id2word = dictionary, alpha = 'auto', eta = 'auto', passes = 50, random_state=448)\n",
    "lda_model.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared = gensim_vis.prepare(lda_model, doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, f'Jobs_LDA_{NUM_TOPICS}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.025*\"data\" + 0.019*\"research\" + 0.014*\"report\" + 0.012*\"intern\" + 0.011*\"trend\" + 0.009*\"visualization\" + 0.009*\"collaborate\" + 0.009*\"dataset\" + 0.009*\"generate\" + 0.009*\"written_verbal\"'),\n",
       " (1,\n",
       "  '0.037*\"data\" + 0.011*\"team\" + 0.010*\"business\" + 0.010*\"project\" + 0.009*\"design\" + 0.008*\"technical\" + 0.008*\"software\" + 0.008*\"skill\" + 0.008*\"model\" + 0.007*\"system\"'),\n",
       " (2,\n",
       "  '0.038*\"ai\" + 0.025*\"model\" + 0.016*\"llm\" + 0.012*\"development\" + 0.010*\"system\" + 0.010*\"knowledge\" + 0.010*\"infrastructure\" + 0.010*\"year\" + 0.010*\"include\" + 0.009*\"machine_learning\"'),\n",
       " (3,\n",
       "  '0.023*\"model\" + 0.012*\"team\" + 0.011*\"machine_learning\" + 0.011*\"develop\" + 0.011*\"research\" + 0.010*\"deep_learning\" + 0.010*\"ai\" + 0.009*\"year\" + 0.008*\"language\" + 0.008*\"project\"'),\n",
       " (4,\n",
       "  '0.085*\"data\" + 0.022*\"report\" + 0.021*\"analysis\" + 0.021*\"skill\" + 0.014*\"team\" + 0.014*\"analyze\" + 0.013*\"ability\" + 0.013*\"visualization\" + 0.013*\"analytical\" + 0.012*\"business\"'),\n",
       " (5,\n",
       "  '0.037*\"model\" + 0.030*\"data\" + 0.022*\"machine_learning\" + 0.010*\"engineering\" + 0.010*\"ai\" + 0.009*\"team\" + 0.008*\"product\" + 0.008*\"develop\" + 0.008*\"performance\" + 0.008*\"pipeline\"'),\n",
       " (6,\n",
       "  '0.043*\"data\" + 0.015*\"system\" + 0.015*\"user\" + 0.015*\"machine_learning\" + 0.015*\"business\" + 0.015*\"analytic\" + 0.012*\"store\" + 0.011*\"processing\" + 0.011*\"personalize\" + 0.011*\"pipeline\"'),\n",
       " (7,\n",
       "  '0.039*\"data\" + 0.010*\"team\" + 0.010*\"knowledge\" + 0.010*\"aw\" + 0.009*\"work\" + 0.009*\"model\" + 0.009*\"pipeline\" + 0.008*\"analysis\" + 0.008*\"validation\" + 0.007*\"fraud\"'),\n",
       " (8,\n",
       "  '0.029*\"data\" + 0.025*\"model\" + 0.020*\"machine_learning\" + 0.019*\"fraud\" + 0.018*\"product\" + 0.016*\"engineering\" + 0.016*\"team\" + 0.016*\"develop\" + 0.016*\"requirement\" + 0.015*\"affirm\"'),\n",
       " (9,\n",
       "  '0.021*\"research\" + 0.019*\"machine_learning\" + 0.017*\"model\" + 0.015*\"engineering\" + 0.014*\"data\" + 0.013*\"system\" + 0.012*\"development\" + 0.010*\"impact\" + 0.009*\"significant\" + 0.009*\"technical\"'),\n",
       " (10,\n",
       "  '0.015*\"channel\" + 0.011*\"code\" + 0.010*\"mining\" + 0.010*\"performance\" + 0.010*\"analytical\" + 0.009*\"model\" + 0.009*\"data\" + 0.009*\"key\" + 0.009*\"pay\" + 0.008*\"recommendation\"'),\n",
       " (11,\n",
       "  '0.124*\"data\" + 0.021*\"pipeline\" + 0.015*\"tool\" + 0.011*\"ensure\" + 0.011*\"process\" + 0.009*\"skill\" + 0.009*\"performance\" + 0.009*\"best_practices\" + 0.008*\"design\" + 0.008*\"azure\"'),\n",
       " (12,\n",
       "  '0.014*\"year\" + 0.013*\"model\" + 0.012*\"retailer\" + 0.012*\"algorithm\" + 0.010*\"machine_learning\" + 0.009*\"new\" + 0.009*\"vision\" + 0.009*\"machine\" + 0.009*\"software\" + 0.008*\"product\"')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model = LdaModel(doc_term_matrix, num_topics = NUM_TOPICS, id2word = dictionary, alpha = 'auto', eta = 'auto', passes = 50, random_state=448)\n",
    "lda_model.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared = gensim_vis.prepare(lda_model, doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, f'Jobs_LDA_{NUM_TOPICS}.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying TF-IDF Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "# Duplicating these to avoid modifying the originals\n",
    "tf_corpus = doc_term_matrix\n",
    "tf_dictionary = dictionary\n",
    "\n",
    "tfidf = TfidfModel(corpus=tf_corpus, id2word=tf_dictionary)\n",
    "\n",
    "low_value = 0.03\n",
    "words  = []\n",
    "words_missing_in_tfidf = []\n",
    "for i in range(0, len(tf_corpus)):\n",
    "    bow = tf_corpus[i]\n",
    "    low_value_words = [] #reinitialize to be safe. You can skip this.\n",
    "    tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "    bow_ids = [id for id, value in bow]\n",
    "    low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "    drops = low_value_words+words_missing_in_tfidf\n",
    "    for item in drops:\n",
    "        words.append(tf_dictionary[item])\n",
    "    words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] # The words with tf-idf score 0 will be missing\n",
    "\n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "    tf_corpus[i] = new_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.069*\"data\" + 0.013*\"analysis\" + 0.012*\"tool\" + 0.009*\"ability\" + 0.009*\"report\" + 0.009*\"business\" + 0.008*\"process\" + 0.007*\"analytic\" + 0.007*\"analytical\" + 0.006*\"query\"'),\n",
       " (1,\n",
       "  '0.071*\"data\" + 0.011*\"pipeline\" + 0.008*\"ensure\" + 0.008*\"engineering\" + 0.008*\"business\" + 0.007*\"system\" + 0.006*\"team\" + 0.006*\"analytic\" + 0.006*\"azure\" + 0.006*\"tool\"'),\n",
       " (2,\n",
       "  '0.033*\"model\" + 0.020*\"machine_learning\" + 0.015*\"ai\" + 0.008*\"research\" + 0.007*\"algorithm\" + 0.007*\"product\" + 0.006*\"technique\" + 0.006*\"engineering\" + 0.006*\"ml\" + 0.006*\"deep_learning\"')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_lda_model_3 = LdaModel(corpus=tf_corpus, id2word=tf_dictionary, num_topics=3, random_state=448, passes=50, alpha=\"auto\", eta = \"auto\")\n",
    "idf_lda_model_3.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared = gensim_vis.prepare(idf_lda_model_3, doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, 'Jobs_IDF_LDA_3.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.069*\"data\" + 0.013*\"tool\" + 0.010*\"pipeline\" + 0.009*\"report\" + 0.009*\"performance\" + 0.008*\"query\" + 0.007*\"ability\" + 0.007*\"optimize\" + 0.007*\"best_practices\" + 0.007*\"governance\"'),\n",
       " (1,\n",
       "  '0.081*\"data\" + 0.011*\"business\" + 0.010*\"pipeline\" + 0.008*\"analytic\" + 0.008*\"ensure\" + 0.007*\"analysis\" + 0.007*\"tool\" + 0.007*\"engineering\" + 0.007*\"process\" + 0.007*\"support\"'),\n",
       " (2,\n",
       "  '0.035*\"model\" + 0.017*\"ai\" + 0.017*\"machine_learning\" + 0.008*\"algorithm\" + 0.007*\"deploy\" + 0.007*\"year\" + 0.007*\"performance\" + 0.007*\"technique\" + 0.007*\"deployment\" + 0.006*\"tool\"'),\n",
       " (3,\n",
       "  '0.024*\"model\" + 0.018*\"machine_learning\" + 0.010*\"ai\" + 0.010*\"research\" + 0.009*\"engineering\" + 0.008*\"product\" + 0.006*\"team\" + 0.005*\"develop\" + 0.005*\"development\" + 0.005*\"include\"')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_lda_model = LdaModel(corpus=tf_corpus, id2word=tf_dictionary, num_topics=4, random_state=448, passes=50, alpha=\"auto\", eta = \"auto\")\n",
    "idf_lda_model.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared = gensim_vis.prepare(idf_lda_model, doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, 'Jobs_IDF_LDA_4.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.071*\"data\" + 0.014*\"tool\" + 0.013*\"pipeline\" + 0.010*\"optimize\" + 0.010*\"performance\" + 0.009*\"query\" + 0.009*\"governance\" + 0.009*\"best_practices\" + 0.008*\"engineer\" + 0.008*\"gcp\"'),\n",
       " (1,\n",
       "  '0.074*\"data\" + 0.013*\"pipeline\" + 0.009*\"ensure\" + 0.008*\"engineering\" + 0.008*\"business\" + 0.008*\"analytic\" + 0.007*\"system\" + 0.007*\"azure\" + 0.006*\"tool\" + 0.006*\"support\"'),\n",
       " (2,\n",
       "  '0.030*\"model\" + 0.023*\"ai\" + 0.013*\"machine_learning\" + 0.011*\"year\" + 0.008*\"algorithm\" + 0.007*\"ml\" + 0.007*\"code\" + 0.007*\"deploy\" + 0.006*\"system\" + 0.006*\"deep_learning\"'),\n",
       " (3,\n",
       "  '0.032*\"model\" + 0.024*\"machine_learning\" + 0.012*\"research\" + 0.010*\"engineering\" + 0.010*\"ai\" + 0.009*\"product\" + 0.006*\"team\" + 0.006*\"technical\" + 0.006*\"develop\" + 0.006*\"deep_learning\"'),\n",
       " (4,\n",
       "  '0.048*\"data\" + 0.018*\"analysis\" + 0.015*\"business\" + 0.012*\"report\" + 0.011*\"ability\" + 0.010*\"tool\" + 0.009*\"statistical\" + 0.009*\"visualization\" + 0.008*\"skill\" + 0.008*\"client\"')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_lda_model_5 = LdaModel(corpus=tf_corpus, id2word=tf_dictionary, num_topics=5, random_state=448, passes=50, alpha=\"auto\", eta = \"auto\")\n",
    "idf_lda_model_5.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared = gensim_vis.prepare(idf_lda_model_5, doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, 'Jobs_IDF_LDA_5.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.088*\"data\" + 0.019*\"pipeline\" + 0.017*\"tool\" + 0.013*\"governance\" + 0.012*\"optimize\" + 0.011*\"performance\" + 0.010*\"query\" + 0.010*\"engineer\" + 0.009*\"best_practices\" + 0.008*\"quality\"'),\n",
       " (1,\n",
       "  '0.079*\"data\" + 0.013*\"business\" + 0.010*\"analytic\" + 0.008*\"system\" + 0.008*\"pipeline\" + 0.007*\"support\" + 0.007*\"design\" + 0.007*\"database\" + 0.006*\"ensure\" + 0.006*\"process\"'),\n",
       " (2,\n",
       "  '0.023*\"model\" + 0.021*\"ai\" + 0.008*\"training\" + 0.008*\"deep_learning\" + 0.008*\"system\" + 0.008*\"tool\" + 0.007*\"infrastructure\" + 0.007*\"ml\" + 0.007*\"image\" + 0.006*\"large\"'),\n",
       " (3,\n",
       "  '0.018*\"research\" + 0.017*\"model\" + 0.015*\"machine_learning\" + 0.014*\"ai\" + 0.008*\"engineering\" + 0.008*\"development\" + 0.007*\"team\" + 0.007*\"system\" + 0.006*\"impact\" + 0.006*\"technical\"'),\n",
       " (4,\n",
       "  '0.073*\"data\" + 0.017*\"analysis\" + 0.016*\"report\" + 0.014*\"ability\" + 0.012*\"analyze\" + 0.010*\"visualization\" + 0.010*\"skill\" + 0.008*\"tool\" + 0.008*\"analytical\" + 0.008*\"statistical\"'),\n",
       " (5,\n",
       "  '0.035*\"model\" + 0.021*\"machine_learning\" + 0.010*\"engineering\" + 0.010*\"ai\" + 0.009*\"product\" + 0.008*\"technique\" + 0.007*\"science\" + 0.007*\"algorithm\" + 0.007*\"data\" + 0.007*\"business\"')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_lda_model = LdaModel(corpus=tf_corpus, id2word=tf_dictionary, num_topics=6, random_state=448, passes=50, alpha=\"auto\", eta = \"auto\")\n",
    "idf_lda_model.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared = gensim_vis.prepare(idf_lda_model, doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, 'Jobs_IDF_LDA_6.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.101*\"data\" + 0.026*\"pipeline\" + 0.017*\"tool\" + 0.013*\"optimize\" + 0.012*\"governance\" + 0.012*\"query\" + 0.012*\"performance\" + 0.011*\"engineer\" + 0.010*\"best_practices\" + 0.009*\"gcp\"'),\n",
       " (1,\n",
       "  '0.088*\"data\" + 0.010*\"business\" + 0.009*\"analysis\" + 0.009*\"analytic\" + 0.007*\"process\" + 0.007*\"ensure\" + 0.007*\"design\" + 0.007*\"support\" + 0.007*\"database\" + 0.007*\"skill\"'),\n",
       " (2,\n",
       "  '0.026*\"model\" + 0.018*\"ai\" + 0.009*\"ml\" + 0.009*\"tool\" + 0.009*\"large\" + 0.008*\"aw\" + 0.008*\"training\" + 0.008*\"deep_learning\" + 0.008*\"project\" + 0.006*\"year\"'),\n",
       " (3,\n",
       "  '0.018*\"research\" + 0.018*\"model\" + 0.018*\"machine_learning\" + 0.016*\"ai\" + 0.008*\"system\" + 0.008*\"engineering\" + 0.007*\"development\" + 0.007*\"deep_learning\" + 0.006*\"include\" + 0.006*\"llm\"'),\n",
       " (4,\n",
       "  '0.037*\"data\" + 0.015*\"report\" + 0.013*\"team\" + 0.012*\"business\" + 0.011*\"client\" + 0.009*\"analysis\" + 0.008*\"analytical\" + 0.007*\"ability\" + 0.007*\"support\" + 0.007*\"visualization\"'),\n",
       " (5,\n",
       "  '0.036*\"model\" + 0.021*\"machine_learning\" + 0.010*\"ai\" + 0.010*\"engineering\" + 0.009*\"product\" + 0.008*\"algorithm\" + 0.007*\"technique\" + 0.007*\"pipeline\" + 0.007*\"science\" + 0.007*\"performance\"'),\n",
       " (6,\n",
       "  '0.037*\"data\" + 0.013*\"system\" + 0.013*\"user\" + 0.011*\"pipeline\" + 0.011*\"analytic\" + 0.010*\"business\" + 0.010*\"machine_learning\" + 0.010*\"contribute\" + 0.009*\"good\" + 0.009*\"engineer\"')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_lda_model = LdaModel(corpus=tf_corpus, id2word=tf_dictionary, num_topics=7, random_state=448, passes=50, alpha=\"auto\", eta = \"auto\")\n",
    "idf_lda_model.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared = gensim_vis.prepare(idf_lda_model, doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, 'Jobs_IDF_LDA_7.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, we go with the 3 topic and 5 topic models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some interesting words that might not be in the lectures dictionary could be: deploy, pipeline, etl, llm, power_bi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding words in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "\n",
    "for item in dictionary.items():\n",
    "    vocab.append(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ability', 'accuracy', 'actively', 'activity', 'adapt', 'analysis', 'analytical', 'assist', 'attention_detail', 'bachelor_degree']\n",
      "2764\n"
     ]
    }
   ],
   "source": [
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploy: [(1, 0.0022688073), (2, 0.0070800367), (3, 0.001913702)]\n",
      "pipeline: [(0, 0.012908377), (1, 0.012512266), (2, 0.0019316918)]\n",
      "etl: [(1, 0.0040628584)]\n",
      "llm: [(2, 0.0043812576), (3, 0.0054251635)]\n",
      "power_bi: [(4, 0.004621462)]\n",
      "generative_ai: [(2, 0.0031441827), (3, 0.0017050924)]\n",
      "gcp: [(0, 0.007744878), (1, 0.0016145506), (2, 0.0011161047)]\n",
      "spark: [(1, 0.0027215357), (3, 0.0017538423)]\n",
      "hadoop: [(1, 0.0012411025)]\n",
      "git: []\n",
      "training: [(2, 0.0050621596), (3, 0.0017649262), (4, 0.0023221765)]\n",
      "simulation: []\n",
      "probability: []\n",
      "regression: []\n",
      "database: [(1, 0.004898108), (4, 0.003718958)]\n",
      "experiment: [(2, 0.0037261294), (3, 0.0013175071), (4, 0.001304515)]\n",
      "excel: [(4, 0.0044799517)]\n",
      "time_series: [(4, 0.0026590936)]\n",
      "deep_learning: [(2, 0.0057217707), (3, 0.00581918)]\n",
      "visualization: [(1, 0.0022268298), (4, 0.008709503)]\n",
      "data: [(0, 0.07078444), (1, 0.07411064), (4, 0.047758993)]\n",
      "markov_chain: not found\n",
      "optimization: [(0, 0.0028213423), (1, 0.0013771141), (2, 0.0027666984), (3, 0.0028853973)]\n",
      "machine_learning: [(0, 0.006763099), (1, 0.0042846035), (2, 0.012873426), (3, 0.024157466)]\n",
      "pytorch: [(3, 0.002023223)]\n",
      "tensorflow: [(3, 0.00111164)]\n",
      "kafka: [(0, 0.0024062586)]\n",
      "nlp: [(2, 0.0013379429), (3, 0.0027483455)]\n"
     ]
    }
   ],
   "source": [
    "words_to_find = [\"deploy\", \"pipeline\", \"etl\", \"llm\", \"power_bi\", \"generative_ai\", \"gcp\", \"spark\", \"hadoop\", \"git\", \"training\", \"simulation\", \"probability\", \"regression\", \"database\", \"experiment\", \"excel\", \"time_series\", \"deep_learning\", \"visualization\", \"data\", \"markov_chain\", \"optimization\", \"machine_learning\", \"pytorch\", \"tensorflow\", \"kafka\", \"nlp\"]\n",
    "for word in words_to_find:\n",
    "    print(word, end=\": \")\n",
    "    if word in vocab:\n",
    "        print(lda_model_5.get_term_topics(word, minimum_probability=0.001))\n",
    "    else:\n",
    "        print(\"not found\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dir-st",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
