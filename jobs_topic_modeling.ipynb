{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Used to tokenize the text; i.e. create a dictionary mapping words to integers. The dictionary can be used to create a term-document matrix.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models.phrases import ENGLISH_CONNECTOR_WORDS\n",
    "\n",
    "import spacy\n",
    "\n",
    "from textacy import extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_list_from_csv(path):\n",
    "    corpus = []\n",
    "    with open(path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            columns = line.split(',')   \n",
    "            # Columns 1 and 2 contain the company name and the job title, both guaranteed to not include commas, and both separated by a comma. \n",
    "            # We are not analyzing this information, so we can safely discard the first two columns.\n",
    "            # The third \"column\" contains the job description, but it may contain commas, so we use \",\".join() to concatenate all the columns after the second one.\n",
    "            # csv.reader()'s quotechar parameter does not seem to work for whatever reason, and this just seemed faster. \n",
    "            description = \",\".join(columns[2:]).strip('\"')      # strip('\"') to remove leading and trailing quotes\n",
    "            corpus.append(description)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = create_list_from_csv('jobs.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "1812\n",
      " \"What You'll Do Analyze, design, develop, test, review, document and troubleshoot data pipeline / ELT solutions against multiple structured and unstructured data sources. Support our team of analysts through developing requirements and delivering solutions. Develop code to scrape public websites for data and perform ELT processes. Maintain, monitor, and support production ELT processes and respond to error and emergency issues. Who You Are You have excellent knowledge and experience with Big Da\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))\n",
    "print(len(corpus[-1]))\n",
    "print(corpus[-1][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 178\n",
      "Number of words: 45\n",
      "Number of words: 180\n",
      "Number of words: 180\n",
      "Number of words: 284\n",
      "Number of words: 284\n",
      "Number of words: 284\n",
      "Number of words: 284\n",
      "Number of words: 284\n",
      "Number of words: 284\n",
      "Number of words: 329\n",
      "Number of words: 198\n",
      "Number of words: 142\n",
      "Number of words: 134\n",
      "Number of words: 134\n",
      "Number of words: 180\n",
      "Number of words: 291\n",
      "Number of words: 291\n",
      "Number of words: 134\n",
      "Number of words: 134\n",
      "Number of words: 230\n",
      "Number of words: 552\n",
      "Number of words: 391\n",
      "Number of words: 118\n",
      "Number of words: 275\n",
      "Number of words: 293\n",
      "Number of words: 355\n",
      "Number of words: 307\n",
      "Number of words: 366\n",
      "Number of words: 209\n",
      "Number of words: 666\n",
      "Number of words: 191\n",
      "Number of words: 448\n",
      "Number of words: 139\n",
      "Number of words: 244\n",
      "Number of words: 216\n",
      "Number of words: 142\n",
      "Number of words: 51\n",
      "Number of words: 109\n",
      "Number of words: 172\n",
      "Number of words: 116\n",
      "Number of words: 151\n",
      "Number of words: 115\n",
      "Number of words: 136\n",
      "Number of words: 271\n",
      "Number of words: 177\n",
      "Number of words: 165\n",
      "Number of words: 177\n",
      "Number of words: 186\n",
      "Number of words: 211\n",
      "Number of words: 218\n",
      "Number of words: 305\n",
      "Number of words: 330\n",
      "Number of words: 199\n",
      "Number of words: 230\n",
      "Number of words: 196\n",
      "Number of words: 320\n",
      "Number of words: 343\n",
      "Number of words: 260\n",
      "Number of words: 356\n",
      "Number of words: 169\n",
      "Number of words: 148\n",
      "Number of words: 271\n",
      "Number of words: 396\n",
      "Number of words: 288\n",
      "Number of words: 74\n",
      "Number of words: 270\n",
      "Number of words: 225\n",
      "Number of words: 395\n",
      "Number of words: 305\n",
      "Number of words: 202\n",
      "Number of words: 230\n",
      "Number of words: 88\n",
      "Number of words: 277\n",
      "Number of words: 298\n",
      "Number of words: 147\n",
      "Number of words: 105\n",
      "Number of words: 230\n",
      "Number of words: 462\n",
      "Number of words: 227\n",
      "Number of words: 136\n",
      "Number of words: 162\n",
      "Number of words: 309\n",
      "Number of words: 269\n",
      "Number of words: 89\n",
      "Number of words: 89\n",
      "Number of words: 162\n",
      "Number of words: 381\n",
      "Number of words: 205\n",
      "Number of words: 245\n",
      "Number of words: 283\n",
      "Number of words: 428\n",
      "Number of words: 170\n",
      "Number of words: 153\n",
      "Number of words: 164\n",
      "Number of words: 157\n",
      "Number of words: 226\n",
      "Number of words: 248\n",
      "Standard deviation: 105.23867332488568\n",
      "Mean: 233.39795918367346\n",
      "Total number of words in the corpus: 22873\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "doc_lengths = []\n",
    "for doc in corpus:\n",
    "    sum += len(doc.split())\n",
    "    print(\"Number of words:\", len(doc.split()))\n",
    "    doc_lengths.append(len(doc.split()))\n",
    "    \n",
    "print(\"Standard deviation:\", np.std(doc_lengths))\n",
    "print(\"Mean:\", np.mean(doc_lengths))\n",
    "print(f\"Total number of words in the corpus: {sum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_with_spacy(doc):\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    # Add custom stop words that don't add anything to each topic.\n",
    "    nlp.Defaults.stop_words |= {\"experience\", \"preferred\", \"skill\", \"yelp\"}\n",
    "    \n",
    "    spacy_doc = nlp(doc.lower())\n",
    "\n",
    "    ngrams = [\n",
    "        ngram.text.replace(\" \", \"_\")    # ngrams are separated by spaces, so we replace them with underscores\n",
    "        for ngram in extract.ngrams(spacy_doc, n = 2, min_freq = 4, filter_punct = True, filter_nums = True, exclude_pos=[\"PROPN\", \"ORG\", \"DATE\", \"X\"]) \n",
    "        if not ngram.text.__contains__(\"=\") \n",
    "            and not ngram.text.__contains__(\"@\") \n",
    "            and not ngram.text.__contains__(\"$\")\n",
    "    ]\n",
    "    \n",
    "    allowed_pos_tags = [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]\n",
    "    \n",
    "    # Remove stopwords, punctuation, and numeric tokens\n",
    "    tokens = [\n",
    "        token.text \n",
    "        for token in spacy_doc \n",
    "        if not token.is_stop and not token.is_punct and not token.is_digit and token.is_alpha       # Keep only words that are not stop words\n",
    "            and token.text not in [\"_\", \"+\", \"=\", \"\\n\",\"-\",\"*\",\"<\",\">\"]                             # Remove special characters     \n",
    "            and not len(token.text) == 1                                                            # Remove single character words\n",
    "            and token.pos_ in allowed_pos_tags                                                      # Keep only words that are nouns, adjectives, verbs, and adverbs\n",
    "    ]                                                                             \n",
    "    \n",
    "    return tokens + ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the cleaned corpus: 14872\n"
     ]
    }
   ],
   "source": [
    "corpus_with_bigrams = [clean_with_spacy(doc) for doc in corpus]\n",
    "sum = 0\n",
    "for doc in corpus_with_bigrams:\n",
    "    sum += len(doc)\n",
    "\n",
    "print(f\"Total number of words in the cleaned corpus: {sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'collection', 'cleaning', 'assist', 'collection', 'organization', 'data', 'data', 'cleaning', 'preprocessing', 'activities', 'ensure', 'data', 'accuracy', 'data', 'analysis', 'utilize', 'statistical', 'methods', 'tools', 'assist', 'analysis', 'datasets', 'work', 'gsp', 'team', 'members', 'identify', 'trends', 'patterns', 'insights', 'data', 'data', 'visualization', 'support', 'creation', 'visualizations', 'reports', 'communicating', 'data', 'findings', 'collaboration', 'collaborate', 'team', 'members', 'understand', 'data', 'requirements', 'provide', 'support', 'delivering', 'analytical', 'solutions', 'learn', 'experienced', 'team', 'members', 'actively', 'seek', 'guidance', 'generation', 'learn', 'summarize', 'communicatedatainsights', 'clear', 'understandable', 'learning', 'actively', 'participate', 'training', 'development', 'opportunities', 'enhance', 'skills', 'job', 'qualifications', 'bachelor', 'degree', 'relevant', 'field', 'mathematics', 'computer', 'science', 'equivalent', 'basic', 'understanding', 'data', 'analysis', 'concepts', 'methodologies', 'familiarity', 'data', 'analysis', 'tools', 'power', 'bi', 'excel', 'strong', 'attention', 'detail', 'analytical', 'skills', 'good', 'communication', 'skills', 'ability', 'work', 'collaboratively', 'team', 'eagerness', 'learn', 'adapt', 'new', 'technologies', 'techniques']\n"
     ]
    }
   ],
   "source": [
    "print(corpus_with_bigrams[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 2), (3, 1), (4, 1), (5, 4), (6, 2), (7, 2), (8, 1), (9, 1), (10, 1), (11, 1), (12, 2), (13, 1), (14, 1), (15, 1), (16, 1), (17, 2), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 11), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1), (44, 1), (45, 1), (46, 3), (47, 1), (48, 1), (49, 3), (50, 1), (51, 1), (52, 1), (53, 1), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 1), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 3), (67, 1), (68, 1), (69, 1), (70, 1), (71, 2), (72, 4), (73, 1), (74, 1), (75, 2), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 1), (84, 2)]\n"
     ]
    }
   ],
   "source": [
    "dictionary = Dictionary(corpus_with_bigrams)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in corpus_with_bigrams]\n",
    "print(doc_term_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 5\n",
    "PATH_TO_MODEL = f\"Entry_Jobs_Test_LDA_{NUM_TOPICS}_topics\"\n",
    "lda_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc_term_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LdaModel\n\u001b[1;32m----> 3\u001b[0m lda_model \u001b[38;5;241m=\u001b[39m LdaModel(\u001b[43mdoc_term_matrix\u001b[49m, num_topics\u001b[38;5;241m=\u001b[39mNUM_TOPICS, id2word \u001b[38;5;241m=\u001b[39m dictionary, alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, passes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      4\u001b[0m lda_model\u001b[38;5;241m.\u001b[39mshow_topics(num_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'doc_term_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "lda_model = LdaModel(doc_term_matrix, num_topics=NUM_TOPICS, id2word = dictionary, alpha = \"auto\", passes = 10)\n",
    "lda_model.show_topics(num_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([(0.02891476, 'data'),\n",
       "   (0.008088132, 'work'),\n",
       "   (0.0072611975, 'business'),\n",
       "   (0.0070102685, 'analytics'),\n",
       "   (0.006782891, 'skills'),\n",
       "   (0.0049247625, 'analysis'),\n",
       "   (0.0049240515, 'learning'),\n",
       "   (0.0044365465, 'software'),\n",
       "   (0.0044176574, 'tools'),\n",
       "   (0.0042466777, 'develop'),\n",
       "   (0.004184552, 'systems'),\n",
       "   (0.004161931, 'design'),\n",
       "   (0.0041446867, 'insights'),\n",
       "   (0.004088707, 'solutions'),\n",
       "   (0.0040054517, 'machine'),\n",
       "   (0.0039584856, 'knowledge'),\n",
       "   (0.003935261, 'python'),\n",
       "   (0.0038474668, 'sql'),\n",
       "   (0.0038412695, 'understanding'),\n",
       "   (0.0037641556, 'strong')],\n",
       "  -0.5648223318265668),\n",
       " ([(0.07230802, 'data'),\n",
       "   (0.01074812, 'skills'),\n",
       "   (0.009567367, 'business'),\n",
       "   (0.0080453735, 'work'),\n",
       "   (0.007714178, 'analysis'),\n",
       "   (0.007149902, 'strong'),\n",
       "   (0.006488929, 'python'),\n",
       "   (0.006410108, 'ability'),\n",
       "   (0.0063387807, 'team'),\n",
       "   (0.006117394, 'science'),\n",
       "   (0.005660055, 'tools'),\n",
       "   (0.0056115407, 'systems'),\n",
       "   (0.005559718, 'learning'),\n",
       "   (0.005268041, 'pipelines'),\n",
       "   (0.0052333237, 'sql'),\n",
       "   (0.0050958362, 'solutions'),\n",
       "   (0.005075421, 'processes'),\n",
       "   (0.0050050598, 'knowledge'),\n",
       "   (0.0047784904, 'machine'),\n",
       "   (0.0044911955, 'insights')],\n",
       "  -0.6349444316938649),\n",
       " ([(0.034113392, 'data'),\n",
       "   (0.010836611, 'skills'),\n",
       "   (0.00889087, 'learning'),\n",
       "   (0.0069045825, 'ai'),\n",
       "   (0.0068553407, 'work'),\n",
       "   (0.00643834, 'science'),\n",
       "   (0.006432666, 'models'),\n",
       "   (0.0064224484, 'tools'),\n",
       "   (0.005824497, 'machine'),\n",
       "   (0.0058187656, 'design'),\n",
       "   (0.005760386, 'python'),\n",
       "   (0.0051296195, 'business'),\n",
       "   (0.005103962, 'analytics'),\n",
       "   (0.0050460002, 'knowledge'),\n",
       "   (0.0050069224, 'computer'),\n",
       "   (0.004981829, 'analysis'),\n",
       "   (0.004959789, 'systems'),\n",
       "   (0.0048907483, 'engineering'),\n",
       "   (0.0047719697, 'strong'),\n",
       "   (0.0046383985, 'software')],\n",
       "  -0.6358250606031932),\n",
       " ([(0.024761682, 'data'),\n",
       "   (0.01617654, 'ai'),\n",
       "   (0.015434353, 'learning'),\n",
       "   (0.012155527, 'models'),\n",
       "   (0.00870897, 'skills'),\n",
       "   (0.008654711, 'machine'),\n",
       "   (0.0066801026, 'tools'),\n",
       "   (0.006417511, 'ml'),\n",
       "   (0.005516299, 'solutions'),\n",
       "   (0.005033901, 'design'),\n",
       "   (0.0049755005, 'performance'),\n",
       "   (0.0049047656, 'work'),\n",
       "   (0.00487979, 'strong'),\n",
       "   (0.0047906763, 'systems'),\n",
       "   (0.004770623, 'teams'),\n",
       "   (0.004623797, 'python'),\n",
       "   (0.0044880332, 'analysis'),\n",
       "   (0.0044697975, 'business'),\n",
       "   (0.004454867, 'team'),\n",
       "   (0.004040362, 'cloud')],\n",
       "  -0.6859948061811858),\n",
       " ([(0.031964976, 'data'),\n",
       "   (0.011505084, 'learning'),\n",
       "   (0.010727648, 'machine'),\n",
       "   (0.010412169, 'skills'),\n",
       "   (0.008183393, 'models'),\n",
       "   (0.0077167153, 'work'),\n",
       "   (0.007301739, 'development'),\n",
       "   (0.0064467867, 'science'),\n",
       "   (0.006428569, 'business'),\n",
       "   (0.006296896, 'tools'),\n",
       "   (0.006069444, 'design'),\n",
       "   (0.0058901235, 'strong'),\n",
       "   (0.0057364046, 'ai'),\n",
       "   (0.005464033, 'knowledge'),\n",
       "   (0.005420681, 'collaborate'),\n",
       "   (0.00542032, 'cloud'),\n",
       "   (0.00522144, 'team'),\n",
       "   (0.005166171, 'solutions'),\n",
       "   (0.0050342246, 'ability'),\n",
       "   (0.004993678, 'machine_learning')],\n",
       "  -0.9403951943996461)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.top_topics(doc_term_matrix, dictionary=dictionary, coherence='u_mass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "lda_model.save(datapath(PATH_TO_MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.014*data + 0.007*business + 0.005*systems + 0.004*machine + 0.004*aws + 0.004*product + 0.004*learning + 0.004*skills + 0.004*analytics + 0.004*building + 0.003*use + 0.003*yelp + 0.003*kafka + 0.003*apache + 0.003*users + 0.003*knowledge + 0.003*scale + 0.003*work + 0.003*content + 0.003*strong'),\n",
       " (1,\n",
       "  '0.009*data + 0.003*consistent + 0.003*design + 0.003*workshops + 0.003*ai + 0.003*required + 0.002*skills + 0.002*client + 0.002*gitlab + 0.002*analytical + 0.002*procedures + 0.002*updated + 0.002*unlock + 0.002*cloud + 0.002*configurations + 0.002*science + 0.002*existing + 0.002*cd + 0.002*evaluation + 0.002*work'),\n",
       " (2,\n",
       "  '0.011*data + 0.004*term + 0.003*successful + 0.003*schema + 0.003*learning + 0.003*analysis + 0.003*submitted + 0.002*trusted + 0.002*starting + 0.002*industrial + 0.002*sql + 0.002*machine + 0.002*elt + 0.002*uncover + 0.002*improvements + 0.002*lifecycle + 0.002*reference + 0.002*spark + 0.002*goolge + 0.002*cloud'),\n",
       " (3,\n",
       "  '0.008*data + 0.003*attention + 0.003*learning + 0.003*ai + 0.003*models + 0.003*modalities + 0.003*assets + 0.003*computer + 0.002*procedures + 0.002*work + 0.002*trust + 0.002*strong + 0.002*ml + 0.002*science + 0.002*responsibilities + 0.002*uses + 0.002*solutions + 0.002*associate + 0.002*patents + 0.002*national'),\n",
       " (4,\n",
       "  '0.016*data + 0.005*pipelines + 0.003*sql + 0.003*work + 0.003*community + 0.002*modern + 0.002*ci + 0.002*cloud + 0.002*intelligence + 0.002*devops + 0.002*snowflake + 0.002*skilled + 0.002*services + 0.002*numerous + 0.002*team + 0.002*processes + 0.002*report + 0.002*science + 0.002*sports + 0.002*implementation'),\n",
       " (5,\n",
       "  '0.005*data + 0.003*skills + 0.003*document + 0.003*disseminate + 0.003*ai + 0.003*transformer + 0.003*transparency + 0.002*models + 0.002*development + 0.002*strong + 0.002*reproducibility + 0.002*datasets + 0.002*leads + 0.002*order + 0.002*python + 0.002*usable + 0.002*guidance + 0.002*visualizes + 0.002*beautifulsoup + 0.002*ui'),\n",
       " (6,\n",
       "  '0.009*data + 0.004*practical + 0.003*multitasking + 0.003*highlighting + 0.003*reports + 0.003*ensures + 0.003*teams + 0.002*intricacies + 0.002*debugging + 0.002*corporate + 0.002*ui + 0.002*account + 0.002*client + 0.002*managed + 0.002*private + 0.002*entering + 0.002*externally + 0.002*insights + 0.002*monthly + 0.002*particular'),\n",
       " (7,\n",
       "  '0.003*data + 0.003*parties + 0.003*kdd + 0.003*ones + 0.003*multiple + 0.002*clients + 0.002*evaluates + 0.002*practices + 0.002*actioning + 0.002*enjoy + 0.002*cases + 0.002*enable + 0.002*tools + 0.002*actionable + 0.002*junior + 0.002*reservoir + 0.002*sustainable + 0.002*sw + 0.002*limited + 0.002*word'),\n",
       " (8,\n",
       "  '0.011*data + 0.004*processes + 0.003*tools + 0.003*advocate + 0.003*powerful + 0.003*gitlab + 0.002*cassandra + 0.002*trends + 0.002*export + 0.002*stages + 0.002*skills + 0.002*sector + 0.002*visualization + 0.002*analytical + 0.002*recall + 0.002*causes + 0.002*skilled + 0.002*quality + 0.002*prepare + 0.002*discovery'),\n",
       " (9,\n",
       "  '0.010*data + 0.004*reservoir + 0.003*art + 0.003*analysis + 0.003*ingestion + 0.003*instrument + 0.003*ability + 0.003*achievement + 0.003*businesses + 0.002*consumer + 0.002*relevant + 0.002*management + 0.002*information + 0.002*questions + 0.002*especially + 0.002*bayesian + 0.002*hands + 0.002*appreciative + 0.002*work + 0.002*results'),\n",
       " (10,\n",
       "  '0.005*data + 0.003*discussions + 0.003*compute + 0.003*clean + 0.003*import + 0.003*offers + 0.003*configure + 0.002*ai + 0.002*tier + 0.002*requirements + 0.002*visualisations + 0.002*serverless + 0.002*cloudformation + 0.002*backlog + 0.002*datascience + 0.002*leading + 0.002*analytics + 0.002*smell + 0.002*hyper + 0.002*rdbms'),\n",
       " (11,\n",
       "  '0.008*data + 0.004*disseminate + 0.003*analysis + 0.003*members + 0.003*transformer + 0.003*short + 0.003*optical + 0.003*team + 0.003*experienced + 0.003*structured + 0.003*empowers + 0.002*language + 0.002*automotive + 0.002*skills + 0.002*concurrently + 0.002*learn + 0.002*visualization + 0.002*scipy + 0.002*bug + 0.002*collection'),\n",
       " (12,\n",
       "  '0.004*data + 0.004*repositories + 0.003*cloud + 0.003*manageable + 0.003*platform + 0.003*prior + 0.003*lean + 0.002*scale + 0.002*spark + 0.002*architecting + 0.002*communicator + 0.002*dbt + 0.002*bigtable + 0.002*paired + 0.002*innovative + 0.002*distribute + 0.002*experimentation + 0.002*pi + 0.002*processes + 0.002*excellent'),\n",
       " (13,\n",
       "  '0.003*hardware + 0.002*moderate_level + 0.002*subject + 0.002*andanalyzea + 0.002*native + 0.002*completion + 0.002*ml + 0.002*mindset + 0.002*backend + 0.002*quantitative + 0.002*uses + 0.002*serving + 0.002*deployment + 0.002*confluence + 0.002*career + 0.002*rrm + 0.002*bigqueryml + 0.002*verbal + 0.002*quickly + 0.002*respond'),\n",
       " (14,\n",
       "  '0.007*data + 0.003*fostering + 0.003*data_science + 0.003*skilled + 0.003*curiosity + 0.003*growing + 0.002*insights + 0.002*iclr + 0.002*thescore + 0.002*list + 0.002*dead + 0.002*issue + 0.002*improved + 0.002*packages + 0.002*improving + 0.002*natural + 0.002*payment + 0.002*utilized + 0.002*define + 0.002*html'),\n",
       " (15,\n",
       "  '0.003*utilized + 0.003*post + 0.003*implementation + 0.003*reviews + 0.003*stata + 0.003*development + 0.003*code + 0.002*mathematical + 0.002*tdd + 0.002*restlessness + 0.002*cleaning + 0.002*data + 0.002*integrates + 0.002*effectively + 0.002*relentless + 0.002*machine_learning + 0.002*paired + 0.002*enhancing + 0.002*communication + 0.002*technical'),\n",
       " (16,\n",
       "  '0.006*data + 0.003*bug + 0.003*functions + 0.003*prompt + 0.003*datasets + 0.003*disseminate + 0.003*globally + 0.003*play + 0.003*trends + 0.003*direction + 0.003*collect + 0.003*surveillance + 0.002*range + 0.002*bachelor + 0.002*findings + 0.002*skills + 0.002*science + 0.002*alerting + 0.002*portion + 0.002*dictionaries'),\n",
       " (17,\n",
       "  '0.004*data + 0.004*curious + 0.003*related + 0.003*corporate + 0.003*transforming + 0.003*medeloop + 0.003*suited + 0.002*internships + 0.002*daily + 0.002*causes + 0.002*identify + 0.002*diffusion + 0.002*scale + 0.002*ecosystems + 0.002*postgresql + 0.002*gateways + 0.002*includes + 0.002*smooth + 0.002*assigned + 0.002*existing'),\n",
       " (18,\n",
       "  '0.003*appreciated + 0.003*improved + 0.003*carrying + 0.003*polite + 0.003*overall + 0.003*eccv + 0.003*level + 0.002*adoption + 0.002*trainingdeep + 0.002*applications + 0.002*supervisors + 0.002*share + 0.002*workflows + 0.002*team + 0.002*photshop + 0.002*terabytes + 0.002*frameworks + 0.002*basics + 0.002*implementation + 0.002*deletes'),\n",
       " (19,\n",
       "  '0.006*data + 0.003*documentation + 0.003*aimodels + 0.003*deliver + 0.003*developed + 0.003*hierarchical + 0.003*regional + 0.003*common + 0.002*dust + 0.002*tooling + 0.002*dead + 0.002*largest + 0.002*sizes + 0.002*skills + 0.002*query + 0.002*insights + 0.002*modelers + 0.002*ability + 0.002*findings + 0.002*ssrs')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import HdpModel\n",
    "# from pprint import pprint\n",
    "\n",
    "hdp_model = HdpModel(doc_term_matrix, id2word = dictionary)\n",
    "hdp_model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, beta = hdp_model.hdp_to_lda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150,)\n",
      "(150, 2291)\n"
     ]
    }
   ],
   "source": [
    "print(alpha.shape)\n",
    "print(beta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dir-st",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
