{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\syeda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\syeda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\syeda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string   # contains a public variable with all ASCII punctuation characters\n",
    "import nltk\n",
    "\n",
    "# list of all stopwords such as 'and', 'the', 'is', etc.\n",
    "nltk.download('stopwords')  \n",
    "\n",
    "# WordNet is a lexical database of English words that groups words into sets of synonyms, while also recording semantic relationships between words such as \"is-a\", \"part-of\", and \"opposite-of\" relationships.\n",
    "nltk.download('wordnet')    \n",
    "\n",
    "# Open Multilingual WordNet (omw) links hand created wordnets and automatically created wordnets for different languages.\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk import ngrams\n",
    "\n",
    "# Used to tokenize the text; i.e. create a dictionary mapping words to integers. The dictionary can be used to create a term-document matrix.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.phrases import ENGLISH_CONNECTOR_WORDS\n",
    "\n",
    "import spacy\n",
    "from textacy import extract\n",
    "\n",
    "import re\n",
    "\n",
    "def read_txt_to_string(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data530 = read_txt_to_string('Parsed_Slides/DATA530.txt')\n",
    "# print(data530[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with textacy's extract() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Add custom stop words, mostly including header and footer information like names of instructors, name of university, etc.\n",
    "\n",
    "nlp.Defaults.stop_words |= {\"ubc\", \"mds\", \"lecture\", \"lab\", \"assignments\", \"example\", \"page\"}\n",
    "\n",
    "doc = nlp(data530.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = [\n",
    "    ngram.text.replace(\" \", \"_\") \n",
    "        for ngram in extract.ngrams(doc, n = 2, min_freq = 4, filter_punct = True, filter_nums = True, exclude_pos=[\"PROPN\", \"ORG\", \"DATE\", \"X\"]) \n",
    "        if not ngram.text.__contains__(\"=\") \n",
    "            and not ngram.text.__contains__(\"@\") \n",
    "            and not ngram.text.__contains__(\"$\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'format_painter', 'visual_basic', 'cell_value', 'data_analysis', 'pivot_table', 'ubco_master', 'returns_true', 'quick_access', 'goal_seek', 'access_toolbar', 'operating_system', 'relative_path', 'records_found', 'representing_data', 'text_file', 'basic_editor', 'file_system', 'row_labels', 'data_values', 'forecast_outline', 'ex_delete', 'import_random', 'clothing_jacket', 'pivot_tables', 'overall_revenue', 'regression_equation', 'immediate_window', 'version_control', 'text_files', 'grand_total', 'syntax_errors', 'external_new', 'macro_security', 'r_session', 'selection_font', 'learning_studio', 'computer_system', 'api_key', 'file_encoding', 'current_directory', 'data_set', 'cell_styles', 'selecting_cells', 'command_line', 'aggregate_functions', 'machine_learning', 'following_statements', 'analysis_sheet', 'org_eclipse', 'import_module', 'shortcut_key', 'memory_size', 'analysis_toolpak', 'data_size', 'binary_file', 'data_science', 'file_encodings', 'conditional_formatting', 'computing_platforms', 'edit_links', 'object_browser', 'data_tools'}\n",
      "62\n"
     ]
    }
   ],
   "source": [
    "unique_ngrams = set(ngrams)\n",
    "print(unique_ngrams)\n",
    "print(len(unique_ngrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuing preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_with_spacy(doc):\n",
    "\n",
    "    spacy_parser = spacy.load(\"en_core_web_sm\")\n",
    "    # Add custom stop words that are likely to appear in all topic models\n",
    "    spacy_parser.Defaults.stop_words |= {\"ubc\", \"mds\", \"lecture\", \"lab\", \"assignments\", \"example\", \"page\", \"ex\", \"import\"}\n",
    "    \n",
    "    spacy_doc = spacy_parser(doc.lower())\n",
    "\n",
    "    ngrams = [\n",
    "        ngram.text.replace(\" \", \"_\")    # ngrams are separated by spaces, so we replace them with underscores\n",
    "        for ngram in extract.ngrams(spacy_doc, n = 2, min_freq = 4, filter_punct = True, filter_nums = True, exclude_pos=[\"PROPN\", \"ORG\", \"DATE\", \"X\"]) \n",
    "        if not ngram.text.__contains__(\"=\") \n",
    "            and not ngram.text.__contains__(\"@\") \n",
    "            and not ngram.text.__contains__(\"$\")\n",
    "    ]\n",
    "    \n",
    "    # Remove stopwords, punctuation, and numeric tokens\n",
    "    tokens = [token.lemma_ \n",
    "                for token in spacy_doc \n",
    "                if not token.is_stop and not token.is_punct and not token.is_digit and token.is_alpha # Keep only words that are not stop words\n",
    "                and token.text not in [\"_\", \"+\", \"=\", \"\\n\",\"-\",\"*\",\"<\",\">\"]                           # Remove special characters\n",
    "                and not token.lemma_ == \"datum\"]                                                      # Do not lemmatize anything related to data                              \n",
    "    \n",
    "    return tokens + ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data530 = clean_with_spacy(data530)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words after cleaning: 11524\n",
      "['visual_basic', 'visual_basic', 'basic_editor', 'immediate_window', 'immediate_window', 'immediate_window', 'immediate_window', 'immediate_window', 'immediate_window', 'data_analysis', 'visual_basic', 'basic_editor', 'visual_basic', 'basic_editor', 'object_browser', 'ubco_master', 'data_science', 'conditional_formatting', 'data_analysis', 'data_analysis', 'cell_styles', 'clothing_jacket', 'cell_styles', 'clothing_jacket', 'clothing_jacket', 'selecting_cells', 'selecting_cells', 'selecting_cells', 'cell_styles', 'conditional_formatting', 'cell_styles', 'conditional_formatting', 'cell_styles', 'selecting_cells', 'cell_value', 'following_statements', 'cell_styles', 'clothing_jacket', 'aggregate_functions', 'data_values', 'aggregate_functions', 'conditional_formatting', 'cell_styles', 'aggregate_functions', 'aggregate_functions', 'cell_styles', 'clothing_jacket', 'clothing_jacket', 'clothing_jacket', 'aggregate_functions', 'following_statements', 'aggregate_functions', 'cell_styles', 'clothing_jacket', 'clothing_jacket', 'clothing_jacket', 'returns_true', 'returns_true', 'returns_true', 'returns_true', 'conditional_formatting', 'conditional_formatting', 'data_values', 'conditional_formatting', 'cell_styles', 'conditional_formatting', 'cell_value', 'cell_value', 'conditional_formatting', 'format_painter', 'conditional_formatting', 'cell_styles', 'format_painter', 'format_painter', 'format_painter', 'conditional_formatting', 'cell_styles', 'conditional_formatting', 'cell_styles', 'operating_system', 'operating_system', 'data_analysis', 'data_values', 'aggregate_functions', 'data_values', 'conditional_formatting', 'ubco_master', 'data_science', 'command_line', 'command_line', 'file_system', 'relative_path', 'command_line', 'text_files', 'data_science', 'data_science', 'data_analysis', 'data_science', 'data_analysis', 'data_analysis', 'command_line', 'data_analysis', 'command_line', 'command_line', 'command_line', 'command_line', 'command_line', 'command_line', 'text_files', 'command_line', 'operating_system', 'command_line', 'command_line', 'operating_system', 'operating_system', 'command_line', 'command_line', 'command_line', 'command_line', 'file_system', 'file_system', 'org_eclipse', 'org_eclipse', 'org_eclipse', 'org_eclipse', 'file_system', 'relative_path', 'current_directory', 'current_directory', 'current_directory', 'current_directory', 'relative_path', 'following_statements', 'relative_path', 'relative_path', 'relative_path', 'text_files', 'text_files', 'current_directory', 'text_file', 'version_control', 'version_control', 'computing_platforms', 'data_science', 'computing_platforms', 'data_science', 'data_science', 'computing_platforms', 'data_science', 'computing_platforms', 'data_science', 'command_line', 'command_line', 'command_line', 'data_science', 'command_line', 'command_line', 'version_control', 'version_control', 'ubco_master', 'data_science', 'syntax_errors', 'r_session', 'r_session', 'r_session', 'r_session', 'data_set', 'syntax_errors', 'syntax_errors', 'computer_system', 'computer_system', 'computer_system', 'computer_system', 'syntax_errors', 'ubco_master', 'data_science', 'computing_platforms', 'data_science', 'data_science', 'operating_system', 'operating_system', 'api_key', 'api_key', 'api_key', 'api_key', 'api_key', 'api_key', 'machine_learning', 'machine_learning', 'machine_learning', 'machine_learning', 'learning_studio', 'machine_learning', 'machine_learning', 'learning_studio', 'machine_learning', 'machine_learning', 'learning_studio', 'machine_learning', 'learning_studio']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of words after cleaning: {len(cleaned_data530)}\")\n",
    "print(cleaned_data530[-200:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dir-st",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
