Staffing Industry Analysts,Junior Data Analyst - Entry Level(100% Remote),"Data Collection and Cleaning: Assist in the collection and organization of data from various sources.Participate in data cleaning and preprocessing activities to ensure data accuracy. Data Analysis: Utilize statistical methods and tools to assist in the analysis of datasets. Work with GSP team members to identify trends, patterns, and insights in the data. Data Visualization: Support the creation of visualizations and reports for communicating data findings. Collaboration: Collaborate with team members to understand data requirements and provide support in delivering analytical solutions. Learn from experienced team members and actively seek guidance on projects.Report Generation:Learn to summarize and communicatedatainsights in a clear and understandable manner.Continuous Learning: Actively participate in training and development opportunities to enhance skills. Job Qualifications Bachelor's degree in a relevant field (e.g.,Statistics, Mathematics, Computer Science) or equivalent experience. Basic understanding of data analysis concepts and methodologies. Familiarity with data analysis tools such as Power BI and Excel. Strong attention to detail and analytical skills. Good communication skills and the ability to work collaboratively in a team. Eagerness to learn and adapt to new technologies and techniques."
New iTalent Digital,Data Scientist Intern / Remote- Part Time,"Your Role and Responsibilities: Perform accurate and efficient data entry in various systems and databases. Proofread and edit data for correctness, performing updates as necessary. Maintain strict confidentiality of sensitive information. Ensure data backups are correctly performed.Support IT team with other administrative tasks as needed."
New iTalent Digital,Junior Data Analyst - Entry Level,"Data Collection and Cleaning: Assist in the collection and organization of data from various sources.Participate in data cleaning and preprocessing activities to ensure data accuracy. Data Analysis: Utilize statistical methods and tools to assist in the analysis of datasets. Work with GSP team members to identify trends, patterns, and insights in the data. Data Visualization: Support the creation of visualizations and reports for communicating data findings. Collaboration: Collaborate with team members to understand data requirements and provide support in delivering analytical solutions. Learn from experienced team members and actively seek guidance on projects. Report Generation: Learn to summarize and communicatedatainsights in a clear and understandable manner.Continuous Learning: Actively participate in training and development opportunities to enhance skills. Job Qualifications Bachelor's degree in a relevant field (e.g.,Statistics, Mathematics, Computer Science) or equivalent experience. Basic understanding of data analysis concepts and methodologies. Familiarity with data analysis tools such as Power BI and Excel. Strong attention to detail and analytical skills. Good communication skills and the ability to work collaboratively in a team. Eagerness to learn and adapt to new technologies and techniques."
Staffing Industry Analysts,Junior Data Analyst - Entry Level(100% Remote),"Data Collection and Cleaning: Assist in the collection and organization of data from various sources.Participate in data cleaning and preprocessing activities to ensure data accuracy. Data Analysis: Utilize statistical methods and tools to assist in the analysis of datasets. Work with GSP team members to identify trends, patterns, and insights in the data. Data Visualization: Support the creation of visualizations and reports for communicating data findings. Collaboration: Collaborate with team members to understand data requirements and provide support in delivering analytical solutions. Learn from experienced team members and actively seek guidance on projects.  Report Generation: Learn to summarize and communicatedatainsights in a clear and understandable manner.Continuous Learning: Actively participate in training and development opportunities to enhance skills. Job Qualifications Bachelor's degree in a relevant field (e.g.,Statistics, Mathematics, Computer Science) or equivalent experience. Basic understanding of data analysis concepts and methodologies. Familiarity with data analysis tools such as Power BI and Excel. Strong attention to detail and analytical skills. Good communication skills and the ability to work collaboratively in a team. Eagerness to learn and adapt to new technologies and techniques."
Yelp,"Entry Level Software Engineer - Data Backend (Remote, Canada)","We're looking for entry-level engineers who are eager to learn and contribute to building elegant, scalable systems. You'll have the opportunity to work with SQL and NoSQL data stores, data warehouses, batch processing, and stream processing solutions. Join us in leveraging machine learning across Yelp to address significant business challenges, improve user experiences, facilitate data-driven decisions, and maintain the reliability of Yelp's content. What You'll Do: Contribute to building systems that can effectively store and crunch terabytes of data. Support the infrastructure that empowers millions of Yelp’s users to make the best decisions. Engage with diverse challenges such as personalizing ads and search ranking, AI telephony and chatbots, advertiser retention and churn prevention, data-driven storytelling, clickstream analytics, content type classification, delivering personalized recommended businesses to users and sophisticated bot detection. Collaborate with cross functional teams, including software engineers, product managers and data scientists to identify and use the most relevant consumer and business data. Develop expertise in cutting-edge infrastructure for machine learning or data analytics or product feature use cases. Learn the fine art of balancing scale, latency and availability depending on the problem. What It Takes To Succeed: Good coding skills in Python or equivalent (ideally Java or C++). A passion for architecting large systems with elegant interfaces that can scale easily.A hunger for tracking down root causes—no matter how deep it takes you—and fixing them in systematic ways. Understanding of building data pipelines to train and deploy machine learning models and/or ETL pipelines for metrics and analytics or product feature use cases. Exposure to some of the following technologies: Apache Spark, AWS Redshift, AWS S3, Cassandra (and other NoSQL systems), AWS Athena, Apache Kafka, Apache Flink, Java, AWS and service oriented architecture."
Yelp,"Entry Level Software Engineer - Data Backend (Remote, Canada)","We're looking for entry-level engineers who are eager to learn and contribute to building elegant, scalable systems. You'll have the opportunity to work with SQL and NoSQL data stores, data warehouses, batch processing, and stream processing solutions. Join us in leveraging machine learning across Yelp to address significant business challenges, improve user experiences, facilitate data-driven decisions, and maintain the reliability of Yelp's content. What You'll Do: Contribute to building systems that can effectively store and crunch terabytes of data. Support the infrastructure that empowers millions of Yelp’s users to make the best decisions. Engage with diverse challenges such as personalizing ads and search ranking, AI telephony and chatbots, advertiser retention and churn prevention, data-driven storytelling, clickstream analytics, content type classification, delivering personalized recommended businesses to users and sophisticated bot detection. Collaborate with cross functional teams, including software engineers, product managers and data scientists to identify and use the most relevant consumer and business data. Develop expertise in cutting-edge infrastructure for machine learning or data analytics or product feature use cases. Learn the fine art of balancing scale, latency and availability depending on the problem. What It Takes To Succeed: Good coding skills in Python or equivalent (ideally Java or C++). A passion for architecting large systems with elegant interfaces that can scale easily.A hunger for tracking down root causes—no matter how deep it takes you—and fixing them in systematic ways. Understanding of building data pipelines to train and deploy machine learning models and/or ETL pipelines for metrics and analytics or product feature use cases. Exposure to some of the following technologies: Apache Spark, AWS Redshift, AWS S3, Cassandra (and other NoSQL systems), AWS Athena, Apache Kafka, Apache Flink, Java, AWS and service oriented architecture."
Yelp,"Entry Level Software Engineer - Data Backend (Remote, Canada)","We're looking for entry-level engineers who are eager to learn and contribute to building elegant, scalable systems. You'll have the opportunity to work with SQL and NoSQL data stores, data warehouses, batch processing, and stream processing solutions. Join us in leveraging machine learning across Yelp to address significant business challenges, improve user experiences, facilitate data-driven decisions, and maintain the reliability of Yelp's content. What You'll Do: Contribute to building systems that can effectively store and crunch terabytes of data. Support the infrastructure that empowers millions of Yelp’s users to make the best decisions. Engage with diverse challenges such as personalizing ads and search ranking, AI telephony and chatbots, advertiser retention and churn prevention, data-driven storytelling, clickstream analytics, content type classification, delivering personalized recommended businesses to users and sophisticated bot detection. Collaborate with cross functional teams, including software engineers, product managers and data scientists to identify and use the most relevant consumer and business data. Develop expertise in cutting-edge infrastructure for machine learning or data analytics or product feature use cases. Learn the fine art of balancing scale, latency and availability depending on the problem. What It Takes To Succeed: Good coding skills in Python or equivalent (ideally Java or C++). A passion for architecting large systems with elegant interfaces that can scale easily.A hunger for tracking down root causes—no matter how deep it takes you—and fixing them in systematic ways. Understanding of building data pipelines to train and deploy machine learning models and/or ETL pipelines for metrics and analytics or product feature use cases. Exposure to some of the following technologies: Apache Spark, AWS Redshift, AWS S3, Cassandra (and other NoSQL systems), AWS Athena, Apache Kafka, Apache Flink, Java, AWS and service oriented architecture."
Yelp,"Entry Level Software Engineer - Data Backend (Remote, Canada)","We're looking for entry-level engineers who are eager to learn and contribute to building elegant, scalable systems. You'll have the opportunity to work with SQL and NoSQL data stores, data warehouses, batch processing, and stream processing solutions. Join us in leveraging machine learning across Yelp to address significant business challenges, improve user experiences, facilitate data-driven decisions, and maintain the reliability of Yelp's content. What You'll Do: Contribute to building systems that can effectively store and crunch terabytes of data. Support the infrastructure that empowers millions of Yelp’s users to make the best decisions. Engage with diverse challenges such as personalizing ads and search ranking, AI telephony and chatbots, advertiser retention and churn prevention, data-driven storytelling, clickstream analytics, content type classification, delivering personalized recommended businesses to users and sophisticated bot detection. Collaborate with cross functional teams, including software engineers, product managers and data scientists to identify and use the most relevant consumer and business data. Develop expertise in cutting-edge infrastructure for machine learning or data analytics or product feature use cases. Learn the fine art of balancing scale, latency and availability depending on the problem. What It Takes To Succeed: Good coding skills in Python or equivalent (ideally Java or C++). A passion for architecting large systems with elegant interfaces that can scale easily.A hunger for tracking down root causes—no matter how deep it takes you—and fixing them in systematic ways. Understanding of building data pipelines to train and deploy machine learning models and/or ETL pipelines for metrics and analytics or product feature use cases. Exposure to some of the following technologies: Apache Spark, AWS Redshift, AWS S3, Cassandra (and other NoSQL systems), AWS Athena, Apache Kafka, Apache Flink, Java, AWS and service oriented architecture."
Yelp,"Entry Level Software Engineer - Data Backend (Remote, Canada)","We're looking for entry-level engineers who are eager to learn and contribute to building elegant, scalable systems. You'll have the opportunity to work with SQL and NoSQL data stores, data warehouses, batch processing, and stream processing solutions. Join us in leveraging machine learning across Yelp to address significant business challenges, improve user experiences, facilitate data-driven decisions, and maintain the reliability of Yelp's content. What You'll Do: Contribute to building systems that can effectively store and crunch terabytes of data. Support the infrastructure that empowers millions of Yelp’s users to make the best decisions. Engage with diverse challenges such as personalizing ads and search ranking, AI telephony and chatbots, advertiser retention and churn prevention, data-driven storytelling, clickstream analytics, content type classification, delivering personalized recommended businesses to users and sophisticated bot detection. Collaborate with cross functional teams, including software engineers, product managers and data scientists to identify and use the most relevant consumer and business data. Develop expertise in cutting-edge infrastructure for machine learning or data analytics or product feature use cases. Learn the fine art of balancing scale, latency and availability depending on the problem. What It Takes To Succeed: Good coding skills in Python or equivalent (ideally Java or C++). A passion for architecting large systems with elegant interfaces that can scale easily.A hunger for tracking down root causes—no matter how deep it takes you—and fixing them in systematic ways. Understanding of building data pipelines to train and deploy machine learning models and/or ETL pipelines for metrics and analytics or product feature use cases. Exposure to some of the following technologies: Apache Spark, AWS Redshift, AWS S3, Cassandra (and other NoSQL systems), AWS Athena, Apache Kafka, Apache Flink, Java, AWS and service oriented architecture."
Yelp,"Entry Level Software Engineer - Data Backend (Remote, Canada)","We're looking for entry-level engineers who are eager to learn and contribute to building elegant, scalable systems. You'll have the opportunity to work with SQL and NoSQL data stores, data warehouses, batch processing, and stream processing solutions. Join us in leveraging machine learning across Yelp to address significant business challenges, improve user experiences, facilitate data-driven decisions, and maintain the reliability of Yelp's content. What You'll Do: Contribute to building systems that can effectively store and crunch terabytes of data. Support the infrastructure that empowers millions of Yelp’s users to make the best decisions. Engage with diverse challenges such as personalizing ads and search ranking, AI telephony and chatbots, advertiser retention and churn prevention, data-driven storytelling, clickstream analytics, content type classification, delivering personalized recommended businesses to users and sophisticated bot detection. Collaborate with cross functional teams, including software engineers, product managers and data scientists to identify and use the most relevant consumer and business data. Develop expertise in cutting-edge infrastructure for machine learning or data analytics or product feature use cases. Learn the fine art of balancing scale, latency and availability depending on the problem. What It Takes To Succeed: Good coding skills in Python or equivalent (ideally Java or C++). A passion for architecting large systems with elegant interfaces that can scale easily.A hunger for tracking down root causes—no matter how deep it takes you—and fixing them in systematic ways. Understanding of building data pipelines to train and deploy machine learning models and/or ETL pipelines for metrics and analytics or product feature use cases. Exposure to some of the following technologies: Apache Spark, AWS Redshift, AWS S3, Cassandra (and other NoSQL systems), AWS Athena, Apache Kafka, Apache Flink, Java, AWS and service oriented architecture."
Maneva,Junior AI Engineer,"This entry-level role is ideal for someone eager to learn and grow in AI applications for real-world manufacturing challenges. The position involves contributing to training vision models for tasks such as classification, object detection, and segmentation, assisting with MLOps workflows, and monitoring model performance. The ideal candidate is passionate about learning and applying AI technologies, thrives in hands-on environments, and is motivated to grow alongside an innovative team. Main Responsibilities: Assist in developing and training vision-based AI models, including tasks like classification, object detection, and segmentation. Collaborate on building and maintaining pipelines for deploying AI/ML models in production. Support the integration and use of MLOps tools to streamline workflows. Monitor deployed models, collect performance data, and suggest optimizations. Help troubleshoot and update AI models under guidance to ensure reliability and performance. Contribute to cross-functional discussions to align AI applications with manufacturing needs. Assist with cloud platform setups (AWS, Azure, GCP) for training and deployment under supervision. Document workflows and best practices for reproducibility and operational improvements. Qualifications Education: Bachelor’s or Master’s degree in Computer Science, Machine Learning,DataScience, Engineering, or a related field. Relevant certifications or coursework in MLOps or AI model development is a plus. Experience: Familiarity with computer vision or machine learning through academic projects, internships, or personal initiatives. Exposure to Python, Linux, and basic AI/ML libraries (e.g., PyTorch, TensorFlow, or Scikit-learn).Any experience with real-world AI applications or internships in industrial or manufacturing environments is a bonus. Technical Skills: Proficiency in Python, Linux, Docker, and Git. Familiarity with machine learning workflows and tools for training and fine-tuning models. Familiarity with cloud platforms (AWS, Azure, GCP) for compute, storage, and AI services.Experience with ARM devices such as Jetson or Raspberry Pi is a plus. Eagerness to learn and gain hands-on experience with neural networks and MLOps. Soft Skills: Strong desire to learn and grow in the field of AI. Problem-solving mindset with attention to detail. Excellent communication and collaboration skills. Willingness to adapt and contribute in a fast-paced environment."
W Advisory France,Jr Data Analyst,"You will work closely with senior analysts and data scientists to collect, process, and analyze data from various sources. Your analytical skills will help identify trends and patterns that can drive strategic decision-making. The ideal candidate will possess a strong foundation in data analysis, statistical methods, and database management. Responsibilities: Collect and organize data from various sources to ensure accuracy and completeness. Conduct preliminary data analysis to identify trends, patterns, and anomalies. Assist in the preparation of reports and dashboards to communicate findings. Collaborate with team members to identify data needs and requirements. Support the development of data models and visualizations to enhance decision-making processes. Perform quality assurance checks on datasets to maintain data integrity. Stay updated with industry trends and best practices in data analysis and visualization techniques. Requirements: Bachelor's degree in DataScience, Statistics, Mathematics, or a related field. Knowledge of data analysis tools such as Excel, SQL, or Python. Familiarity with data visualization software like Tableau or Power BI. Strong analytical skills with a problem-solving mindset. Excellent attention to detail and ability to work with large datasets. Effective communication skills, both written and verbal. Ability to work collaboratively in a team environment and manage multiple projects"
BCJobs,Data Entry Analyst (Entry-Level) (Rebel Apparel),"Key Responsibilities: Accurately input and update data into various systems and databases. Review and clean data to ensure accuracy and completeness. Assist in generating and analyzing reports based on data trends. Collaborate with team members to resolve data discrepancies and issues. Maintain organized records and documentation for easy retrieval. Support data-related projects and tasks as assigned by supervisors. Qualifications: High school diploma or equivalent; associate’s degree or coursework in data management is a plus. Basic proficiency in Microsoft Office Suite (Excel, Word). Strong attention to detail and commitment to data accuracy. Good organizational skills with the ability to handle multiple tasks. Effective written and verbal communication skills. Ability to work independently and meet deadlines.Reliable internet connection and a quiet workspace if remote. Preferred Skills (Not Required): Familiarity with data analysis tools or software. Previous experience in a data entry or analytical role."
BCJobs,Entry-Level Data Analyst (Rebel Apparel),"Key Responsibilities: Collect, clean, and analyze data to generate actionable insights. Prepare reports and presentations of findings, clearly communicating complex data to stakeholders. Assist in identifying trends and patterns in datasets. Collaborate with cross-functional teams to understand data needs and deliver solutions. Contribute to the development and improvement of data collection and analysis processes. Stay updated with industry trends and best practices indataanalysis. Requirements: Bachelor’s degree in Data Science,Statistics, Mathematics, Computer Science, Economics, or a related field. Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.Proficiency in data analysis tools such as Excel, SQL, Python, or R (experience with statistical software is a plus). Ability to work effectively in a team environment and independently when needed. Excellent communication and presentation skills."
BCJobs,Entry-Level Data Analyst (Rebel Apparel),"Key Responsibilities: Collect, clean, and analyze data to generate actionable insights. Prepare reports and presentations of findings, clearly communicating complex data to stakeholders. Assist in identifying trends and patterns in datasets. Collaborate with cross-functional teams to understand data needs and deliver solutions. Contribute to the development and improvement of data collection and analysis processes. Stay updated with industry trends and best practices indataanalysis. Requirements: Bachelor’s degree in Data Science,Statistics, Mathematics, Computer Science, Economics, or a related field. Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.Proficiency in data analysis tools such as Excel, SQL, Python, or R (experience with statistical software is a plus). Ability to work effectively in a team environment and independently when needed. Excellent communication and presentation skills."
BCJobs,Entry Level Data Analyst (Rebel Apparel),"Key Responsibilities: Collect, clean, and preprocess data from various sources. Perform exploratory data analysis to identify trends, patterns, and anomalies. Utilize statistical methods and tools to analyze data and generate insights. Create and maintain dashboards, reports, and visualizations using tools like Excel, Tableau, or Power BI. Assist in developing and implementing data-driven strategies to support business objectives. Collaborate with team members to understand data needs and provide actionable recommendations. Document analysis processes and results for transparency and future reference. Qualifications: Bachelor’s degree in Data Science,Statistics, Mathematics, Computer Science, or a related field. Familiarity with data analysis tools and techniques, such as Excel, SQL, or Python/R. Strong analytical skills with attention to detail and accuracy. Ability to interpret and present data in a clear and meaningful way. Basic knowledge of data visualization tools (e.g., Tableau, Power BI) is a plus. Excellent communication and teamwork skills. Eagerness to learn and grow in the field of data analysis. Preferred Skills: Experience with data cleaning and transformation.Understanding of basic statistical concepts and methods. Exposure to machine learning algorithms and data modeling is an advantage."
Keyloop,Data Analyst,"You will be responsible for a variety of work streams including generating insight, data visualisation and BI product optimisation. To succeed in this role, you need to be creative with both data and visualisations and have an analytical mindset. This entry-level Data Analyst will be responsible for manually entering, analyzing preparingdatato provide valuable insights and recommendations to clients They will work closely with the account teams to gather andanalyzedata, develop and send reports, and provide solutions to help the Regional Go-To-Market Team gain a competitive edge and bring value to clients. Analyze and Collect Data. Analyze datasets to uncover trends, opportunities, and insights for account teams. Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes. Manage data cleansing, to ensure data is client friendly. Data Cleansing / Data Matching: Manage manual cleansing reports and pull data. Working alongside account teams, clients, dealers and internal teams to assist with lead event and source launches (testing the data), ensuring it comes through correctly. Help trouble-shoot data matches between various reports.Running and Modifying SQL Queries. Correcting issues with the data when it does not match. Report Preparation: Running Monthly Industry data for Monthly and Quarterly Client Reviews. Preparing and sending the data for other client monthly and ad hoc reports. Prepare data-driven reports and insights for clients highlighting key findings and recommendations. Support the account teams with various client reports. Skills and Qualifications: 1+ years of relevant experience or education in data analysis. Hyper attention to detail. Organized PowerPoint. A Master at Excel and experience with pivot tables. SQL experience with data manipulation preferred. Graphic design and or Photshop skills are appreciated. Experience with data visualization and Power BI. Interest in AI, and data automation. Interest in the automotive industry."
Keyloop,Data Analyst,"You will be responsible for a variety of work streams including generating insight, data visualisation and BI product optimisation. To succeed in this role, you need to be creative with both data and visualisations and have an analytical mindset. This entry-level Data Analyst will be responsible for manually entering, analyzing preparingdatato provide valuable insights and recommendations to clients They will work closely with the account teams to gather andanalyzedata, develop and send reports, and provide solutions to help the Regional Go-To-Market Team gain a competitive edge and bring value to clients. Analyze and Collect Data. Analyze datasets to uncover trends, opportunities, and insights for account teams. Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes. Manage data cleansing, to ensure data is client friendly. Data Cleansing / Data Matching: Manage manual cleansing reports and pull data. Working alongside account teams, clients, dealers and internal teams to assist with lead event and source launches (testing the data), ensuring it comes through correctly. Help trouble-shoot data matches between various reports.Running and Modifying SQL Queries. Correcting issues with the data when it does not match. Report Preparation: Running Monthly Industry data for Monthly and Quarterly Client Reviews. Preparing and sending the data for other client monthly and ad hoc reports. Prepare data-driven reports and insights for clients highlighting key findings and recommendations. Support the account teams with various client reports. Skills and Qualifications: 1+ years of relevant experience or education in data analysis. Hyper attention to detail. Organized PowerPoint. A Master at Excel and experience with pivot tables. SQL experience with data manipulation preferred. Graphic design and or Photshop skills are appreciated. Experience with data visualization and Power BI. Interest in AI, and data automation. Interest in the automotive industry."
BCJobs,Entry-Level Data Analyst (Rebel Apparel),"Key Responsibilities: Collect, clean, and analyze data to generate actionable insights. Prepare reports and presentations of findings, clearly communicating complex data to stakeholders. Assist in identifying trends and patterns in datasets. Collaborate with cross-functional teams to understand data needs and deliver solutions. Contribute to the development and improvement of data collection and analysis processes. Stay updated with industry trends and best practices indataanalysis. Requirements: Bachelor’s degree in Data Science,Statistics, Mathematics, Computer Science, Economics, or a related field. Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.Proficiency in data analysis tools such as Excel, SQL, Python, or R (experience with statistical software is a plus). Ability to work effectively in a team environment and independently when needed. Excellent communication and presentation skills."
BCJobs,Entry-Level Data Analyst (Rebel Apparel),"Key Responsibilities: Collect, clean, and analyze data to generate actionable insights. Prepare reports and presentations of findings, clearly communicating complex data to stakeholders. Assist in identifying trends and patterns in datasets. Collaborate with cross-functional teams to understand data needs and deliver solutions. Contribute to the development and improvement of data collection and analysis processes. Stay updated with industry trends and best practices indataanalysis. Requirements: Bachelor’s degree in Data Science,Statistics, Mathematics, Computer Science, Economics, or a related field. Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.Proficiency in data analysis tools such as Excel, SQL, Python, or R (experience with statistical software is a plus). Ability to work effectively in a team environment and independently when needed. Excellent communication and presentation skills."
Ericsson,Data Professional (Entry Level),"In this role you will work with an outstanding local and international team while interacting closely with our customers, influence and drive complex, innovative projects, overcome barriers and address sophisticated technical challenges in application development. What you will do: Define how we instrument, prioritize, and store data that powers AI/ML solutions. Design and implement scalable data models, predictive models andmachine learning algorithms. Evolve and optimize our data and data pipeline architecture, including data flow & collection for multi-functional teams, and integrate them with the company-wide architecture. Work closely with other team members to ‘digitize’ & ‘automate’ various reports (i.e. financial/progress tracking/planning etc.) Gather requirements from team-members to design and implement various data dashboards, templates, KPI scorecards. Evaluate and improve existing reports/visualizations. Create & maintain data repositories. The skills you bring: Bachelor’s degree in Business, IT, Engineering, Mathematics, Statistics, or a related discipline. Practical experience in building data pipelines and models. Familiarity with key data engineering technologies such as Python and SQL, with exposure to tools and languages like Spark and Java. Exposure to batch and streaming data architectures. Knowledge of various database structures, including relational and NoSQL databases. Proficiency in MS Excel and basic experience with data visualization tools like Power BI. Introductory experience with machine learning frameworks such as PyTorch, TensorFlow, or Keras. Understanding of cloud-native architectures and services, with basic knowledge of platforms like AWS, Azure, or GCP."
Applicantz,Data Scientist,"Candidate needs to have a combination of cloud infrastructure skills (particularly on AWS), strong proficiency in Python and JavaScript, expertise in NLP, and advanced knowledge of ML frameworks like PyTorch and HuggingFace. Additionally, they must have practical experience in using LLMs to build robust AI applications, including a solid understanding of agentic frameworks, RAG workflows, and prompt engineering.By leveraging these tools and skills, the team will be well-equipped to develop an AI code generation platform that enhances engineering productivity, ensures code consistency, and embeds Autodesk’s security and compliance standards.Technical Tools and Skills Required1. Cloud Infrastructure: AWS• The project will be built on Amazon Web Services (AWS), leveraging its robust cloud infrastructure for scalability, security, and reliability. Team members should be proficient in using AWS services such as EC2, S3, Lambda, RDS, and DynamoDB. Familiarity with AWS tools like CloudFormation or Terraform for Infrastructure as Code (IaC) is also essential to ensure automated and consistent deployment of resources.2. Programming Languages: Python/JavaScript• Python is required for developing core components, including AI andmachine learning models,data processing pipelines, and backend services. Proficiency in Python libraries such as NumPy, Pandas, and scikit-learn is valuable.• JavaScript is needed for frontend development, particularly for building user interfaces and integrating with APIs. Familiarity with frameworks like React or Vue.js will help in developing responsive and dynamic UI components for the platform. 3. Natural Language Processing (NLP) Experience:• Team members should have experience in Natural Language Processing (NLP) to understand and interpret various input sources like LUCID diagrams, API specifications, and FIGMA designs. This includes expertise in text preprocessing, entity recognition, and language modeling techniques, which are crucial for extracting relevant information and generating code. 4.Experience in Agentic Frameworks, Retrieval-Augmented Generation (RAG) Workflows, and Prompt Engineering: • Knowledge of Agentic frameworks (frameworks that enable autonomous agent behavior in AI systems) is essential to develop sophisticated AI models that can autonomously generate code based on input data.• Proficiency in Retrieval-Augmented Generation (RAG) workflows is required to ensure that the AI models can retrieve relevant information dynamically and generate accurate code snippets. Understanding how to implement RAG will help in building an efficient and context-aware AI platform.• Prompt engineering skills are critical for designing and optimizing the prompts used to interact with the underlying large language models(LLMs). This involves crafting effective prompts that guide the model to generate the desired outputs, improving both accuracy and relevance.5. Proficiency in Machine Learning Frameworks: PyTorch, HuggingFace• The team must be adept in using popular ML frameworks like PyTorch for building and trainingdeep learning models. PyTorch is highly flexible and well-suited for experimentation and custom model development.• Familiarity with the HuggingFace ecosystem, particularly its Transformers library, is crucial for working with pre-trained large language models (LLMs) and fine-tuning them for specific tasks. Experience with HuggingFace’s tools will enable the team to leverage state-of-the-art models and accelerate the development process. 6.Must-Have Experience: Building Applications on Top of Large Language Models(LLMs)• Team members should have hands-on experience in creating applications that utilize Large Language Models(LLMs) like GPT-3, GPT-4, or other transformer-based models. This includes knowledge of integrating LLMs into applications, handling model inputs and outputs, and optimizing their performance for specific use cases.• Practical experience in deploying LLM-based applications in a production environment, managing their scalability, latency, and cost-effectiveness, is also critical to ensure the success of the project."
Refonte Learning,AI & Data Scientist Study & Internship,"Responsibilities: Collaborate with cross-functional teams to develop, deploy, and maintain AI-driven solutions. Assist in collecting, cleaning, and analyzing data from various sources to derive actionable insights. Contribute to the design and implementation of scalable data pipelines for model training and deployment. Support the integration of machine learningmodelsinto production systems using DevOps best practices.Participate in designing and optimizing CI/CD pipelines for continuous integration, testing, and deployment of AI applications on cloud platforms. Assist in infrastructure provisioning, configuration, and monitoring using cloud services such as AWS, Azure, or Google Cloud.Work closely withdatascientists and engineers to understand business requirements and translate them into technical solutions.Research and experiment with emerging technologies and tools in AI, Data Science, DevOps, and Cloud domains. Collaborate on documentation efforts to ensure knowledge sharing and best practices across the team. Projects You Will Work On:- Multi Cloud AI Infrastructure Configuration, Automation and Deployment- Full Stack AI DevOps & Development- Generative AI model, Large Language Models and Foundations models to transform input to output. NB: Input can be text, images, audios or videos; Output can also be text, images, audios or videos- Finance Fraud Detection: Develop advanced fraud detection algorithms leveraging financial data analysis.- Recommender System: Contribute to personalized recommendation systems, enhancing user experiences across platforms.- Sentiment Analysis: Explore sentiment analysis to extract insights from textual data, shaping user sentiment understanding.- Chatbots: Engage in intelligent chatbot development, revolutionizing customer interactions and support.- Image/Audio Video Classification: Push boundaries with multimedia technology by working on image and audio video classification projects.- Text Analysis: Uncover hidden patterns in textual data through sophisticated text analysis techniques.Roles & Responsibilities:- Collaborate with our esteemed AI & data science experts to collect, clean, and analyze extensive datasets, honing skills in data preprocessing and visualization.- Contribute to the development of predictive models and algorithms, employing cutting-edge machine learning techniques to solve real-world challenges.- Work closely with team members to design, implement, and evaluate experiments, fostering a collaborative and innovative environment.- Stay updated with the latest industry trends and best practices in data science, applying newfound knowledge to enhance project outcomes. Qualifications:- Currently pursuing any degree showcasing a strong commitment to continuous learning and professional growth.- Exceptional written and verbal communication skills, vital for effective collaboration and articulation of complex ideas.- Demonstrated ability to work both independently and as part of a cohesive team, highlighting adaptability and strong teamwork capabilities."
Jerry,Staff Data Scientist (User Growth),"How you will make an impact: Define, understand, and test levers to drive profitable and scalable user acquisition across our paid and organic channels. Design, run, andanalyzeA/B experiments across different channels, extract key insights, share learnings and continue iterating. Build key reports, dashboards, and predictive models to monitor the performance of our paid ads and marketing channels, communicate analytical outcomes to our teams, and make recommendations on next steps. Transform and refine raw production data for analytical needs. Preferred experience: Bachelor’s degree in a quantitatively or intellectually rigorous discipline. A few years of consulting experience (MBB preferred). Prior experience with customer acquisition or performance marketing. High level of comfort with SQL and Python (or similar ML programming language)"
Microsoft,Research Intern - Agent Systems for AI Infrastructure,"Responsibilities: Research Interns put inquiry and theory into practice. Alongside fellow doctoral candidates and some of the world’s best researchers, Research Interns learn, collaborate, and network for life. Research Interns not only advance their own careers, but they also contribute to exciting research and development strides. During the 12-week internship, Research Interns are paired with mentors and expected to collaborate with other Research Interns and researchers, present findings, and contribute to the vibrant life of the community. Research internships are available in all areas of research, and are offered year-round, though they typically begin in the summer. Additional Responsibilities: Development and Implementation: Design LLM-based agent systems for AI infrastructure. Implement prototypes and conduct experiments to test and validate them. Research and Analysis: Conduct thorough research on emerging trends in agent systems for AI software and hardware infrastructure.Collaboration: Work closely with cross-functional teams, including hardware engineers, software developers, anddatascientists, to integrate your ideas with existing and future agent projects. Documentation and Reporting: Prepare detailed documentation of simulations, methodologies, and findings. Present results and insights to team members and stakeholders. Innovation and Problem-Solving: Identify challenges and bottlenecks in AI infrastructure and propose innovative solutions.QualificationsRequired Qualifications: Currently enrolled in a bachelor's, master's, or PhD program in Computer Science, Electrical Engineering, Machine learning, Mathematics, or a related field. Preferred Qualifications: Proficient in programming languages such as C/C++ or Python. Familiar with fundamental concepts related to LLM, prompt engineering, and LLM-based agents. Experience in AI for systems and/or systems for AI.Passionate about addressing real-world large-scale infrastructure problems using AImodels. Experience with AI infrastructure or LLM would be a plus.Proficient analytical and problem-solving skills and communication skills, both written and verbal"
Amazon,"Machine Learning Engineer MLOps/MLaaS, ICC AI Center of Excellence","Key job responsibilities: You own the development and operationalization of solutions deployed in production. You work across multiple teams to integrate our solutions with products owned by our partners. You help design model experimentation processes and frameworks in synergy with our scientists. You help the team grow and cultivate best practices in software development, MLOps, and experimentation. A day in the life: Almost everyday offers new challenges and opportunities for growth. Where one day will offer implementation of experimentation tooling, the next day may be focused on our operational excellence in maintaining our code base. Later in the week, you may sort technical challenges with our partners to help them enrich their products with ourmodels. On some days or weeks, you may watch over our products and stand ready to intervene and provide support to partners consuming ourmodels.If you are not sure that every qualification on the list above describes you exactly, we'd still love to hear from you! At Amazon, we value people with unique backgrounds, experiences, and skillsets. If you’re passionate about this role and want to make an impact on a global scale, please apply! Basic Qualifications: 2+ years of non-internship professional software development experience1+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience. Experience programming with at least one software programming language. Bachelor's degree in computer science or equivalent. 1+ years experience and knowledge in MLOps, in deploying, operationalizing, and maintaining scalable AI/ML-solutions in production.Preferred Qualifications: 2+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experienceMaster's degree in computer science or equivalent. Experience in machine learning, data mining, information retrieval, statistics or natural language processing"
"InterDigital, Inc.",AIML Research Engineer,"The candidate will have a solid theoretical background in artificial intelligence, neural networks andmachine learning. The candidate will preferably have some practical experience with AI/ML model design and training as well as in applying AI/ML techniques in specific domain expertise e.g., imaging, video, large languagemodelsand/or wireless. The candidate will conduct research with focus on fundamental aspects of deploying AI/ML techniques in wireless systems, develop innovative solutions addressing the challenges of advanced deployments of AI/ML in future wireless systems with focus on the design of solutions applied to physical radio layer techniques (PHY Layer 1), medium access control (MAC Layer 2) and/or resource control functions (RRC Layer 3) as well as novel applications for specific use cases such as joint communication and sensing, mobility optimizations or the likes. The candidate will contribute to the 3GPP RAN AI/ML team: By carrying out research on fundamental and/or applied AI/ML techniques targeting the evolution and/or design of the physical radio layer for 5G-Advanced radio technologies, for 6G radio access and beyond ;By conducting evaluation, analysis, and development of innovative AI-driven solutions applicable in L1/PHY, L2/MAC and/or L3/RRM such as feedback compression, channel state prediction, beam management, mobility and RRM management, positioning and/or sensing ;By supporting 3GPP standardization activities through the preparation of AI/MLmodels, simulations, and contributions in Radio Access Network (RAN) Working Groups (WG). Qualifications/Requirements B.Eng., M.S., or Ph.D. in Computer Science,Statistics, Electrical Engineering, Applied Math or other relevant discipline (or similar / equivalent)0-5 years of relevant experience in the field of machine learning with theoretical understanding of various training methods + neutral networks. In-depth knowledge in one or more of the following areas: machine learning, deep learning, data mining, optimization. Hands on experience with ML frameworks such as Pytorch, Tensorflow. Strong analytical, technical, innovative and leadership skills. Ability to thrive in a dynamic, innovative, collaborative and team-oriented work environment.Dedicated, result oriented professional with an attitude for getting the work done. Willing to take challenges in a fast paced and high-pressure environment. Experience with wireless communications and/or signal processing may be considered an advantage. Experience with GitHub/GitLab, Matlab, NS-3, C/C++ and/or Python may be considered an advantage."
RAD Intel,Machine Learning Engineer (NLP),"Objectives of this Role: Work with the Machine Learning team to build state-of-the-art AI solutions that leverage the power of NLP to unlock critical business and audience related insights and relationships. Develop new features and ML models in support of rapidly emerging business and project requirements. Assume leadership of new projects from conceptualization to deployment. Ensure application performance, uptime, and scale, maintaining high standards of code quality and thoughtful application design. Work with agile development methodologies, adhering to best practices and pursuing continued learning opportunities. Experience: Experienced in large scale (big) data analytics:Deep understanding of feature spaces and their internal relationships. Deep understanding of pattern recognition (unsupervised learning). Must possess strong and proven knowledge of NLP including: Understanding of latest sentence transformer / embedding models. Understanding of multi purpose NLP models(BERT, RoBERTa, ELECTRA, etc). Deep understanding of how your created features work, need to expand on why predictions are made and what needs to be done to change or follow your predictions. Time complexity management - most models/ algorithms should run in real time. Required Skills and Qualifications: Computer Science Master's degree preferred but Bachelor's will also be considered. Python, TensorFlow, Anaconda, Flask, Gunicorn, MySQL, Linux, Nignx, RESTful APIsProficient in Python (2 to 3 years). Proficient in using Jupyter Notebooks (1 to 2 years). Proficient in basic SQL and python database connectors (1 to 2 years). Experienced in Version Control, Git Flow model (1 year). Knowledge of RESTful API development (1 year). Knowledge in launching software or services, iterative development, understanding of Agile principles, high code quality / reusability, Code Design Patterns. Knowledge in writing well-defined requirements including features, user stories, and acceptance criteria using Jira Software. Good Skills to Have: Knowledge of “Textual Replacements” at a word / phrase / sentence level. Knowledge about Text (GPT, etc.) Generative Models. Knowledge of Linux Shell Scripting, SSH Tunnelling."
COMCO Controls,Junior Data Scientist,"Primary Responsibilities: Develop and implement advanced data models. Conduct predictive and prescriptive analytics to forecast trends and outcomes. Collaborate with cross-functional teams to integrate data-driven solutions into business processes. Communicate complex data insights and recommendations to stakeholders.Develop data collection plans and ensure integrity of data collected through all stages of acquisition and processing. Combine expert knowledge of statistical methodology and analysis with advanced programming skills to perform exploratory data analysis to uncover patterns in user's health and activities. Contribute to data quality control, model validation and model explainability investigation. Collect, process, and analyze large datasets to extract meaningful insights. Collaborate with cross-functional teams to understand business requirements and deliverdata-driven solutions.Createdatavisualizations and dashboards to communicate findings to stakeholders.Conduct A/B testing and other experiments to optimize business processes. Stay updated with the latest trends and advancements indatascience andmachine learning. Required Qualifications and experience:Bachelors in computer science,DataScience, or a related field, with a focus on databases anddataanalytics, or equivalent combination of education, training, and experience is required. 2-4 years of previous related job experience. Experience with tools and systems on Microsoft BI Stack, including SSRS and TSQL, Power Query, Power BI. Experience in Microsoft Fabric and Azure DevOps would be considered an asset. Knowledge of database fundamentals such as multidimensional database design, relational database design. Creating complex SQL queries. Understanding of software development architecture as well as technical aspects.Analyzing data in Microsoft Access databases, Excel sheets and summarizing data with pivot tables. Proficiency in programming languages (e.g., Python, R). Ability to handle complex datasets and derive actionable insights. Extensive experience performing data analysis and strong data visualization skills. Ability to convert business requirements into technical specifications and to analyze complex datasets and derive actionable insights. High degree of trust handling confidential information. Adaptable and willing to learn new techniques. Ability to adapt quickly to change and work under pressure with tight deadlines. Self-starter who can initiate, prioritize, and organize projects and decisions to meet deadlines consistently. Strong interpersonal skills with the ability to work independently and within a team environment, taking direction from others and providing guidance and expertise to numerous internal cross-functional groups and external stakeholders. Excellent written and verbal communication skills to present findings to non-technical stakeholders."
Oeson,Data Science Intern,"Projects You Will Work On:- Finance Fraud Detection:Develop advanced fraud detection algorithms leveraging financial data analysis.- Recommender System: Contribute to personalized recommendation systems, enhancing user experiences across platforms.- Sentiment Analysis: Explore sentiment analysis to extract insights from textual data, shaping user sentiment understanding.- Chatbots: Engage in intelligent chatbot development, revolutionizing customer interactions and support.- Image/Audio Video Classification: Push boundaries with multimedia technology by working on image and audio video classification projects.- Text Analysis: Uncover hidden patterns in textual data through sophisticated text analysis techniques. Roles & Responsibilities:- Collaborate with our esteemed data science experts to collect, clean, and analyze extensive datasets, honing skills in data preprocessing and visualization.- Contribute to the development of predictive models and algorithms, employing cutting-edge machine learningtechniques to solve real-world challenges.- Work closely with team members to design, implement, and evaluate experiments, fostering a collaborative and innovative environment.- Stay updated with the latest industry trends and best practices indatascience, applying newfound knowledge to enhance project outcomes. Qualifications:- Currently pursuing any degree showcasing a strong commitment to continuous learning and professional growth.- Exceptional written and verbal communication skills, vital for effective collaboration and articulation of complex ideas.- Demonstrated ability to work both independently and as part of a cohesive team, highlighting adaptability and strong teamwork capabilities."
Darkhorse Emergency,Data Analyst / Consultant,"What you'll be doing: Work at Darkhorse Emergency is diverse and ever-changing. We implement our proprietary software and solve interesting and challenging business problems for a wide variety of fire and emergency clients. Every client is different. The interpretation of their data and unique needs requires strong analytical skills. You will work to understand our clients'data and bring it into our proprietary software and provide them with an in-depth analysis that highlights root causes and opportunities. You will work with a team of analysts and take part in the project life cycle from start to finish, including: Gathering and interpreting Fire and EMS data.Our data can come from different sources, including client tools and open databases, as well as flat files. Bringing data together in our databases will be a big part of what you do here. Data cleaning and summary analysis. Data is rarely what you want it to be. Cleaning and understanding your inputs are the first required step in any quantitative project. Implementing our Darkhorse Emergency Platform. Matching clients' unique data to the fields required for our platform is a crucial step, and requires business context as well as technical savvy. Validating findings and stories in the data. Do your findings pass the smell check? A mix of experience, expertise and creativity is required to validate your findings and distinguish the right signals from data errors. Creating stunning data visualizations. Using tools like QGIS, Jupyter Notebooks or even Excel, you will work with the data to create visually engaging visualizations and maps to clarify our findings. Creating succinct presentations and reports. Often times our analyses need to be translated into a report that is readable, actionable, and isn't going to collect dust on the shelf. We believe less is more, and we take the time to make it succinct.Translating client needs into achievable solutions. Using your understanding of the industry and the client's unique challenges, you will provide our clients with actionable recommendations. Understanding our clients means that we can confidently approach their business problems. Brainstorming creative approaches. Theory and practice never align, so creativity is a must in planning a solution. Improving our tools and processes. As a growing company, we must continually search for the better way. We expect you to spend a good portion of your time on improving processes. Maintaining client relationships. As a SaaS company, You will become the trusted go-to person for our clients. What we look forWe believe the right person is not defined by years of experience or education. We look for the right people at all levels of their career. Here are some of the things that will make you stand out: You are a quick learner. The tools we will use next year have not been invented yet. People thriving on continuously learning and growing do well here.You are a wizard withdata, and comfortable using some combination of Excel, SQL, and Python. You ideally have some experience using GIS tools like QGIS to visualize data geographically. You genuinely enjoy problem-solving and finding patterns. The harder the challenge, the harder it is to put it down until you crack it.You enjoy working with clients, learning about their values and challenges, and use our tools to make a difference.You are able to focus on the most critical parts. In a haystack ofdata, you are able to find the needle that can pinpoint issues or root causes.You can communicate well. Having the correct analytical solution is one thing, but you should also be able to develop your solution methodically and explain the whole thing to your grandmother in the end.You are motivated and comfortable immersing yourself indataand understanding the message behind it. You can be skeptical about your findings if they don't seem correct. You're detail-oriented but can see the big picture.You are creative in your ways of interpreting and analyzing the data. When one avenue leads to a dead-end, you can use different data sources to come to an outcome that satisfies the business need"
Invoke Co.,Data Engineer (Part-Time Contractor),"Responsibilities: Work alongside our development team to deliver high quality products. Collaborate with the product and strategy teams on data requirements and use cases. Research & stay up to date on modern technologies. Assist in the design, development, and maintenance of data pipelines and ETL processes. Perform data sourcing, organization, cleaning and transformation tasks. Collaborate with technology team to ensure data quality and applicability for training machine learning models Develop scripts and tools for data processing and automation Troubleshoot and resolve data-related issues and inconsistencies. Support machine learning model training iterations. Requirements: Familiarity with Python. Experience with data modeling, data cleaning, and data transformation Basic understanding of cloud platforms (e.g., AWS, Azure, Google Cloud) and their data services. Solid understanding of machine learning principles and data requirements. Understanding of data privacy and security principles Strong analytical and problem-solving skills You’re curious to work in an Agile, Scrum-based environment with a team or on your own You’re excited at the opportunity to create quality user experiences You love to collaborate with all types of people including Design, User Experience and Strategy. Completed/Earning a Bachelor’s degree in Computer Science or a related field"
Accentrust,Data Engineer,"As a member of Accentrust, you will work on various aspects of the data engineering lifecycle, from data modeling and ETL processes to data warehousing and analytics. You will collaborate closely with our product team, engineers, and stakeholders to design and implement scalable data pipelines and infrastructure. You’ll also play a critical role in transforming raw data into valuable insights that drive business decisions. This role is ideal for someone with a strong foundation in data engineering who is eager to take ownership of impactful projects and work in a collaborative, fast-paced environment. Responsibilities:Design, develop, and maintain scalable ETL pipelines for data collection, transformation, and storage. Collaborate with cross-functional teams to understand data requirements and build efficient data models and warehouses. Develop, test, and optimize SQL queries and data pipelines to ensure efficient data processing and retrieval. Implement best practices for data governance, including data quality, integrity, and security. Work with cloud-based data platforms and tools to manage and process large-scale datasets. Monitor and troubleshoot data pipelines to ensure reliable data flow and real-time availability. Provide actionable insights by analyzing data and generating reports. Build and manage web scraping pipelines using tools such as Scrapy,Selenium, or BeautifulSoup to collect and process data from various sources. Stay informed about the latest trends in generative AI technologies (e.g., GPT, BERT, LLAMA) and understand how they can be leveraged in data engineering processes. Develop data visualizations using Tableau, Power BI, Matplotlib, or Seaborn to create dashboards and custom visualizations that support decision-making and data-driven insights. Embrace modern tools and workflows like Git, Docker, and CI/CD to enhance operational efficiency. Stay up-to-date with the latest trends and technologies indataengineering and apply them to improve our systems.Qualifications:Bachelor’s or Master’s degree in Computer Science,Data Science or a related field.At least 1 year of relevant work experience (including internships or project work as part of a Master’s program). Strong proficiency in programming languages like Python and SQL. Experience with ETL processes, data modeling, and data warehousing solutions. Familiarity with cloud-based platforms such as AWS, Google Cloud, or Azure. Familiarity with NoSQL and SQL databases (e.g., MongoDB, MySQL, PostgreSQL). Proficiency in using web scraping tools like Scrapy, Selenium, BeautifulSoup for data extraction from various websites. Experience with data transformation and analytics tools such as Pandas and NumPy. Basic understanding of generative AI models(e.g., GPT, BERT, LLAMA) and how they can be applied to data processing or analysis. Familiarity with Tableau, Power BI, Matplotlib, or Seaborn for data visualization and reporting. Strong problem-solving skills and ability to work independently or collaboratively within a team. Familiarity with Docker, CI/CD pipelines, and modern DevOps practices is a plus. Experience with machine learning pipelines or advanced analytics is a bonus"
TikTok,Machine Learning Engineer Intern (Data-TnS-Algo) - 2024 Start (BS/MS/PhD),"Responsibilities: Build industry-leading content safety systems for TikTok. Develop highly-scalable classifiers, tools, models and algorithms leveraging cutting-edge machine learning, computer vision and data mining technologies. Understand product objectives and improve trust and safety strategy and model's performance. Work with cross-functional teams to protect TikTok globally. Qualifications Currently pursuing a BS/MS/PhD degree in Software Development, Computer Science, Computer Engineering, or a related technical discipline.• Solid experience in at least one of the following areas: recommendation system,machine learning, pattern recognition, NLP,data mining, or computer vision. Experience in recommendation, search, advertising, or other related projects. Published papers in the top AI conferences or journals is a plus, including KDD, IJCAI, WWW, WSDM, ICML, NeurIPS, CVPR, ECCV, ICCV, ACL, etc. Competition experience in machine learning,data mining, CV, NLP, etc. Good understanding of data structures and algorithms. Passion for techniques and solving challenging problems.​"
Clio - Cloud-Based Legal Technology,Data Scientist,"What you'll work on: Collaborate with the Clio products teams to refine business problems, develop hypotheses, and provide input that drives growth. Suggest new questions about our business, product, and customers that lead to impactful insights. Work with other team members to develop predictive AI and ML solutions and deploy them in production. Apply rigorous statistical analysis and data mining techniques to evaluate impact of different product features and other business initiatives. Employ statistical analysis, machine learning, GenAI, LLMs, etc. to unlock new product opportunities. Support scientific thinking in product and business teams by enabling discussions with data, disseminating best practices, and leading by example. Effectively communicate complex technical concepts and findings to both technical and non-technical audiences. What you may have: 3+ years applied experience in data science.The ability to translate business requirements into data science solutions. Experience in developing analysis in Python and experience with relevant ML libraries and frameworks (e.g., pandas, PyTorch, scikit-learn) Strong team player mindset, while able to work under your own initiative and prioritize time and tasks effectively. Excellent written and verbal communication skills. Ability to write structured SQL queries for answering questions and manipulating data. Experience in analytics working with product and user behaviordata, e.g., retention or churn analysis. Experience with building ML/AI pipelines and relevant tools (e.g., Kedro, MLFLow) Experience with large datasets and user behavior data. Experience with NLP and LLMs. A graduate degree in a relevant quantitative discipline (computer science, statistics, mathematics, physics, engineering)"
theScore,"Data Analyst","As a key member of our Data Analytics team, you will:Help enhance the useability and adoption of a data exploration tool across the organization. Reduce query and model run times, simplify inter-DAG dependencies, and continuously improve on best practices and documentation. Liaise with analytics engineers to build curated data modelsand solve related issues. Assist team members in solving data issues related to both analysis and modeling. Develop a deep understanding of how users interact with theScore/ESPN Bet apps and websites. Complete analyses and present clear recommendations to business stakeholders. Identify opportunities to apply data modeling and advanced analytics techniques to drive insights. Peer review team's analyses to ensure data and insights shared are accurate and informative. About You: University degree in Computer Science,Statistics, Business, or related field. Strong knowledge of relational databases and SQL. Experience with data exploration tools such as: Looker, Mode, ThoughtSpot, Tableau, Power BI, and Periscope/Sisense. Experience with dbt or dbt Cloud. Experience with modern data warehouses such as: Redshift, BigQuery, Snowflake, and Synapse. A passion and curiosity for solving analytical problems using quantitative approaches. Ability to take complexdataand present it in a clear manner to cross-functional groups. Ability to focus in a fast paced environment and multitask. Excellent written and oral communications skills. Knowledge of analysis tools; R or Python packages preferred."
DataAnnotation,Data Engineer,"You will work with the chatbots that we are building in order to measure their progress, as well as write and evaluate code.To apply to this role, you will need to be proficient in either Python and/or JavaScript. Your role will require proficiency in at least one programming language (JavaScript, Python, C#, C++, HTML, SQL, or Swift) in order to solve coding problems (think LeetCode, HackerRank, etc). For each coding problem, you must be able to explain how your solution solves the problem Responsibilities: Come up with diverse problems and solutions for a coding chatbot. Write high-quality answers and code snippets. Evaluate code quality produced by AI models for correctness and performance. Qualifications Proficient in either Python and/or JavaScript. Excellent writing and grammar skills. A bachelor's degree (completed or in progress). Previous experience as a Software Developer, Coder, Software Engineer, or Programmer."
MethodHub,Data Engineer,"Skills -Synapse/PysparkNotebook/Pipeline development Data warehousing Experience must. ETL pipelines using Pyspark must. Bitbucket code management must. CICD knowledge must.Understanding and implementation knowledge on various Microsoft Azure Tools (ADF, DataBricks, Azure Synapse) preferred. Knowledge and understanding of MS Azure DevOps preferred. Should have experience in Agile methodology must. Retail experience is preferred."
StackAdapt,Data Engineer,"What you'll be doing: Design modular and scalable real time data pipelines to handle huge datasets. Understand and implement custom ML algorithms in a low latency environment. Work on microservice architectures that run training, inference, and monitoring on thousands of ML models concurrently. What you'll bring to the table: Have the ability to take an ambiguously defined task, and break it down into actionable steps. Have deep understanding of algorithm and software design, concurrency, and data structures. Experience in implementing probabilistic or machine learning algorithms. Interest in designing scalable distributed systems. A high GPA from a well-respected Computer Science program. Enjoy working in a friendly, collaborative environment with others"
Experis Canada,Data Engineer,"The ideal candidate will have excellent problem-solving skills, strong communication skills, and the ability to work independently as well as collaboratively in a team. What's the Job? Provide data and implementation support for strategic projects in the our clients Digital Innovation Team.Optimize code run time to meet critical business SLA. Write Python code to extract, clean, and preprocess data from various sources including web scraping to ensure quality and integrity of the data for use in model development. Work closely with modelers/data scientists to support their data transformation, analyses, and model coding activities. Design, develop, and maintain ETL pipelines and APIs to host complex algorithms in an Azure production environment. What's Needed? B.Sc. Degree or higher in Computer Science or Information Technology. 5+ years proven hands-on working experience in data engineering and/or software development, with a strong focus on Python. Expert in data manipulation on large datasets with advanced programming/scripting skills in Python. Building high-performance APIs using Python and Microsoft Azure. Implementing automated ETL data pipelines using Python hosted on Microsoft Azure."
Updata,ML Specialist,"Key Responsibilities: Conduct exploratory data analysis (EDA) and visualize data using various libraries. Build, deploy, and monitor machine learning models.Implement computer vision and time series analysis techniques in projects.Implement and manage MLOps workflows and pipelines on cloud platforms. Utilize big data services and manage data pipelines.Collaborate with teams to integrate research code into industrial applications. Qualifications: Strong experience with Python, including advanced features and real-world application development.Proficiency in cloud platforms, including MLOps, data storage, serverless functions, and cloud security.Experience with tools and techniques such as Scikit-learn, OpenCV, PyTorch, and TensorFlow. Active participation in open-source contributions, workshops, or webinars is a plus. Excellent problem-solving skills and a keen eye for detail.Strong communication skills, both written and verbal."
Electronic Arts (EA),Machine Learning Scientist,"Your Responsibilities: Contribute to the ML research strategy to create new player experiences, exploring frontier technologies to shape the future of Sports Gaming Work closely with the engineering team to support experiments with tooling and platforms, as well as data acquisition and management Share your results through presentations, papers, prototypes and compelling interactive demonstrations. Stay abreast of the latest advancements in relevant technologies and propose impactful projects to drive innovation Collaborate with game teams to understand their design goals and build strategies to discover new engaging experiences. Collaborate with a diverse range of partners, including research, engineering, and game development teams. Your Qualifications: Masters in Computer Science, mathematics or related field, with experience in research Experience with machine learning and familiarity with multiple ML techniques such as transformer models and diffusion models Technical background, experience working with both research engineering, and a proven track record of going from idea to implementation"
Updata,ML Specialist,"Key Responsibilities: Conduct exploratory data analysis (EDA) and visualize data using various libraries. Build, deploy, and monitor machine learning models.Implement computer vision and time series analysis techniques in projects.Implement and manage MLOps workflows and pipelines on cloud platforms.Utilize big data services and manage data pipelines.Collaborate with teams to integrate research code into industrial applications. Qualifications: Strong experience with Python, including advanced features and real-world application development.Proficiency in cloud platforms, including MLOps, data storage, serverless functions, and cloud security.Experience with tools and techniques such as Scikit-learn, OpenCV, PyTorch, and TensorFlow. Active participation in open-source contributions, workshops, or webinars is a plus. Excellent problem-solving skills and a keen eye for detail.Strong communication skills, both written and verbal."
Electronic Arts (EA),AI Data Scientist,"As a Data Scientist you will: Apply current and emerging techniques in deep learning, natural language processing and other machine learning areas.Collect, clean, manage, analyze and visualize large sets of data using multiple data platforms, tools and techniques.Work in partnership with game teams to integrate AI solutions into products and services. Optimize and fine-tune AI models for performance and scalability. Required Qualifications And Experience: Graduate degree in statistics, mathematics, computer science, or related field: Expertise in deep learning and fluency with machine learning frameworks and libraries (e.g. Scikit-Learn, PyTorch) Fluent in Python and good knowledge of SQLExposure to Cloud environments (e.g. GCP, AWS, or Azure) Exposure to devops tools and principles (Git, CI/CD, Docker) Excellent problem-solving skills and attention to detail. Ability to work collaboratively in a team environment Strong communication skills, both written and verbal"
Canonical,Python and Kubernetes Software Engineer - Data AI/ML & Analytics, "What your day will look like: Develop your understanding of the entire Linux stack, from kernel, networking, and storage, to the application layer Design, build and maintain solutions that will be deployed on public and private clouds and local workstations Master distributed systems concepts such as observability, identity, tracing Work with both Kubernetes and machine-oriented open source applications. Collaborate proactively with a distributed team of engineers, designers and product managers Debug issues and interact in public with upstream and Ubuntu communities Generate and discuss ideas, and collaborate on finding good solutions What we are looking for in you Professional or academic software delivery using Python Exceptional academic track record from both high school and university Undergraduate degree in a technical subject or a compelling narrative about your alternative chosen path Confidence to respectfully speak up, exchange feedback, and share ideas without hesitation Track record of going above-and-beyond expectations to achieve outstanding results Passion for technology evidenced by personal projects and initiatives The work ethic and confidence to shine alongside motivated colleagues Professional written and spoken English with excellent presentation skills Experience with Linux (Debian or Ubuntu preferred)  Excellent interpersonal skills, curiosity, flexibility, and accountability Appreciative of diversity, polite and effective in a multi-cultural, multi-national organisation Thoughtfulness and self-motivation  Result-oriented, with a personal drive to meet commitments  Hands-on experience with machine learning libraries, or tools. Proven track record of building highly automated machine learning solutions for the cloud. Experience with container technologies (Docker, LXD, Kubernetes, etc.) Experience with public clouds (AWS, Azure, Google Cloud) Working knowledge of cloud computing Passionate about software quality and testing Experience working on an open source project"
Primate Labs Inc., Machine Learning Software Developer, "Responsibilities As a Machine Learning Software Developer, your primary responsibility will be to investigate and develop machine learning benchmarks. The benchmarks include both computer vision AI models (e.g., Object Detection, Image Segmentation) and generative AI models (e.g., LLMs such as LLaMA, text-to-image models such as Stable Diffusion). The benchmark tests will target high-end mobile devices and desktop systems. You will also be responsible for communicating experimental results from the benchmark tests both internally to project stakeholders and externally to representatives from hardware companies. Qualifications The ideal candidate has the following background and skills: Experience with C++ and Python. Experience with at least one machine learning toolkit (e.g., TensorFlow, PyTorch). An understanding of neural networks and how they work. Strong written and oral communication skills. Able to work independently with minimal supervision. Able to conduct research and problem solve independently The following skills are nice to have but are not necessary: An understanding of generative AI models (e.g., LLMs, Stable Diffusion) Experience working with a variety of operating systems (e.g., Linux, Windows, macOS). Experience training deep learning models."
ThinkCX, Junior Data Engineer 2024, "Responsibilities: Mine large, complex sets of data sets for insights on customer behavior. Work with product and sales team to design meaningful client focused solutions. Work with data analytics team to engineer feature sets. Explore vast data sets for new device insight sources or product applications. Discover new uses to improve key model metrics. Optimize existing production models. Minimum Qualifications: Bachelor’s degree in a quantitative discipline (e.g., statistics, economics, computer science, data science, mathematics, physics, electrical engineering, etc.). Demonstrated effective written and verbal communication skills. Proven self starter and attention to detail. Applied experience with SQL and languages/software such as (Python, R, pandas, MATLAB etc.). Preferred Skills: Can translate data analysis into business recommendations Experience with cloud services like AWS, Azure, Google Cloud Applied experience with machine learning on large datasets Familiarity with data warehousing platforms like Amazon Redshift, Azure Data Warehouse Love solving problems and have analytical skills to boot Have a cool side project or a GitHub/Kaggle link? We're keen to see it!"
Homebase, Applied AI Scientist (Hybrid), "You will make an impact by Drive advancements in LLMs and other AI technologies, navigating a rapidly evolving landscape over the coming years. Leverage pre-existing models, train or fine-tune new ones, and oversee their deployment, scaling, inferencing, and monitoring. Tackle critical business problems across diverse domains with innovative AI solutions. Develop metrics to evaluate the end-user impact of ML models and ensure their alignment with business goals. Effectively communicate data science experiment results to non-technical stakeholders. Collaborate with data, platform, and product engineering teams to productionize models for real-time, batch, or streaming applications. Support Product, Engineering, and leadership teams with data-driven decision-making.You are a bar raiser, which means you come with A strong foundational understanding of ML principles. Experience working with LLMs in industry or academia. Adaptability to learn and evolve as newer AI models emerge. Proficiency in SQL and Python for data manipulation and model development. The ability to break down complex business problems into manageable components. Exceptional problem-solving skills and meticulous attention to detail. Strong communication and collaboration skills to work effectively with cross-functional teams."
CGI,AI Engineering Specialist,"Your future duties and responsibilities Design, implement, and optimize machine learning algorithms and models, including supervised, unsupervised, and reinforcement learning techniques. Collect, clean, preprocess, and transform large datasets from various sources to ensure they are suitable for model training. Train AI models on large datasets, optimizing hyperparameters to enhance model accuracy and performance. Deploy trained AI models into production environments, ensuring they are scalable, reliable, and maintainable. Continuously monitor the performance of deployed models, retraining and updating them as necessary to adapt to new data and conditions. Develop and maintain the necessary infrastructure, including data pipelines, model training frameworks, and deployment platforms, to support AI development. Work closely with software engineers and IT teams to integrate AI solutions with existing systems and applications. Collaborate with cross-functional teams, including data scientists, data engineers, product managers, and business stakeholders, to align AI projects with business objectives. Stay informed about the latest advancements in AI and machine learning, experimenting with new tools, techniques, and technologies to drive innovation. Implement and adhere to ethical guidelines and principles, ensuring AI solutions are fair, transparent, and accountable, and mitigating biases in models."
CGI, Machine Learning Engineering Specialist, "Your future duties and responsibilities Develops and implements machine learning models and algorithms tailored to specific use cases and business objectives. Collects, preprocesses, and cleanses large volumes of structured and unstructured data from various sources, ensuring data quality and integrity. Engineers relevant features from raw data to improve model performance and accuracy, employing techniques such as feature scaling, transformation, and selection. Trains machine learning models using appropriate algorithms and techniques, optimizing hyperparameters to achieve desired performance metrics. Evaluates model performance using metrics such as accuracy, precision, recall, F1 score, and ROC curves, iteratively refining models based on feedback and experimentation. Deploys trained machine learning models into production environments, ensuring scalability, reliability, and efficiency, and integrating models with existing software systems and workflows. Collaborates with cross-functional teams, including data scientists, software engineers, business analysts, and domain experts, to align machine learning initiatives with business objectives. Visualizes and interprets model outputs and insights, communicating findings effectively to stakeholders through dashboards, reports, and presentations. Stays updated with the latest advancements in machine learning research and techniques, experimenting with new algorithms, libraries, and tools to drive innovation and improve model performance. Documents the development process, including methodologies, code, experiments, and results, to ensure reproducibility, knowledge transfer, and compliance with regulatory requirements and data privacy standards."
Capital One, Associate Data Scientist Full-Time 2025, "To give you an idea of what they do, take a look below� Using machine learning and other data mining techniques to get insights from massive datasets, such as identifying fraud rings Understanding what problems create consumer calls through data mining interaction records Analyzing online banking patterns Mining vast amounts of data to identify trends in credit risk Using a combination of quantitative analysis and statistical methods to develop strategic insights and recommendations to optimize products or programs Adapting the latest technologies in modelling and data mining to design scientific tests that optimize dollars spent on new initiatives and marketing channels Building statistical models to form the foundation of business decisions and regularly monitoring the performance of those models so corrective action can be taken What would be great for you to have: Pursuing a degree in Mathematics, Statistics, Computer Science, Engineering, or another quantitative discipline Experience analyzing and manipulating large data sets using tools such as SQL or Python through co-op or school programs Experience with statistical analysis and data mining using tools such as R, SAS, Matlab, Stata, or SPSS through co-op or school programs Experience building and maintaining data pipelines Experience in a Linux/Unix environment, with Git, AWS, or APIs through co-op or school programs is considered an asset Excellent verbal and written communication skills"
BGIS, Jr. Data Scientist - AI and Advanced Analytics, "Key Duties & Responsibilities Analysis and Modelling Utilize your educational background in a quantitative field, such as computer science, mathematics, or engineering, to analyze complex datasets and develop predictive models. Apply your expertise in analytics to solve real-world problems, using machine learning, artificial intelligence, linear programming, generative AI, NLP/Large Language Models (LLMs) or other data science techniques. Design, build, and maintain robust feature engineering, data pipelines and ETL processes, ensuring data accuracy and accessibility for analysis and modeling. Create optimization models for strategic resource allocation and operational efficiency. Best practices and Continuous Learning Showcase your aptitude for continuous learning by staying up to date with the latest advancements in data science and technology. Maintain a strong understanding of data warehousing, cloud technologies, and big data solutions, constantly updating and improving our data and analytics infrastructure. Solutions and Business Strategy Harness your passion for problem-solving to identify opportunities for improvement and drive data-driven decisions. Collaborate with cross-functional teams to ensure the successful implementation of data-driven solutions. Demonstrate strong communication skills, effectively translating technical insights into actionable business strategies. Communication Collaborate closely with data scientists and business analysts, providing them with clean, structured data outputs from your analytic work to facilitate business decision making. Knowledge & Skills Bachelor's degree or higher in a quantitative field (Computer Science, Mathematics, Engineering, etc.). Work experience in data analytics, machine learning, AI, or data science is a nice to have. Understanding of delivering actionable insights and solutions from complex data and analytic models. A natural curiosity and enthusiasm for tackling challenging problems. Excellent communication skills with the ability to convey technical concepts to non-technical concerned parties. Proficiency in data manipulation and programming languages such as Python, R, or SQL. Familiarity with data visualization tools and techniques [Power BI, Tableau, Python libraries). Exposure and growing familiarity with LLMs/Generative AI and potential applications."
HelloFresh, Data Engineer, "You will : Work together in a cross-disciplinary team of Data Scientists, Data Analysts, and Data Engineers to evolve our data pipeline, warehouse, and workflow systems to be more resilient, extensible, and maintainable Partner with the analytics and data science teams to provide the data and tools they need to solve complex problems Identify improvements in the flow of our data, including designing tests Work with new technologies (both self-managed and services) for moving, transforming, modeling, and storing data and make recommendations Build and maintain complex and scalable ETL pipelines Collaborate with various teams to design a data analytic solution and data model that meets the needs of the business You are: Collaborative: you can work hand in hand with stakeholders, data scientists, engineers, and other data professionals to identify needs and opportunities for improved data management and delivery. Sense of ownership: you take responsibility for your projects and pride in your work. Strategic & critical thinking: you get the smaller and the bigger picture. Creative critical thinker and problem solver with meticulous attention to detail Ability to function in a fast-paced environment and have the flexibility to change course based on business needs or environmental conditions At a minimum, you have: Bachelors in a STEM subject. E.g. Computer Science, Data Science, Math, etc At least two years two years experience designing and building highly scalable data pipelines using Airflow & Databricks At least two years of professional experience in Data Warehouse/Data Lake. Past experience in Marketing Analytics is a plus. Experience with Python, Spark, Databricks, DBT, Terraform, Snowflake, AWS (EMR, EC2, S3, etc.) Strong SQL, strong knowledge of databases, and experience in developing data pipelines. Solid knowledge of Python (packages: PySpark/ Pandas & Numpy). Experience with Snowflake & Databricks is a plus. Technical skills, including architecture/design, data modeling, and developing services An understanding of database design and ETL practices Excellent verbal and written communication skills with the ability to effectively advocate technical solutions to engineering teams and business audiences"
Ambyint, Data Scientist, "What you"ll do Combine engineering principles and domain knowledge with data science techniques to solve complex challenges in artificial lift optimization Develop, operationalize, and maintain advanced machine learning models that focus on increasing production efficiency and reducing downtime Analyze large, complex datasets from IoT sensors and industrial systems to identify anomalies and opportunities for operational improvements Communicate complex data insights and deliver technical presentations to internal teams, explaining model functionality, results and recommendations for improvement Collaborate as a part of a cross-functional team to understand business problems, identify requirements and create data-driven solutions Qualifications Bachelor's degree or higher in engineering discipline, Mechanical or Petroleum preferred Demonstrated experience with oil and gas production operations such as artificial lift design/installation/optimization, production surveillance, failure identification, or reservoir engineering Knowledge of data science and machine learning methods, ideally within industrial, IoT, or energy sectors Familiarity with SCADA data, IoT data pipelines, and industrial control systems Proficiency in Python with a solid understanding of machine learning libraries (e.g., scikit-learn, TensorFlow, PyTorch) Experience working with large-scale datasets, SQL, and distributed computing platforms Strong foundation in statistics, optimization, and predictive modeling Excellent problem-solving skills and the ability to work both independently and in a collaborative environment"
TD, Data Scientist I, "Job Functions Document Business Intelligence requirements for our Business partners; Document business rules and validate data mappings; Perform impact analysis Liaise with different teams within Platform and Technology or with other internal partners to ensure data availability and accuracy Collaborate with all members of the Agile team to achieve successful implementations Flexibility to support other team members Communicate business needs to the teams responsible to define the business rules and development, so that they can design and propose solutions. Support related teams in the investigation and design of business solutions; Collaborate in the selection of the best solutions; Present and validate with the business partners the alternative(s) of proposed solution(s). Manage stakeholder expectations Must be able to understand different operational systems: systems functioning of related frontend systems or other backend databases what they contain (understanding available data, investigating data, etc ...). Must show strong adaptability to discuss with different stakeholders in terms of expertise, training and hierarchy. Skills At least 2 years of relevant experience in Business Intelligence projects/Reporting. Good knowledge in data extraction and manipulation (Databricks, SQL or equivalent) Good communication: Bilingual (English and French). Good time management skills to support multiple initiatives of different complexity Demonstrate analytical skills and critical thinking Be quality conscious Can negotiate with business partners Work efficiently in a team setting. Assets Knowledge of the Insurance Industry Jira/Confluence, Agile methodology. Knowledge of Python, Tableau"
Movable Ink, Machine Learning Engineer, "Responsibilities: Generate insights into customer behavior and derive modeling ideas for improving our content recommender system Work with data engineers to define what additional customer data we might want to collect and help make it available in a format suitable for modeling purposes Create meaningful machine-learning features that improve our content recommender's performance measured through offline metrics and online a/b tests Build machine learning models and deploy them as part of our recommender system Qualifications: Master's degree or equivalent experience (2+ years) in a relevant field or industry Solid understanding of machine learning fundamentals High comfort level in Python or other programming language Familiarity with an ML stack such as typical scientific Python libraries (pandas, numpy, sklearn, xgboost) or deep learning frameworks (we use Pytorch) Familiarity with data analysis through SQL or a big-data processing framework such as Spark Ability to collaborate with technical partners you'll be working closely with other teams to determine requirements for your work and to make design decisions that affect our stack The idea of writing and deploying production code, and getting real-world feedback on your models excites you A desire to always be learning and contributing to a collaborative environment"
Spin Master, Data Scientist, "How will you create impact? Exploratory Data Analysis: Collect, clean, and preprocess large datasets from various sources, ensuring data quality and consistency. Perform EDA to uncover patterns, trends, and relationships within the data. Machine Learning: Design, develop, and implement machine learning models to address business challenges and improve decision-making processes by utilizing GCP's ML tools such as AutoML, BigQuery ML and TensorFlow to build scalable models. Continuously monitor and refine models to ensure accuracy and effectiveness. Deploy machine learning models on GCP using Vertex AI. Ensure models are optimized for performance, scalability, and cost-effectiveness. Experimentation and Testing: Design and execute A/B tests, experiments, and other statistical tests to measure the impact of different business strategies. Analyze experiment results and provide actionable recommendations to stakeholders. Collaboration and Communication: Work closely with data engineers, program manager, data visualization engineers and other stakeholders to understand business needs and translate them into technical requirements. Present results and findings to non-technical stakeholders in a clear and concise manner. Collaborate with data engineers to deploy and integrate machine learning models into production systems. Goolge Cloud Platform Utilization: Leverage GCP's suite of products, including BigQuery, Vertex AI, Cloud Storage and Power BI/Tableau to manage and analyze data. What are your skills and experience? Experience in data science, with a focus on using machine learning for higher accuracy and improved time to market on projects delivery. Bachelor's degree in data science and/or computer science and/or statistics and/or mathematics, or a related field. Experience in cloud platforms like Google or Azure or AWS. Technical Skills: Strong programming skills in Python, R, with experience in data science libraries such as pandas, scikit-learn, TensorFlow, or PyTorch. Proficiency in SQL for data querying and manipulation. Experience with data preprocessing, feature engineering, and model evaluation techniques. Experience with data visualization tools (e.g., Power BI, Tableau, Matplotlib) to create clear and impactful visualizations. Strong knowledge of statistical analysis, hypothesis testing, and machine learning techniques."
Loblaw Digital, Machine Learning Software Engineer, "What You'll Do Design, build, and maintain highly scalable, robust, and efficient cloud infrastructure using Google Cloud Platform (GCP) services, including Vertex AI, BigTable, BigQuery, and Cloud Composer. Develop automation and orchestration of ML pipelines, integrating data ingestion, feature engineering, training, and deployment processes. Collaborate with cross-functional teams to understand their needs and build solutions that improve platform usability, scalability, and the overall development experience. Optimize data processing pipelines and cloud resources to ensure low-latency, cost-effective operation. Implement monitoring, alerting, and failover strategies to ensure platform reliability. Stay updated with industry trends and best practices in cloud engineering, data engineering, and machine learning Does this sound like you? Customer-centric mindset: Passionate about delivering an exceptional experience for data scientists through a self-service platform, reducing friction in their workflows. Collaboration: Strong communication skills to work closely with cross-functional teams, including data scientists and engineers, to ensure platform features meet user needs and expectations. Problem-solving: Ability to identify and solve complex technical issues related to ML pipelines, cloud infrastructure, and scalability, ensuring an efficient and robust platform. Automation-first approach: Commitment to streamlining and automating processes for scalability and reliability, enabling data scientists to focus on experimentation and model development. Adaptability: Ability to quickly adjust to new technologies and evolving platform needs to keep the infrastructure cutting-edge and efficient. Ownership and initiative: Comfortable taking ownership of key platform components, driving innovation and improvements that benefit the platform's scalability and usability. Bachelor's or Master's degree in Computer Science, Engineering, or a related field. 2+ years of experience in software engineering with a focus on cloud infrastructure and/or data engineering. Hands-on experience with Google Cloud Platform services such as Vertex AI, BigTable, BigQuery, Cloud Composer, Cloud Storage, etc. Proficiency in one or more programming languages such as Python, Java, and SQL. Experience with orchestration tools such as Apache Airflow (Composer). Knowledge of CI/CD pipelines and DevOps tools for continuous integration and deployment. Familiarity with containerization and orchestration (Docker, Kubernetes). Strong problem-solving skills and attention to detail. Excellent communication skills and ability to work in a collaborative, fast-paced environment"
Medeloop, Machine Learning Engineer, "What You'll Do As a Machine Learning Engineer at Medeloop, you will be responsible for architecting AI-driven data pipelines, designing cutting-edge algorithms, and developing models to analyze complex healthcare datasets. You will collaborate closely with the product, engineering, and science teams to deliver machine-learning solutions that address critical business needs and drive innovation in healthcare. Key Responsibilities Design and build end-to-end AI solutions that manage and track large-scale healthcare datasets. Develop models and algorithms to analyze healthcare data, ensuring performance optimization. Work closely with customers to understand their requirements and deliver features that align with their needs. Stay up-to-date with industry trends and new technologies in machine learning and healthcare data management. Influence the direction and culture of Medeloop with your initiatives and feedback. Who You Are Strong foundation in theoretical machine learning principles and proficiency with machine learning frameworks such as PyTorch, Transformers, NumPy, and Pandas. Creative thinker with a passion for continuous learning and a deep curiosity to explore and understand machine learning research. Fast, results-driven engineer with a hands-on approach, eager to build and deliver solutions efficiently. Hands-on experience with cloud platforms (AWS, Azure, GCP, etc.), and experience working with large-scale healthcare datasets is a plus. Skilled with development tools and best practices, ready to challenge and overhaul existing features for superior solutions. Exceptional communicator, excited to wear multiple hats and work cross-functionally, adjusting swiftly to the shifting needs of the business while maintaining SOC2 and HIPAA compliance. Unafraid to challenge the status quo and driven by a motivation to innovate, build, and experiment with cutting-edge solutions."
Circle K, Data Scientist - Real Estate, "What You'll Do Develop advanced analytics and predictive models from design through implementation in the areas of real estate site selection forecasts and network optimization. Additional knowledge in areas including pricing and promotion, marketing, and merchandising would also be preferred Participate in the research of analytical methods to find or advance solutions to business problems Clearly and concisely explain complex analytical findings to non-analytical peers and business leaders Create analytic datasets in collaboration with data engineering teams that involves data querying, cleaning, transformation, and feature engineering. Define requirements and test cases for Data validation and monitoring Author programming code (e.g., SQL, Python, R, spark) to assemble and analyze data, following/establishing/enhancing organizational standards and coding practices including code management (i.e. Documentation, use ofGit Hub). Lead and participate in peer code reviews for QA/QC/standards compliance. Train/Mentor other team members, provide developmental support where necessary Follow defined project management processes. Actively participate in project plan development and agile ceremonies (development of backlog, sprint planning, daily standups, retros/reviews) and documentation. Knowledge, Skills And Other Qualifications Required Demonstrated knowledge of SQL, R and Python; Experience querying large datasets using SparkSQL or PySpark; Experience querying geospatial data preferred Experience with ML frameworks such as scikit-learn, Tensorflow, Keras, Pytorch, etc.; Exposure with software development practices, object-oriented principles and test automation; Exposure to version control systems such as Git - experience preferred Experience with cloud-based analytics environments (Azure, AWS or GCP); Experience applying operational research, statistical and machine learning techniques such as regression, time series forecasting, clustering, optimization, etc.; Exposure to agile methodologies using project planning and tracking management tools e.g., JIRA; experience preferred Strong problem-solving skills and ability to troubleshoot complex distributed systems; Strong interpersonal skills, including the ability to communicate the business benefits of analytics; Knowledge of commercial real estate preferred Availability to travel up to 10% of the time Hybrid schedule with a minimum three days in the office Education / Training Required Bachelor's or Masters degree required with a quantitative focus (Statistics, Business Analytics, Data Science, Math, Economics, etc.) Masters degree or Bachelors degree with 1+ year of experience in a data science/advanced analytics role. Experience with Geospatial analytics preferred."
Huawei Canada, Engineer - AI Infrastructure Software, "Responsibilities: Apply relevant AI infrastructure and software/hardware acceleration techniques to build & optimize our intelligent AI/ML systems that improve our products and experiences. Apply your distributed system experience to build & optimize the AI/ML infrastructure for scalability, performance and reliability. Set project technical goals and milestones, provide architecture and design of the AI system, and implement and benchmark the design. Collaborate with multiple teams to deliver the project. Apply in depth knowledge of how the AI/ML infrastructure interacts with the other systems around it. Work with other engineers / research scientists & improve the quality of engineering work in the broader team. Job requirements Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience. Specialized experience in one or more of the following machine learning/deep learning domains: Hardware accelerators architecture, GPU architecture, machine learning compilers, or ML systems, AI infrastructure, high performance computing, performance optimizations, or Machine learning frameworks (e.g. PyTorch), and SW/HW co-design. Experience developing AI-System infrastructure or AI algorithms in C/C++ or Python."
Huawei Canada, Machine Learning Researcher - LLM/RAG, "Responsibilities: Build the benchmark and testbed for advanced LLMs based on Transformer, Mamba. Build the testbed for retrieval-augmented generation (RAG) and optimize the efficiency of the database system. Work closely with the researchers, promptly implement and test the new algorithms and new architectures proposed by the researchers. Research and develop innovative DL architecture and algorithms for large language models (LLM). Contribute to the design, implementation, test, and maintenance of research and development frameworks. Job requirements What you'll bring to the team: Hold a Master or PhD (preferred) in computer science, software or electric engineering or related subjects. Deep understanding of fundamentals and state-of-the-art techniques in NLP and ML. Expertise in NLP using machine learning or deep learning. Strong coding skills mainly in Python, with a focused expertise in PyTorch. Excellent oral and written communication skills. You are curious, like to think outside the box and appreciate complex challenges."
Aspire Software, Data Analyst Engineer, "What your day will look like: Data Acquisition: Identify and payment datasets from various internal and external sources, ensuring data quality and compliance Data Cleansing: Cleanse and preprocess data to remove errors, inconsistencies, and duplicates, ensuring high data quality and reliability Data Transformation: Develop and implement data transformation pipelines to convert raw data into structured formats suitable for analysis and reporting frameworks Database Management: Manage data storage and retrieval systems, including databases, data warehouses, or data lakes Collaboration: Collaborate with data scientists, analysts, and other cross-functional teams to understand data requirements and deliver tailored solutions Documentation: Maintain clear and organized documentation of data pipelines, processes, and data sources Data Analytics: Generate insights, trends, and correlations from collected data Data Visualization: Generate dashboards and other interfaces to expose Analytics. Work with the development team to integrate these into merchant-facing products About You: Bachelor's or Master's degree in Data Science, Information Technology, Computer Science or a similar related field Proven experience in data engineering, data visualization, using languages and frameworks from Python, R or other similar technologies Proficiency in data manipulation and transformation using ETL tools Strong SQL skills for data querying and manipulation Strong Knowledge of data storage solutions like databases (SQL, NoSQL), data lakes, and data warehouses Understanding of the basics of payment processing, including knowledge of payment gateways, credit card processing, and e-commerce, is a plus Excellent problem-solving skills, attention to detail and ability to work independently Ability to work independently and manage multiple tasks simultaneously in a fast-paced, always-evolving environment Strong communication and teamwork skills Detail-oriented with strong organizational skills Fluent in English, both written and verbal, is essential"
Allstate Canada, Claims Intelligence Data Insights Analyst, "You will be accountable for supporting the achievement of ACG's short and long term Mission, Vision, and Strategic Objectives by: Report Re-Design Replace existing stakeholder reports with automated, interactive dashboards enabling powerful data exploration and proactive self-serve analytics Innovate and influence the design of new dashboards with a focus on simplicity and usability Optimize the data pipeline by improving existing scripts / data update processes Provision execution of reporting into data visualization tools by gathering requirements, performing data validation, UAT testing and communicating rollout of new dashboards Build on and improve our existing script and report documentation Stakeholder Initiative Data Support Consolidate analytical and data requirements on project initiatives Consult and provide input on current data libraries and reports to help foster the implementation/creation of new initiatives Determine the best way to collect and aggregate data to build a easily-maintainable data model / report to support the monitoring of stakeholder initiatives and support its success Provide analytical assistance to business partners in enhancing operational excellence and supporting sustainable profitable growth of the business with proactive analytics Ad-Hoc Analytics Respond to ad-hoc insights requests from our stakeholders by pulling the right data, presenting it in a simple and usable way to answer their questions and help them make decisions Simplify data and insights through best practices in data-storytelling and data visualization Qualifications Education or equivalent working experience: Post-Secondary Education, Year of Study 2nd year and above GPA: 3.0/4.0 and above (or equivalent) Skills/Compentencies Exceptional analytical and critical thinking skills Must be able to analyze complex data with high accuracy, be open minded towards change, anticipate client needs and work with others to identify and resolve issues Must be able to coordinate various priorities under time pressure and ensure alignment to corporate objectives and stakeholder priorities Strong communication (written and verbal), collaboration and relationship-building skills Demonstrates strong experience in using data to tell a story influencing business decisions and driving stakeholders to action Demonstrates competency and proficiency in data extraction and transformation, developing optimized data models for data visualization Technical Skills Experience using Power BI and/or Tableau Experience with SQL Extensive experience with Microsoft Suite with an emphasis on Excel and O365 Additional Preferred Skills Experience in the insurance industry, especially with claims data Post-Secondary degree in a quantitative study such as statistics, actuarial science, economics, computer science Experience with Tableau Prep, Python, or R Experience with SAS"
Berkeley Canada, Data Operations Analyst, "What will your typical day look like? Receiving sales data in Excel format from our distributors in a wide variety of shapes, sizes and levels of quality and consistency. Analysing, massaging and re-formatting said data to align with our database requirements. Adjusting and running SQL scripts to load sales data into our database. Adjusting and running SQL scripts to export data from our database into Excel-based reports. Working within a monthly financial close timetable. Identifying and actioning opportunities where diverse data sets and complex procedures could be aligned into fewer, simpler, more standardized forms. Documenting processes and workflows. Qualifications Characteristics of the successful candidate: A strong mind for tabular and relational data - data types, column layouts, relationships, CRUD operations. Intermediate-to-Advanced Excel skills - ability to sort, filter, pivot and re-shape Excel data with ease; ability to make effective use of formulas (text, arithmetic and conditional) and to quickly understand formulas written by others. Basic-to-intermediate SQL skills - ability to write basic SQL statements including joins, where clauses, inserts, updates, deletes; ability to understand scripts written by others and make minor adjustments to them to cater to changes in the underlying data. Superb consistency and attention to detail - ability to spot inaccuracies in one's own work and that of others, particularly in Excel and SQL. Excellent communication skills, particularly when issues arise that require escalation. A sense of urgency and a recognition of the importance of meeting financial reporting deadlines. Ability to document processes in a way that others can understand and follow, including the ability to communicate key contextual information that gives meaning to processes. A strong workflow mindset; ability to think about and document processes in terms of hand-offs and escalation points between different roles."
SynergisticIT, Junior Data Analyst/Engineer/Scientist - Remote, "For data Science/Machine learning Positions Required Skills Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT Project work on the technologies needed Highly motivated, self-learner, and technically inquisitive Experience in programming language Java and understanding of the software development life cycle Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools Excellent written and verbal communication skills Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow"
J&M Group, Data Scientist / Machine Learning Specialist, "Key Responsibilities Independently work on end-to-end development of Machine Learning and Natural Language Processing models to derive insights from research publications, legal documents, regulatory requirements etc. Technical analysis and software development. Design and implement business solution in agile squads. Engage in Machine Learning project, which includes problem definition, Data Engineering, Machine Learning design and documentation for the model risk management and running all the needed Client tests to ensure reliability. Develop, maintain, and track detailed delivery plans. Strong Data analysis, processing, discovery skills. Skills Required Master's in mathematics, Statistics, Economics, Data Science, Machine Learning, Operational Research, Physics, and other related quantitative fields. At least 4 years of experience with design and implementation of machine learning, predictive analysis, data science, knowledge bases, recommendation systems, information retrieval. Strong understanding of the foundational concepts and applied experience in Machine Learning and model explain ability (ideally, a combination of excellent academic research and high-impact commercial projects). In depth understanding of common Machine Learning algorithms (e.g., for classification, regression, and clustering). Experience with LLM models and Open AI. In depth knowledge of advanced statistical theories, methodologies, and inference tools. Proven track record in some of the advanced topics such as Bayesian inference, hierarchical models, deep learning, Gaussian processes, and causal inference. Practical experience in preparing data for Machine Learning integrating with big-data platforms and high-performance computing ecosystems. Strong oral and written communication skills. Strong analytical and problem-solving skills. Skills Desired Modeling using vendor products such as: Microsoft AI Builder (Power Platform) and CO Pilot. Familiar with Azure and AWS framework Experience with non-English Natural Language Processing. Client Libraries H2O, Keras, Tensorflow Experience with Deep Learning"
Huawei Canada, Researcher - Embodied AI, "Responsibilities: Working closely with a team of experienced researchers to solve real world challenges in the domain of Embodied AI Developing state-of-the-art approaches for Embodied AI applications. Directions include, but not limited to, generative AI, representation learning, foundation models, reasoning, planning, data generation and augmentation, reinforcement learning, and low-level control. Translating mathematical problem definitions and model/solution specifications into efficient executable code Conducting evaluation and empirical studies using robotic platforms in both simulation and real-world Proposing high-impact intellectual properties (e.g., patents), and publishing or contributing to research papers in top-tier AI venues Job requirements What you'll bring to the team: M.Sc./ PhD degree in computer science or related fields (Exceptional candidates with Bachelor degree will be considered) Prior experience in sequence analysis and generative AI, in particular vision and language Proven research record in AI by having at least one paper as the first author in top tier venues, such as NeurIPS, ICML, ICLR, CVPR, ICCV, ECCV, ICRA Proficiency in Python programming language Experience with one or more mainstream deep learning frameworks e.g. PyTorch, TensorFlow Experience in robotics applications, in particular decision-making, planning and control, or experience with real robot and ROS is an asset Experience with using large-scale datasets and sensor data, or with use of various transformer architectures and diffusion models is an asset Experience with imitation and reinforcement learning algorithms is an asset"
OnDeck Fisheries AI, Machine Learning Engineer, "What you'll be doing This means on top of tweaking our standard vision methods and engineering ML and data pipelines, you'll get to work on cutting-edge open-domain vision methods and even contribute to original research and papers submitted to conferences. Your responsibilities will encompass: Developing state-of-the-art machine learning models that can reason about visual data and retrieve relevant answers from external knowledge sources. ML engineering of systems that ingest large volumes of visual data, and allow finding complex sequences of events or objects never seen in training data. Working on the entire ML lifecycle, from conducting research and developing innovative models to productionizing them and quantifying their real-world improvements. Contributing to automated model lifecycle management, which takes models to production, handling large volumes of video footage while undergoing updates smoothly. You will have significant ownership over mission-critical development, propelling us as a first-to-market at scale. You'll also get to work on cutting edge applied ML research and even publish your work at top conferences. In return, we're looking for commitment to and excitement for OnDeck's journey at the forefront of ocean tech, climate tech and open-domain computer vision. Minimum Qualifications 2+ years of full-time, non-internship work experience in applied research and ML engineering for production environments Demonstrated ability to implement novel machine learning literature Proficiency in programming and implementing machine learning workflows using Python (experience with C, C++, CUDA, and JavaScript/TypeScript is an asset) Proficiency with PyTorch, TensorFlow, and other modern machine learning frameworks/tools Experience in serving ML models (especially for computer vision), cloud/edge development and optimizing model performance Comfortable in Unix/Linux environments, distributed and parallel systems, and doing data engineering Strong technical communication skills in English, both written and verbal Enthusiasm for building software and doing applied research that revolutionizes automated visual reasoning Authorized to work in Canada (work permit or other). Preferred Qualifications Master's or Ph.D. in Computer Science, Statistics, Engineering, or a related field Experience in startups or high-impact roles in smaller organizations Knowledge of containerization and orchestration for large-scale deployment (Docker, Kubernetes) Proficiency with MLOps and cloud infrastructure (e.g. AWS EC2, EKS) Experience in setting up and using CI/CD tools (e.g., GitHub Actions, AWS CodePipeline) Up-to-date knowledge in computer vision research Contributions to research in deep learning and computer vision applications, such as peer-reviewed conference papers at NeurIPS, ICML, ICLR, CVPR, or journal papers at JMLR. Ability to develop accessible technologies"
Metaguest, Data Engineer, "As a Data Engineer at Metaguest, you will design and manage the data architecture that supports our AI-driven solutions for the hospitality industry. This is a hands-on role where you will create scalable data pipelines, optimize ETL processes, and ensure high data quality and reliability across the organization. You'll collaborate with cross-functional teams to enable data-driven decision-making and unlock new insights for our products. In this role, you will: Develop, test, and maintain efficient ETL processes to handle structured and unstructured data from various sources. Design and implement data storage solutions for performance, scalability, and reliability using cloud platforms like AWS, GCP, or Azure. Implement robust data validation, monitoring, and governance processes to maintain high levels of accuracy and consistency. Collaborate with Data Scientists, Machine Learning Engineers, and Product Teams to ensure data readiness for AI models and analytics tools. Work with APIs, relational databases, and big data technologies to consolidate data from Property Management Systems (PMS), CRM tools, and guest engagement platforms. Create systems that support real-time data processing for predictive analytics and personalized guest experiences. Enable business teams to access and analyze data through intuitive dashboards and visualizations. You'll thrive in this role if you: Have expertise in Python, SQL, and data pipeline tools like Apache Airflow or AWS Glue. Are experienced with big data technologies such as Spark, Hadoop, or Kafka. Have worked with cloud platforms like AWS (e.g., Redshift, S3), GCP, or Azure to implement scalable data systems. Understand the intricacies of ETL processes, database design, and data normalization. Are familiar with data lake architectures and implementing efficient data partitioning strategies. Have a strong grasp of API integrations, enabling seamless connectivity between systems. Understand the importance of data security and compliance, particularly for enterprise clients. Have a Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field (preferred)."
Validere, Data Consultant, "Let's Give You a Purpose Lead discovery with clients and internal stakeholders to uncover new business use cases and drive data strategy Work with clients to deliver end-to-end solutions leveraging Validere's software product and proprietary algorithms Develop and standardize best practices for data tooling and product configuration across disparate business use cases Configure and maintain automated data ETL pipelines to enable real-time insights What You'll Bring Along Growth mindset, passion for learning new things, restlessness First-principles thinking - you have the strong desire to understand any topic at a deeper level than most, and are unsatisfied with decisions based on convention or unjustified assumptions Ability to work in a fast-paced environment, formulate and test hypotheses quickly and efficiently Experience with Python and common data packages (NumPy, Pandas) Proficient communication and presentation skills Ability to work with disparate data sources (i.e SQL, CSV, APIs) and leverage a Python codebase Familiarity with leveraging standard API protocols and practices to query, manipulate and ingest data Bachelor's degree in engineering, business or another quantitative field (e.g. physics, math, statistics, etc.) Preferred: 1-2 years experience in a similar role (data analyst, data engineer, etc.) with a proven track record of taking ownership and driving projects to completion"
TD, Data Scientist I, "Job Functions Document Business Intelligence requirements for our Business partners; Document business rules and validate data mappings; Perform impact analysis Liaise with different teams within Platform and Technology or with other internal partners to ensure data availability and accuracy Collaborate with all members of the Agile team to achieve successful implementations Flexibility to support other team members Communicate business needs to the teams responsible to define the business rules and development, so that they can design and propose solutions. Support related teams in the investigation and design of business solutions; Collaborate in the selection of the best solutions; Present and validate with the business partners the alternative(s) of proposed solution(s). Manage stakeholder expectations Must be able to understand different operational systems: systems functioning of related frontend systems or other backend databases what they contain (understanding available data, investigating data, etc ...). Must show strong adaptability to discuss with different stakeholders in terms of expertise, training and hierarchy. Skills At least 2 years of relevant experience in Business Intelligence projects/Reporting. Good knowledge in data extraction and manipulation (Databricks, SQL or equivalent) Good communication: Bilingual (English and French). Good time management skills to support multiple initiatives of different complexity Demonstrate analytical skills and critical thinking Be quality conscious Can negotiate with business partners Work efficiently in a team setting. Assets Knowledge of the Insurance Industry Jira/Confluence, Agile methodology. Knowledge of Python, Tableau"
Advanced Tower Structural Solutions (ATSS), Specialist Data Analytics, "Key Responsibilities Analyze and manage large datasets to extract meaningful insights Develop and sustain reports that effectively convey data findings Create data visualizations that facilitate client decision-making Collaborate with clients to ascertain their data requirements and goals Ensure the accuracy and integrity of data throughout the analytical process Required Skills Strong analytical and problem-solving abilities Proficiency in data visualization tools such as Tableau or Power BI Experience with data manipulation and querying languages, including SQL Familiarity with statistical analysis methodologies Excellent communication skills for clear presentation of findings"
Gore Mutual Insurance, Data Engineer, "What will you be doing in this role? Developing Systems for Collecting and Storing Data Build data pipelines and technical components Automating Data Management Processes and Handling Data Security Automated data management processes. Follow security protocols and best practices to protect against potential security threats. Testing and Optimizing Data Pipelines Optimize data pipelines to ensure efficient data flow. Ensure that the data extracted from sources is accurate, complete, and usable. This might involve checking for missing values, inconsistent formats, or anomalies that could indicate errors. Test the efficiency and speed of data pipelines and databases. This can help identify bottlenecks and optimize performance. Verify that different components of the data infrastructure work together as expected. This includes checking that data flows correctly from sources to databases, and from databases to applications. Maintenance of Data Pipelines Regularly check the health and performance of the data processes. Identify and resolve issues including debugging code, optimizing queries, or adjusting configurations. What will you need to succeed in this role? Bachelor's or Master's degree in Computer Science, Data Engineering, Software Engineering or a related field. A minimum of 1-3 years of relevant experience as a data engineer is required. This includes experience in data engineering, data system development, or related roles. Good understanding of data structures, data modeling, and SQL. Exposure to Cloud based Services, preferably Microsoft Azure. Understanding with software design patterns and test-driven development (TDD) Proficiency in one programming language, preferably Python, including a strong grasp of Object Oriented and Functional programming paradigms. Exposure to Apache Spark concepts and distributed systems, including data transformations, RDDs, DataFrames, and Spark SQL. Strong problem-solving and critical-thinking abilities. Strong communication and collaboration skills."
Oxford Properties Group, Data Engineer, "Key Responsibilities ETL Processes: Develop robust ETL processes to extract, transform, and load data from various sources into our systems, ensuring data accuracy and integrity. SQL Database Management: Maintain and optimize a SQL database containing a cost model, ensuring data availability, security, and performance. Code Reviews: Conduct thorough code reviews to ensure coding standards, best practices, and quality are upheld across the development team. Development Process Improvement: Collaborate with the data scientist and development team to establish and enhance development processes, including version control, unit testing, and continuous integration/delivery. Release Management: Oversee the release management process, coordinating with cross-functional teams to ensure smooth and timely deployment of software solutions. Data Science Support: Collaborate with the data scientist to implement development best practices, set up unit testing frameworks, and provide technical support for data science initiatives. Dashboard Development: Create visually appealing and insightful dashboards to enable data-driven decision-making within the organization. Power Apps Development: Design, develop, and maintain Power Apps solutions that streamline business processes and enhance user experiences. Technical Documentation: Document technical designs, procedures, and guidelines to facilitate knowledge sharing and onboarding. Qualifications And Skills Bachelor's degree in Computer Science, Engineering, or related field. 1 - 3 years of experience in software development, with a focus on ETL processes, SQL database management, and Power Apps. Strong SQL skills and experience in database design, optimization, and maintenance. Solid understanding of ETL principles and practices. Experience with version control systems (e.g. Git) and CI/CD pipelines. Familiarity with software development methodologies and best practices. Proficiency in Microsoft Power Apps development. Excellent problem-solving and analytical skills. Strong communication and collaboration abilities. Attention to detail and a commitment to producing high-quality work. Experience in dashboard development and data visualization tools is a plus. Previous exposure to supporting data science initiatives is an advantage."
W3Global, Data Engineer, "We are looking for a skilled Data Engineer and ETL Developer to design and manage data pipelines, ETL processes, and cloud-based data solutions. You will work with cutting-edge tools and platforms to ensure efficient data integration and processing Key Responsibilities: Develop and maintain ETL workflows using Ascend.io / Informatica / IICS. Automate and strong python knowledge to build the pipeline with Airflow Composer. Build scalable data pipelines on Google Cloud Platform (GCP). Optimize data storage and querying in BigQuery / DBMS. Write and optimize SQL queries for data extraction and analysis. Use Python to automate and streamline data processes. Collaborate with teams to meet data requirements and troubleshoot issues. Skills & Qualifications: Proficiency with Ascend.io, Informatica, Airflow Composer, and GCP services. Strong SQL and Python programming skills. Experience in data pipeline design and cloud data management, particularly in BigQuery. Solid problem-solving skills and ability to work collaboratively."
ServiceNow, Machine Learning Developer, "Build the best cloud-based AI/ML solutions to power intelligent enterprise services Collaborate daily with a team of like-minded developers, product managers and quality engineers to produce quality software Work with product owners to understand detailed requirements and own your code from design, implementation, testing and delivery of high-quality solutions to our users Qualifications To be successful in this role you have: Expertise in Java or Python, OOP, Design Patterns, time and space-efficient algorithms Experience building new products that use challenging algorithms Expertise in coding efficient, object-oriented, modularized and quality software Knowledge of core AI/ML techniques and algorithms Knowledge of unit testing, profiling, and code tuning"
Neighbourly Pharmacy, Data Engineer, "Responsibilities Design, build, and maintain a scalable, secure data infrastructure that supports cross-functional data needs across departments, ensuring efficient data pipelines and reliable access to clean data for analytics and decision-making purposes. Implement and uphold data governance and quality standards across the organization, creating automated processes for data extraction, transformation, and loading (ETL) to reduce manual workload, support compliance, and enhance data-driven insights Analyze existing data processes and workflows to identify areas for improvement. Collaborate with Finance and IT professionals to design and implement process enhancements. Develop and maintain documentation for data processes, including data flow diagrams and process descriptions. Monitor data processes to ensure compliance with data governance and quality standards. Assist in the development and execution of data quality checks and controls. Provide training and support to end-users on new data processes and tools. Participate in cross-functional projects to support data-driven decision-making across the organization. Stay informed on the latest industry trends and technologies in data process management. Qualifications & Skills Bachelor's degree in Information Technology, Data Science, Computer Science, or a related field. Proven experience in data analysis, process improvement, or a related role. Strong understanding of data management principles and best practices. Proficiency in data analysis tools and software, such as SQL, Excel, Python and process mapping tools. Excellent analytical and problem-solving skills. Strong communication and collaboration abilities. Detail-oriented with a focus on accuracy and quality."
NTT Data Inc., Junior Data Scientist and ML Engineer, "Key Responsibilities: Proactively supports the design, development, and programing methods, processes, and systems to consolidate and analyze unstructured, diverse big data sources to generate actionable insights and solutions for client services and product enhancement. Receives instructions to research, design, implement and deploy scalable data analytics vision and machine learning solutions to challenge business issues. Contributes to the design and enhancement of data collection procedures to include information that is relevant for building analytic systems. Ensures that data used for analysis is processed, cleaned and, integrally verified and build algorithms necessary to find meaningful answers. Contributes to the designing and coding of software programs, algorithms, and automated processes to cleanse, integrates and evaluates large datasets from multiple disparate sources. Takes direction from management/leadership to provide meaningful insights from large data and metadata sources; interprets and communicates insights and findings from analysis and experiments to product, service, and business managers. Supports the design of scalable and highly available applications leveraging the latest tools and technologies. Participates in creatively visualizing and effectively communicating results of data analysis, insights, and ideas in a variety of formats to key decision-makers within the business. Receives instructions to create SQL queries for the analysis of data and visualize the output of the models. Contributes to ensuring that industry standards best practices are applied to development activities. Knowledge and Attributes: Developing in data modelling, statistical methods and machine learning techniques. Ability to thrive in a dynamic, fast-paced environment. Quantitative and qualitative analysis skills. Desire to acquire more knowledge to keep up to speed with the ever-evolving field of data science. Curiosity to sift through data to find answers and more insights. Developing understanding of the information technology industry within a matrixed organization and the typical business problems such organizations face. Ability to translate technical findings clearly and fluently to non-technical team business stakeholders to enable informed decision-making. Developing ability to create a storyline around the data to make it easy to interpret and understand. Self-driven and able to work independently yet acts as a team player. Developing ability to apply data science principles through a business lens. Academic Qualifications and Certifications: Bachelor's degree or equivalent in Data Science, Business Analytics, Mathematics, Economics, Engineering, Computer Science or a related field. Relevant programming certification preferred. Agile certification preferred. Required Experience: Moderate level experience in a data science position in a corporate environment and/or related industry. Moderate level experience in statistical modelling and data modelling, machine learning, data mining, unstructured data analytics, natural language processing. Familiarity with programming languages (R, Python, etc.). Moderate level experience working with and creating data architectures. Moderate level experience with extracting, cleaning, and transforming data and working with data owners to understand the data. Familiarity with visualizing and/or presenting data for stakeholder use and reuse across the business."
Electric Mind, Data Engineer, "While working with teams in an Agile environment, you will be: Designing high quality data pipelines/architectures that are highly scalable and extensible Providing guidance on data modeling, analysis, visualization, and implementation of data solutions Estimating, tasking, and prototyping Collaborating with cross-functional team members on software features, design and implementation Participating in the end-to-end delivery of data consulting projects, ensuring completion on time, on budget, and meeting or exceeding client expectations Investigating, learning and applying new technologies and ?processes Clarifying requirements with team and client representatives 'Must Have' Skills/Experience Snowflake, Databricks, or Apache Spark experience Design and implementation experience with Azure Cloud Data Services and/or AWS Cloud Data Services Design and implementation experience with Master Data Management, especially customer and reference data for financial services Proficiency in SQL In-depth knowledge of data management, analytics, business intelligence, and related technologies Growth mindset and desire to learn Bachelor's Degree in: Data Science, Business Analytics or Computer Science (or equivalent education/experience) 'Nice To Have' Skills/Experience PowerBI or Tableau or data visualization experience Talend, Apache Airflow, AWS Glue, and Azure Data Factor experience Proficiency in Python or R Experience in data modeling or schema design Experience with Kafka or other data streaming tools Financial Services industry experience Sales, pre-sales, or consulting experience Exposure to and enthusiasm for Agile approaches Master's Degree in: Data Science, Business Analytics or Computer Science (or equivalent education/experience)"
J&M Group, Data Engineer, "Design, develop, and maintain data processing pipelines using Apache Spark. Collaborate with data engineers, data scientists, and business analysts to understand data requirements and deliver solutions that meet business needs. Write efficient Spark code to process, transform, and analyze large datasets. Optimize Spark jobs for performance, scalability, and resource utilization. Integrate Hadoop, Hive, Spring, Hibernate, Kafka, and ETL processes into Spark applications. Troubleshoot and resolve issues related to data pipelines and Spark applications. Monitor and manage Spark clusters to ensure high availability and reliability. Implement data quality and validation processes to ensure accuracy and consistency of data. Stay up-to-date with industry trends and best practices related to Spark, big data technologies, Python, and AWS services. Document technical designs, processes, and procedures related to Spark development. Provide technical guidance and mentorship to junior developers on Spark-related projects."
AXIS (AXIS Capital), Junior AI Engineer, "Collaborating with the Lead Data Scientist, data scientists, Innovation & Analytics Leads to support the design, development, and deployment of AI-driven models and solutions. Leading the deployment and infrastructure activities relating to AI-driven models and solutions. Assisting in gathering, processing, and analyzing large datasets to extract insights for machine learning and AI applications. Developing and optimizing machine learning models and Generative AI systems for real-world business applications. Deploying AI models and solutions to production environments, ensuring seamless integration with existing systems and monitoring performance. Building and maintaining scalable AI solutions, ensuring efficient model deployment and integration into business workflows. Supporting the end-to-end lifecycle of AI solutions, from data engineering to model training, evaluation, and deployment in production environments. Contributing to improving AI model performance and scalability through rigorous testing, monitoring, and optimization. Collaborating with stakeholders to ensure AI solutions align with business objectives and comply with regulatory requirements. Staying current with the latest trends and advancements in AI, ML, and GenAI technologies."
Bitstrapped, Machine Learning Engineer, "Responsibilities Develop, train, and deploy machine learning models on Google Cloud Platform (GCP), leveraging tools such as VertexAI, GenApp Builder, PalmApi and BigQueryML. Collaborate with cross-functional teams to gather requirements, define project goals, and design scalable machine learning architectures. Conduct data preprocessing, feature engineering, and model evaluation to ensure high-quality and reliable models. Implement and optimize machine learning algorithms and pipelines to handle large-scale text-based datasets. Explore and experiment with generative AI techniques, including large language models, to solve complex text-related problems. Monitor and maintain deployed models, ensuring performance, scalability, and reliability in production environments. Stay up-to-date with the latest advancements in machine learning, generative AI, and text-based models, and proactively propose innovative solutions to enhance existing systems. Requirements Bachelor's or advanced degree in Computer Science, Engineering, or a related field. Strong experience in machine learning engineering, with a focus on developing and deploying models in production environments. Proficiency in using Google Cloud Platform (GCP) tools and services for machine learning, such as Vertex AI, BigQuery, and TensorFlow. Solid understanding of deep learning architectures, natural language processing (NLP), and generative AI models, particularly in the text domain. Proficiency in Python and experience with relevant libraries and frameworks, such as TensorFlow, PyTorch, or Keras. Experience with data preprocessing, feature engineering, and model evaluation techniques for text-based datasets. Familiarity with MLOps practices, version control, and CI/CD pipelines for machine learning. Strong problem-solving skills and the ability to work on complex projects independently or collaboratively. Excellent communication skills, with the ability to convey complex technical concepts to both technical and non-technical stakeholders. Preferred Qualifications Experience with large language models, such as GPT-3, BERT, or Transformer-based models. Familiarity with cloud-based machine learning services and technologies, beyond Google Cloud Platform. Knowledge of scalable distributed computing frameworks, such as Apache Spark or Hadoop. Contributions to open-source machine learning projects or research publications in the field."
ATS Software, Machine Learning Engineer, "Development of Advanced ML Models: Take part in the development and training of advanced machine learning models, with a focus on large language models (LLMs) and optical character recognition (OCR) across various data modalities. Utilize Python and Pandas to make significant contributions to our interdisciplinary projects.   Optimization and MLOps Application: Assist in refining our codebase and adopting MLOps practices to enhance the machine learning lifecycle across modalities, from data preparation to deployment and monitoring. Develop and implement strategies to manage the machine learning lifecycle for seamless integration of AI and ML models within the data warehouse.   Interdisciplinary Model Integration: Collaborate with data engineers and data scientists to ensure data quality and model accuracy. text and vision, into our application ecosystem. Focus on enhancing performance and scalability under the guidance of senior leadership. Required Qualifications:   Deep Learning Expertise Across Modalities: Strong knowledge and experience with deep learning algorithms such as RCNN, LSTM, and Transformers, particularly applied to various modalities like text and vision.   Experience with Interdisciplinary CV and NLP Models: Demonstrated experience working with machine learning models in computer vision and NLP, showcasing the ability to tackle complex, interdisciplinary problems through innovative ML solutions.   MLOps Platforms Proficiency: Familiarity with MLOps platforms such as AWS SageMaker, Databricks, and Weights & Biases, with a competency in leveraging these tools to streamline the ML lifecycle across various data modalities.   Coding Proficiency and Best Practices: Experience in writing clean, efficient code following Object-Oriented Programming (OOP) principles and coding best practices, with a proven ability to navigate and contribute to complex codebases.   Proficiency in Python, SQL, and relevant data warehouse technologies (e.g., Snowflake, Redshift, BigQuery)."
Advanced Tower Structural Solutions (ATSS), Data Specialist, "Your expertise will be utilized in managing extensive datasets, uncovering trends, generating reports, and crafting compelling data visualizations. Key Responsibilities Analyze expansive datasets to extract meaningful trends Generate detailed reports tailored for client needs Create visually engaging data representations to clarify findings Collaborate effectively with team members to refine data-centric strategies Required Skills Proficient in data analysis tools and software, including Excel, SQL, and Python Strong analytical abilities and attention to detail Skilled in developing data visualizations and comprehensive reports Excellent problem-solving capabilities Strong written and verbal communication skills"
Advanced Tower Structural Solutions (ATSS), Data Specialist, "Your expertise will be utilized in managing extensive datasets, uncovering trends, generating reports, and crafting compelling data visualizations. Key Responsibilities Analyze expansive datasets to extract meaningful trends Generate detailed reports tailored for client needs Create visually engaging data representations to clarify findings Collaborate effectively with team members to refine data-centric strategies Required Skills Proficient in data analysis tools and software, including Excel, SQL, and Python Strong analytical abilities and attention to detail Skilled in developing data visualizations and comprehensive reports Excellent problem-solving capabilities Strong written and verbal communication skills"
Animo Tech, Data Analyst, "Responsibilities: Analyze Market Data: Evaluate market trends and conditions, interpreting large datasets to extract actionable insights. Analyze Sales Data: Perform in-depth analysis of sales patterns to identify opportunities for growth, while managing downside risks. Market Strategy Recommendations: Based on data analysis, propose strategies to optimize market positioning and enhance business performance. Hypothesis Testing: Conduct hypothesis tests using natural experiments to validate findings and improve decision-making. Statistical Modeling: Develop and apply statistical models to predict future sales trends and forecast outcomes. Active participation: Engage with Subject Matter Experts (SMEs) and stakeholders through interviews and field research to refine models, ensuring they align with business needs and identifying factors outside statistical scopes. Qualifications: A bachelor's degree in data analytics, computer science, statistics, information systems, or a STEM discipline with demonstrated knowledge in statistics. A high level of Python proficiency: Strong background in Python programming, particularly for data analysis and modeling. Strong experience in commonly used data libraries like Pandas, Scikit-learn (sklearn), and NumPy, etc."
CGI, Salesforce Data Analyst, "Attributes that define our ideal candidates will include: Hands on experience with Salesforce core product schemas, Salesforce Reports and Dashboards, CRM Analytics, Tableau, Salesforce Einstein. Hands-on experience writing complex SQL queries and working with relational databases such as Oracle, DB2, or Microsoft, SQL Server Hands-on experience building reports outside of Salesforce platform with PowerBI, Tableau, or other reporting and analytical tools Analytical curiosity, strong data-oriented skills, statistical programming or computer science background preferred Strong data management and data cleansing hands on experience, including managing large data volumes, performing ETL activities, and monitoring data quality Experience in a mature data governance environment, with practical exposure to data governance tools and processes at enterprise scale Experience working in highly secure environments, handling PII and PHI Knowledge of software development techniques and methodologies, with background in Agile environments Exceptional communication skills, with an ability to make advanced analytics concepts accessible and understandable to non-technical business users Highly detailed-oriented with exceptional organizational and follow-through skills, self-starter and able to execute without a lot of direction or oversight Ability to handle multiple priorities and deadlines Bachelor's degree in mathematics, business, statistics, economics, computer science or information systems (or equivalent combination of skill and experience) Your future duties and responsibilities: Solve some of the most challenging and impactful problems for healthcare, public sector, financial services, insurance and utilities by using artificial intelligence, machine learning, and natural language processing and big data Help create the platform, tools and APIs necessary to enable other teams to work with data Efficiently handle vast amounts of data from multiple sources and destinations, including relational and NoSQL databases as well as external systems, both in batch processing and real-time delivery Drive innovation into Analytics solutions and focus on humanizing enterprise software to achieve better customer experience and to enable data-driven business decisions Work on high priority initiatives using advanced analytics, predictive modeling and a variety of data sources to produce actionable business insights Provide guidance to a customer and project team with respect to technical feasibility, complexity, and level of effort required to deliver a solution Work closely with other team members to further develop metrics, KPIs, and insights that measure business performance improvements Assist in the development and delivery of pre and post sales POCs, presentations and proposals for client engagements"
Canadian Cancer Society, Data and Analytics (BI) Specialist, "What You'll Be Doing Dive into Data: Extract, clean, and transform large datasets from various sources, turning raw data into valuable insight. Uncover Trends: Analyze data to identify trends, patterns, and correlations, providing actionable insights that drive strategic decisions. Create Visual Masterpieces: Develop and maintain dynamic reports, dashboards, and visualizations using BI tools (e.g., Tableau, Power BI, Salesforce Reports). Ensure Data Excellence: Maintain data accuracy and integrity, ensuring that our decisions are always data-driven and reliable. Collaborate and Innovate: Work closely with stakeholders to develop and maintain data models, ETL processes, and data pipelines, fostering a culture of innovation. Predict the Future: Utilize predictive analytics and statistical models to support business planning and identify growth opportunities. Empower the Team: Train and support end users on BI tools, fostering a data-driven culture across the organization. QUALIFICATIONS: University or College degree in information technology or an equivalent combination of education and experience. Strong analytical skills with the ability to interpret complex data sets. Proficiency in BI tools such as Tableau, Power BI, and Salesforce reports. Strong organization skills and ability to manage multiple tasks. Experience with data modeling, ETL processes, and data pipeline development. Excellent communication and collaboration skills Creative and analytical thinker with strong problem-solving skills."
Bombardier, Data Engineering Analyst, "What are your contributions to the team? Develop or enhance current framework to ingest the data from the source and load into the Lakehouse Support development and maintenance activities on enterprise data platform (Data Warehouse, Lakehouse). Support the creation and maintenance of backend data solutions to be used by data analysts and data scientists across the enterprise. Support the creation and maintenance of data solutions (report, dashboard) for various areas of the business. Experience with troubleshooting and optimization of the current process Optimize data pipelines to meet business needs. Develop the code using Software Development Life Cycle (SDLC) best practices using DevOps Continuous Integration and Continuous Deployment (CICD) of data systems. Identify data quality issues and make recommendations for addressing root causes. How to thrive in this role? You hold a bachelor's degree in computer science, Statistics, Informatics, Information Systems or another quantitative field. You have some experience (internship or other) in a similar role. You have developed Power BI solutions which make use of DAX. You have experience working with API for developing simple applications. You have knowledge of Agile / SCRUM project delivery, DevOps and CICD practices. You can create analytics solutions using SQL on relational databases. You have knowledge of modern data services on Microsoft (Power BI, Azure Data Factory, Synapse Analytics, Azure Data Lake Storage and Databricks) You have good knowledge of Object-oriented/object function scripting languages: Python, Java, C++, Scala, etc. You have good people skills and are a team player."
Pattison Food Group, Data Analyst, "You will be responsible for: Collaborating with stakeholders to understand business questions, processes, and KPIs. Formulating testable hypotheses aligned with business questions and conducting statistical experiments in a live environment. Writing SQL scripts to automate data pipelines and workflows. Using Python scripts in a Jupyter Notebook environment for exploratory data analysis, data wrangling, and statistical testing. Developing and maintaining dashboards and visualizations in Power BI to convey insights and report on KPIs and test results. Learning cloud-based analytics environments and big data tools to build analytics workflows on large datasets. Discovering unique and creative insights to identify new opportunities for driving business value. Becoming an expert in the relevant datasets within your analytics domain. Supporting the Data & Analytics with key projects. You have: A degree in a STEM discipline with 2 to 3 years of relevant experience in data analytics or data science. Intermediate Python coding skills. Experience in other programming languages is a considered an asset. Intermediate understanding of SQL. Experience using experimental design principles such as A/B and multivariate testing and working with business stakeholders to formulate testable hypotheses. Experience with visualization and reporting tools such as Power BI or Tableau. Working knowledge of cloud environments such as Azure, GCP, or AWS. Experience with big data tools such as Spark and Databricks is a considered an asset. You are: A confident communicator, both verbally and written, with strong interpersonal skills and the ability to communicate a story with data and insights to non-technical stakeholders. A problem solver with strong analytical and critical thinking skills. Effective in task management and delivering high-quality work. Capable of multitasking and thriving in a fast-paced environment. Self-motivated with the ability to work independently and collaboratively."
Mueller Water Products, Data Analyst, "Key Responsibilities Manage key client sites using the EchoShore product line: an industry leading Internet of Things (IoT) leak detection system. Perform acoustic analysis and large-scale data processing for leak detection and pipe condition assessment and provide reports to water utilities (customers) to help them manage their water distribution networks. Communicate the status of data analysis to Project Managers. Identify data anomalies and significant variances and flag them to senior staff. Assisting with the development of automatic software tools to improve productivity and efficiency of data analysis process. You will work closely with various departments (IT, software, sales) to implement these tools, understand their data needs and provide data-driven insights. Analyze data and work with Senior Data Analysts and Software Engineers to adjust analysis software for best outcomes including defining acceptance criteria Develop processes and procedures for accurate and efficient data analysis Monitor process performance through KPIs and strive for continuous improvement and further development of the leak detection center Prepare and update Standard Operating Procedure documents as required Build and maintain relationships with external leak detection customers Prepare and distribute reports to external customers regarding their leak assessment results Work in compliance with the MWP Code of conduct and all Environmental, Health and Safety policies, procedures and regulations Position Requirements Bachelor's degree in quantitative fields such as Engineering, Statistics, Mathematics, Computer Science, or a related field. Previous experience in data analytics Strong analytical skills, including data mining and statistical analysis Proficiency in data analysis tools and software (e.g., SQL, Excel, R, Python). The ability to perform large scale data frame processing using Pandas and NumPy is a plus. Database skills: good command of SQL queries. Understand database technologies: relational database (RDBMS), NoSQL, document-oriented, etc. Good command of numerical methods and associated tools: Scipy, Matlab, Octave, or other. Personal skills: creative, perseverant, detail-oriented, self-starter. Well developed communication skills, both verbal and written, with the ability to present analysis results and data insights to technical and non-technical stakeholders. Online asynchronous communication is required. Critical Competencies: technical learning, time management, problem solving. Position Assets Experience with acoustic, seismic, and vibration analysis is a plus. Knowledge of signal processing techniques and time-series analysis to identify anomalies, trends and other data patterns. Experience with GIS processing software such as ArcGIS, Geocortex, DMTI Spatial Design and implement instrumentation for data collection and analysis. Conduct experiments to test hypotheses and validate data models. Analyze experimental results and provide actionable insights. Working experience in an Agile environment. Familiar with issue and bug tracking using JIRA Familiar with Data Visualization tools such as Matplotlib, Plotly, etc"
BC Housing, Data and Process Analyst, "EDUCATION & EXPERIENCE: Bachelor's degree in data management, Information Systems, Business Administration or similar from a recognized post secondary institution. Sound experience in Data Analysis, Data quality Assurance, and Business Process Analysis. Some experience in Data Governance, policies, and procedures. Or an equivalent combination of education, training, and experience acceptable to the employer. KNOWLEDGE, SKILLS AND ABILITIES: Good knowledge of database systems and data warehousing concepts, Good knowledge of data quality improvement process, data dictionaries and repositories. Good knowledge of organizational system, ERP, or process management system Some knowledge of data governance principles including data governance framework, data analytics, data design and practices. Some knowledge of technical and user manual documentation. Familiarity with data visualization tools (e.g., Power BI). Strong communication, report writing, presentation and interpersonal skills. Strong research, analytical and problem-solving skills. Ability to assist with data investigations and recommend updates. Attention to detail and ability to collaborate and work independently. Ability to adapt to evolving technologies and methodologies. Ability to facilitate training on data governance, policies, and procedures."
Electronic Arts (EA), Data Analyst - Quality Verification, "You will define the data analytics roadmap for some of EA's largest titles. You will use your technical skills to develop data products that improve decision-making across the Quality Verification organization. You will guide best practices in tools and visual technologies to help evolve the project dashboards to provide essential data for project monitoring. You will develop scripts and queries to import and manipulate data from multiple sources. You will help us promote a data-driven culture and support data governance practices to improve our organization's data maturity. The Next Great Data Analyst Needs 2+ years of professional analytical experience, using data to improve processes. Programming and Database experience with advanced SQL experience and efficient in Python. Knowledge of different data software analytics and visualization tools such as Looker and PowerBI. Able to develop data modelling and warehousing solutions. Able to plan and prioritize multiple concurrent projects. Able to foster working relationships with partners."
Services SFT, Data Analyst, "The role we are offering you: Perform analysis on large data set, extract insights and communicate various stakeholders with the objective of improving the overall business performance. Interact with customers around the world Develop and maintain data management tools to help with every-day decision planning Create tools and build dashboards to facilitate reporting with relevant KPIs for different stakeholders Collaborate with Digital Product Managers to explore new product ideas or enhancements Possibility to work on simulator related software Requirements We are looking for someone who: Bachelor's degree in computer science, Management Information Systems, or equivalent experience Proficiency creating SQL queries, Use of PowerBI or other data visualization tools Comfort cleaning and analyzing large data sets with appropriate data technologies (e.g., Python, Databricks, SAS, R) Experience in data warehouse design and implementation with an emphasis on data preparation Strong experience in analytical business decision-making, data analytics, and statistics Strong analytical skills with the ability to discover patterns in data and figure out puzzling data relationships"
Exadel, Data Scientist / Analyst, "Qualifications Bachelor's degree in Computer Science, Software Engineering or a relevant work experience. Strong SQL/data skills. Programming skills in Python or equivalent to implement ML Experience with financial and capital markets data. Solid understanding of statistical and machine learning techniques. Excellent problem solving and analytical skills. Strong communication and presentation skills. Ability to work independently and collaboratively in a fast-paced environment. Nice To Have Experience in big data technologies (e.g. Spark, AWS Glue, Redshift, etc.) Familiar with AWS data services (e.g. S3, RDS, Redshift, Glue, etc.) Responsibilities Data Acquisition and Cleansing - source and catalog data from multiple sources, cleanse, preprocess and validate data to ensure accuracy and consistency. Data Analysis and Modelling - Employ statistical and ML techniques to extract meaningful insights from complex datasets. Model Implementation and Validation - Collaborate with tech teams to deploy data feeds, models into production, ensuring scalability and reliability. Data Catalog - Maintain an Overall data catalog for the team."
Snaplii, Data Analyst - Informatics and Systems, "You Will: Develop and implement databases, data collection systems, data analytics, and other strategies that optimize statistical efficiency and quality. Interpret data, analyze results using statistical techniques, and provide ongoing reports. Identify, analyze, and interpret trends or patterns in complex data sets. Work closely with management to prioritize business and information needs. Locate and define new process improvement opportunities. Design and create data reports and reporting tools to help business executives in their decision-making process. Collaborate with engineering and product development teams to improve product and data systems. Ensure adherence to data privacy laws and policies. Required Qualifications and Skills: Bachelor's degree in Informatics, Computer Science, Statistics, or a related field. Proven working experience as a Data Analyst or Business Data Analyst. Technical expertise regarding data models, database design development, data mining, and segmentation techniques. Strong knowledge of and experience with reporting packages (Business Objects etc.), databases (SQL etc.), programming (XML, Javascript, or ETL frameworks). Knowledge of statistics and experience using statistical packages for analyzing datasets (Excel, SPSS, SAS, etc.). Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy. Adept at queries, report writing, and presenting findings. Excellent verbal and written communication skills. Preferred Skills: Experience with data visualization tools (Tableau, Power BI). Knowledge in machine learning and predictive modeling is a plus."
Orennia, Data Engineer, "What You'll Do Analyze, design, develop, test, review, document and troubleshoot data pipeline / ELT solutions against multiple structured and unstructured data sources. Support our team of analysts through developing requirements and delivering solutions. Develop code to scrape public websites for data and perform ELT processes. Maintain, monitor, and support production ELT processes and respond to error and emergency issues. Who You Are You have excellent knowledge and experience with Big Data concepts like data lakes, data warehouses, ELT strategies, and best practices. You have a strong understanding of relational and dimensional data modeling. You possess strong analytical and problem-solving skills. You have experience with DBT and SQL, and are proficent with Python. You have extensive experience with cloud-based data processing and warehousing technologies (Databricks, Snowflake, etc) You have experience with Lean and Agile development methodologies (such as Kanban or SCRUM). You are comfortable in entrepreneurial, self-starting, and fast-paced environment, working both independently and with our highly skilled teams. You have experience with other Big Data processing technologies and cloud services (AWS, GCP, Snowflake, Hive, Hadoop, MS SQL, etc.). You have experience with JIRA and similar organizational tools. You have experience building web-scraping tools against publicly available datasets (considered an asset). You have experience with GIS/geospatial data processing, integration, and analysis is (considered an asset). You have experience building or supporting data visualizations (considered an asset). Deep intellectual curiosity with a results-focused relentless pursuit of answers. Ability to work in a fast-paced start-up environment, embrace change and ambiguity."
