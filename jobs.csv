Staffing Industry Analysts,Junior Data Analyst - Entry Level,"Data Collection and Cleaning: Assist in the collection and organization of data from various sources.Participate in data cleaning and preprocessing activities to ensure data accuracy. Data Analysis: Utilize statistical methods and tools to assist in the analysis of datasets. Work with GSP team members to identify trends, patterns, and insights in the data. Data Visualization: Support the creation of visualizations and reports for communicating data findings. Collaboration: Collaborate with team members to understand data requirements and provide support in delivering analytical solutions. Learn from experienced team members and actively seek guidance on projects.Report Generation:Learn to summarize and communicatedatainsights in a clear and understandable manner.Continuous Learning: Actively participate in training and development opportunities to enhance skills. Job Qualifications Bachelor's degree in a relevant field (e.g.,Statistics, Mathematics, Computer Science) or equivalent experience. Basic understanding of data analysis concepts and methodologies. Familiarity with data analysis tools such as Power BI and Excel. Strong attention to detail and analytical skills. Good communication skills and the ability to work collaboratively in a team. Eagerness to learn and adapt to new technologies and techniques."
New iTalent Digital,Data Scientist Intern / Remote- Part Time,"Your Role and Responsibilities: Perform accurate and efficient data entry in various systems and databases. Proofread and edit data for correctness, performing updates as necessary. Maintain strict confidentiality of sensitive information. Ensure data backups are correctly performed.Support IT team with other administrative tasks as needed."
New iTalent Digital,Junior Data Analyst - Entry Level,"Data Collection and Cleaning: Assist in the collection and organization of data from various sources.Participate in data cleaning and preprocessing activities to ensure data accuracy. Data Analysis: Utilize statistical methods and tools to assist in the analysis of datasets. Work with GSP team members to identify trends, patterns, and insights in the data. Data Visualization: Support the creation of visualizations and reports for communicating data findings. Collaboration: Collaborate with team members to understand data requirements and provide support in delivering analytical solutions. Learn from experienced team members and actively seek guidance on projects. Report Generation: Learn to summarize and communicatedatainsights in a clear and understandable manner.Continuous Learning: Actively participate in training and development opportunities to enhance skills. Job Qualifications Bachelor's degree in a relevant field (e.g.,Statistics, Mathematics, Computer Science) or equivalent experience. Basic understanding of data analysis concepts and methodologies. Familiarity with data analysis tools such as Power BI and Excel. Strong attention to detail and analytical skills. Good communication skills and the ability to work collaboratively in a team. Eagerness to learn and adapt to new technologies and techniques."
Staffing Industry Analysts,Junior Data Analyst - Entry Level(100% Remote),"Data Collection and Cleaning: Assist in the collection and organization of data from various sources.Participate in data cleaning and preprocessing activities to ensure data accuracy. Data Analysis: Utilize statistical methods and tools to assist in the analysis of datasets. Work with GSP team members to identify trends, patterns, and insights in the data. Data Visualization: Support the creation of visualizations and reports for communicating data findings. Collaboration: Collaborate with team members to understand data requirements and provide support in delivering analytical solutions. Learn from experienced team members and actively seek guidance on projects.  Report Generation: Learn to summarize and communicatedatainsights in a clear and understandable manner.Continuous Learning: Actively participate in training and development opportunities to enhance skills. Job Qualifications Bachelor's degree in a relevant field (e.g.,Statistics, Mathematics, Computer Science) or equivalent experience. Basic understanding of data analysis concepts and methodologies. Familiarity with data analysis tools such as Power BI and Excel. Strong attention to detail and analytical skills. Good communication skills and the ability to work collaboratively in a team. Eagerness to learn and adapt to new technologies and techniques."
Yelp,"Entry Level Software Engineer - Data Backend (Remote, Canada)","We're looking for entry-level engineers who are eager to learn and contribute to building elegant, scalable systems. You'll have the opportunity to work with SQL and NoSQL data stores, data warehouses, batch processing, and stream processing solutions. Join us in leveraging machine learning across Yelp to address significant business challenges, improve user experiences, facilitate data-driven decisions, and maintain the reliability of Yelp's content. What You'll Do: Contribute to building systems that can effectively store and crunch terabytes of data. Support the infrastructure that empowers millions of Yelp’s users to make the best decisions. Engage with diverse challenges such as personalizing ads and search ranking, AI telephony and chatbots, advertiser retention and churn prevention, data-driven storytelling, clickstream analytics, content type classification, delivering personalized recommended businesses to users and sophisticated bot detection. Collaborate with cross functional teams, including software engineers, product managers and data scientists to identify and use the most relevant consumer and business data. Develop expertise in cutting-edge infrastructure for machine learning or data analytics or product feature use cases. Learn the fine art of balancing scale, latency and availability depending on the problem. What It Takes To Succeed: Good coding skills in Python or equivalent (ideally Java or C++). A passion for architecting large systems with elegant interfaces that can scale easily.A hunger for tracking down root causes—no matter how deep it takes you—and fixing them in systematic ways. Understanding of building data pipelines to train and deploy machine learning models and/or ETL pipelines for metrics and analytics or product feature use cases. Exposure to some of the following technologies: Apache Spark, AWS Redshift, AWS S3, Cassandra (and other NoSQL systems), AWS Athena, Apache Kafka, Apache Flink, Java, AWS and service oriented architecture."
Yelp,"Entry Level Software Engineer - Data Backend (Remote, Canada)","We're looking for entry-level engineers who are eager to learn and contribute to building elegant, scalable systems. You'll have the opportunity to work with SQL and NoSQL data stores, data warehouses, batch processing, and stream processing solutions. Join us in leveraging machine learning across Yelp to address significant business challenges, improve user experiences, facilitate data-driven decisions, and maintain the reliability of Yelp's content. What You'll Do: Contribute to building systems that can effectively store and crunch terabytes of data. Support the infrastructure that empowers millions of Yelp’s users to make the best decisions. Engage with diverse challenges such as personalizing ads and search ranking, AI telephony and chatbots, advertiser retention and churn prevention, data-driven storytelling, clickstream analytics, content type classification, delivering personalized recommended businesses to users and sophisticated bot detection. Collaborate with cross functional teams, including software engineers, product managers and data scientists to identify and use the most relevant consumer and business data. Develop expertise in cutting-edge infrastructure for machine learning or data analytics or product feature use cases. Learn the fine art of balancing scale, latency and availability depending on the problem. What It Takes To Succeed: Good coding skills in Python or equivalent (ideally Java or C++). A passion for architecting large systems with elegant interfaces that can scale easily.A hunger for tracking down root causes—no matter how deep it takes you—and fixing them in systematic ways. Understanding of building data pipelines to train and deploy machine learning models and/or ETL pipelines for metrics and analytics or product feature use cases. Exposure to some of the following technologies: Apache Spark, AWS Redshift, AWS S3, Cassandra (and other NoSQL systems), AWS Athena, Apache Kafka, Apache Flink, Java, AWS and service oriented architecture."
Yelp,"Entry Level Software Engineer - Data Backend (Remote, Canada)","We're looking for entry-level engineers who are eager to learn and contribute to building elegant, scalable systems. You'll have the opportunity to work with SQL and NoSQL data stores, data warehouses, batch processing, and stream processing solutions. Join us in leveraging machine learning across Yelp to address significant business challenges, improve user experiences, facilitate data-driven decisions, and maintain the reliability of Yelp's content. What You'll Do: Contribute to building systems that can effectively store and crunch terabytes of data. Support the infrastructure that empowers millions of Yelp’s users to make the best decisions. Engage with diverse challenges such as personalizing ads and search ranking, AI telephony and chatbots, advertiser retention and churn prevention, data-driven storytelling, clickstream analytics, content type classification, delivering personalized recommended businesses to users and sophisticated bot detection. Collaborate with cross functional teams, including software engineers, product managers and data scientists to identify and use the most relevant consumer and business data. Develop expertise in cutting-edge infrastructure for machine learning or data analytics or product feature use cases. Learn the fine art of balancing scale, latency and availability depending on the problem. What It Takes To Succeed: Good coding skills in Python or equivalent (ideally Java or C++). A passion for architecting large systems with elegant interfaces that can scale easily.A hunger for tracking down root causes—no matter how deep it takes you—and fixing them in systematic ways. Understanding of building data pipelines to train and deploy machine learning models and/or ETL pipelines for metrics and analytics or product feature use cases. Exposure to some of the following technologies: Apache Spark, AWS Redshift, AWS S3, Cassandra (and other NoSQL systems), AWS Athena, Apache Kafka, Apache Flink, Java, AWS and service oriented architecture."
Yelp,"Entry Level Software Engineer - Data Backend (Remote, Canada)","We're looking for entry-level engineers who are eager to learn and contribute to building elegant, scalable systems. You'll have the opportunity to work with SQL and NoSQL data stores, data warehouses, batch processing, and stream processing solutions. Join us in leveraging machine learning across Yelp to address significant business challenges, improve user experiences, facilitate data-driven decisions, and maintain the reliability of Yelp's content. What You'll Do: Contribute to building systems that can effectively store and crunch terabytes of data. Support the infrastructure that empowers millions of Yelp’s users to make the best decisions. Engage with diverse challenges such as personalizing ads and search ranking, AI telephony and chatbots, advertiser retention and churn prevention, data-driven storytelling, clickstream analytics, content type classification, delivering personalized recommended businesses to users and sophisticated bot detection. Collaborate with cross functional teams, including software engineers, product managers and data scientists to identify and use the most relevant consumer and business data. Develop expertise in cutting-edge infrastructure for machine learning or data analytics or product feature use cases. Learn the fine art of balancing scale, latency and availability depending on the problem. What It Takes To Succeed: Good coding skills in Python or equivalent (ideally Java or C++). A passion for architecting large systems with elegant interfaces that can scale easily.A hunger for tracking down root causes—no matter how deep it takes you—and fixing them in systematic ways. Understanding of building data pipelines to train and deploy machine learning models and/or ETL pipelines for metrics and analytics or product feature use cases. Exposure to some of the following technologies: Apache Spark, AWS Redshift, AWS S3, Cassandra (and other NoSQL systems), AWS Athena, Apache Kafka, Apache Flink, Java, AWS and service oriented architecture."
Yelp,"Entry Level Software Engineer - Data Backend (Remote, Canada)","We're looking for entry-level engineers who are eager to learn and contribute to building elegant, scalable systems. You'll have the opportunity to work with SQL and NoSQL data stores, data warehouses, batch processing, and stream processing solutions. Join us in leveraging machine learning across Yelp to address significant business challenges, improve user experiences, facilitate data-driven decisions, and maintain the reliability of Yelp's content. What You'll Do: Contribute to building systems that can effectively store and crunch terabytes of data. Support the infrastructure that empowers millions of Yelp’s users to make the best decisions. Engage with diverse challenges such as personalizing ads and search ranking, AI telephony and chatbots, advertiser retention and churn prevention, data-driven storytelling, clickstream analytics, content type classification, delivering personalized recommended businesses to users and sophisticated bot detection. Collaborate with cross functional teams, including software engineers, product managers and data scientists to identify and use the most relevant consumer and business data. Develop expertise in cutting-edge infrastructure for machine learning or data analytics or product feature use cases. Learn the fine art of balancing scale, latency and availability depending on the problem. What It Takes To Succeed: Good coding skills in Python or equivalent (ideally Java or C++). A passion for architecting large systems with elegant interfaces that can scale easily.A hunger for tracking down root causes—no matter how deep it takes you—and fixing them in systematic ways. Understanding of building data pipelines to train and deploy machine learning models and/or ETL pipelines for metrics and analytics or product feature use cases. Exposure to some of the following technologies: Apache Spark, AWS Redshift, AWS S3, Cassandra (and other NoSQL systems), AWS Athena, Apache Kafka, Apache Flink, Java, AWS and service oriented architecture."
Yelp,"Entry Level Software Engineer - Data Backend (Remote, Canada)","We're looking for entry-level engineers who are eager to learn and contribute to building elegant, scalable systems. You'll have the opportunity to work with SQL and NoSQL data stores, data warehouses, batch processing, and stream processing solutions. Join us in leveraging machine learning across Yelp to address significant business challenges, improve user experiences, facilitate data-driven decisions, and maintain the reliability of Yelp's content. What You'll Do: Contribute to building systems that can effectively store and crunch terabytes of data. Support the infrastructure that empowers millions of Yelp’s users to make the best decisions. Engage with diverse challenges such as personalizing ads and search ranking, AI telephony and chatbots, advertiser retention and churn prevention, data-driven storytelling, clickstream analytics, content type classification, delivering personalized recommended businesses to users and sophisticated bot detection. Collaborate with cross functional teams, including software engineers, product managers and data scientists to identify and use the most relevant consumer and business data. Develop expertise in cutting-edge infrastructure for machine learning or data analytics or product feature use cases. Learn the fine art of balancing scale, latency and availability depending on the problem. What It Takes To Succeed: Good coding skills in Python or equivalent (ideally Java or C++). A passion for architecting large systems with elegant interfaces that can scale easily.A hunger for tracking down root causes—no matter how deep it takes you—and fixing them in systematic ways. Understanding of building data pipelines to train and deploy machine learning models and/or ETL pipelines for metrics and analytics or product feature use cases. Exposure to some of the following technologies: Apache Spark, AWS Redshift, AWS S3, Cassandra (and other NoSQL systems), AWS Athena, Apache Kafka, Apache Flink, Java, AWS and service oriented architecture."
Maneva,Junior AI Engineer,"This entry-level role is ideal for someone eager to learn and grow in AI applications for real-world manufacturing challenges. The position involves contributing to training vision models for tasks such as classification, object detection, and segmentation, assisting with MLOps workflows, and monitoring model performance. The ideal candidate is passionate about learning and applying AI technologies, thrives in hands-on environments, and is motivated to grow alongside an innovative team. Main Responsibilities: Assist in developing and training vision-based AI models, including tasks like classification, object detection, and segmentation. Collaborate on building and maintaining pipelines for deploying AI/ML models in production. Support the integration and use of MLOps tools to streamline workflows. Monitor deployed models, collect performance data, and suggest optimizations. Help troubleshoot and update AI models under guidance to ensure reliability and performance. Contribute to cross-functional discussions to align AI applications with manufacturing needs. Assist with cloud platform setups (AWS, Azure, GCP) for training and deployment under supervision. Document workflows and best practices for reproducibility and operational improvements. Qualifications Education: Bachelor’s or Master’s degree in Computer Science, Machine Learning,DataScience, Engineering, or a related field. Relevant certifications or coursework in MLOps or AI model development is a plus. Experience: Familiarity with computer vision or machine learning through academic projects, internships, or personal initiatives. Exposure to Python, Linux, and basic AI/ML libraries (e.g., PyTorch, TensorFlow, or Scikit-learn).Any experience with real-world AI applications or internships in industrial or manufacturing environments is a bonus. Technical Skills: Proficiency in Python, Linux, Docker, and Git. Familiarity with machine learning workflows and tools for training and fine-tuning models. Familiarity with cloud platforms (AWS, Azure, GCP) for compute, storage, and AI services.Experience with ARM devices such as Jetson or Raspberry Pi is a plus. Eagerness to learn and gain hands-on experience with neural networks and MLOps. Soft Skills: Strong desire to learn and grow in the field of AI. Problem-solving mindset with attention to detail. Excellent communication and collaboration skills. Willingness to adapt and contribute in a fast-paced environment."
W Advisory France,Jr Data Analyst,"You will work closely with senior analysts and data scientists to collect, process, and analyze data from various sources. Your analytical skills will help identify trends and patterns that can drive strategic decision-making. The ideal candidate will possess a strong foundation in data analysis, statistical methods, and database management. Responsibilities: Collect and organize data from various sources to ensure accuracy and completeness. Conduct preliminary data analysis to identify trends, patterns, and anomalies. Assist in the preparation of reports and dashboards to communicate findings. Collaborate with team members to identify data needs and requirements. Support the development of data models and visualizations to enhance decision-making processes. Perform quality assurance checks on datasets to maintain data integrity. Stay updated with industry trends and best practices in data analysis and visualization techniques. Requirements: Bachelor's degree in DataScience, Statistics, Mathematics, or a related field. Knowledge of data analysis tools such as Excel, SQL, or Python. Familiarity with data visualization software like Tableau or Power BI. Strong analytical skills with a problem-solving mindset. Excellent attention to detail and ability to work with large datasets. Effective communication skills, both written and verbal. Ability to work collaboratively in a team environment and manage multiple projects"
BCJobs,Data Entry Analyst (Entry-Level) (Rebel Apparel),"Key Responsibilities: Accurately input and update data into various systems and databases. Review and clean data to ensure accuracy and completeness. Assist in generating and analyzing reports based on data trends. Collaborate with team members to resolve data discrepancies and issues. Maintain organized records and documentation for easy retrieval. Support data-related projects and tasks as assigned by supervisors. Qualifications: High school diploma or equivalent; associate’s degree or coursework in data management is a plus. Basic proficiency in Microsoft Office Suite (Excel, Word). Strong attention to detail and commitment to data accuracy. Good organizational skills with the ability to handle multiple tasks. Effective written and verbal communication skills. Ability to work independently and meet deadlines.Reliable internet connection and a quiet workspace if remote. Preferred Skills (Not Required): Familiarity with data analysis tools or software. Previous experience in a data entry or analytical role."
BCJobs,Entry-Level Data Analyst (Rebel Apparel),"Key Responsibilities: Collect, clean, and analyze data to generate actionable insights. Prepare reports and presentations of findings, clearly communicating complex data to stakeholders. Assist in identifying trends and patterns in datasets. Collaborate with cross-functional teams to understand data needs and deliver solutions. Contribute to the development and improvement of data collection and analysis processes. Stay updated with industry trends and best practices indataanalysis. Requirements: Bachelor’s degree in Data Science,Statistics, Mathematics, Computer Science, Economics, or a related field. Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.Proficiency in data analysis tools such as Excel, SQL, Python, or R (experience with statistical software is a plus). Ability to work effectively in a team environment and independently when needed. Excellent communication and presentation skills."
BCJobs,Entry-Level Data Analyst (Rebel Apparel),"Key Responsibilities: Collect, clean, and analyze data to generate actionable insights. Prepare reports and presentations of findings, clearly communicating complex data to stakeholders. Assist in identifying trends and patterns in datasets. Collaborate with cross-functional teams to understand data needs and deliver solutions. Contribute to the development and improvement of data collection and analysis processes. Stay updated with industry trends and best practices indataanalysis. Requirements: Bachelor’s degree in Data Science,Statistics, Mathematics, Computer Science, Economics, or a related field. Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.Proficiency in data analysis tools such as Excel, SQL, Python, or R (experience with statistical software is a plus). Ability to work effectively in a team environment and independently when needed. Excellent communication and presentation skills."
BCJobs,Entry Level Data Analyst (Rebel Apparel),"Key Responsibilities: Collect, clean, and preprocess data from various sources. Perform exploratory data analysis to identify trends, patterns, and anomalies. Utilize statistical methods and tools to analyze data and generate insights. Create and maintain dashboards, reports, and visualizations using tools like Excel, Tableau, or Power BI. Assist in developing and implementing data-driven strategies to support business objectives. Collaborate with team members to understand data needs and provide actionable recommendations. Document analysis processes and results for transparency and future reference. Qualifications: Bachelor’s degree in Data Science,Statistics, Mathematics, Computer Science, or a related field. Familiarity with data analysis tools and techniques, such as Excel, SQL, or Python/R. Strong analytical skills with attention to detail and accuracy. Ability to interpret and present data in a clear and meaningful way. Basic knowledge of data visualization tools (e.g., Tableau, Power BI) is a plus. Excellent communication and teamwork skills. Eagerness to learn and grow in the field of data analysis. Preferred Skills: Experience with data cleaning and transformation.Understanding of basic statistical concepts and methods. Exposure to machine learning algorithms and data modeling is an advantage."
Keyloop,Data Analyst,"You will be responsible for a variety of work streams including generating insight, data visualisation and BI product optimisation. To succeed in this role, you need to be creative with both data and visualisations and have an analytical mindset. This entry-level Data Analyst will be responsible for manually entering, analyzing preparingdatato provide valuable insights and recommendations to clients They will work closely with the account teams to gather andanalyzedata, develop and send reports, and provide solutions to help the Regional Go-To-Market Team gain a competitive edge and bring value to clients. Analyze and Collect Data. Analyze datasets to uncover trends, opportunities, and insights for account teams. Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes. Manage data cleansing, to ensure data is client friendly. Data Cleansing / Data Matching: Manage manual cleansing reports and pull data. Working alongside account teams, clients, dealers and internal teams to assist with lead event and source launches (testing the data), ensuring it comes through correctly. Help trouble-shoot data matches between various reports.Running and Modifying SQL Queries. Correcting issues with the data when it does not match. Report Preparation: Running Monthly Industry data for Monthly and Quarterly Client Reviews. Preparing and sending the data for other client monthly and ad hoc reports. Prepare data-driven reports and insights for clients highlighting key findings and recommendations. Support the account teams with various client reports. Skills and Qualifications: 1+ years of relevant experience or education in data analysis. Hyper attention to detail. Organized PowerPoint. A Master at Excel and experience with pivot tables. SQL experience with data manipulation preferred. Graphic design and or Photshop skills are appreciated. Experience with data visualization and Power BI. Interest in AI, and data automation. Interest in the automotive industry."
Keyloop,Data Analyst,"You will be responsible for a variety of work streams including generating insight, data visualisation and BI product optimisation. To succeed in this role, you need to be creative with both data and visualisations and have an analytical mindset. This entry-level Data Analyst will be responsible for manually entering, analyzing preparingdatato provide valuable insights and recommendations to clients They will work closely with the account teams to gather andanalyzedata, develop and send reports, and provide solutions to help the Regional Go-To-Market Team gain a competitive edge and bring value to clients. Analyze and Collect Data. Analyze datasets to uncover trends, opportunities, and insights for account teams. Develop, maintain and analyze performance metrics and reports that support data-driven decision-making processes. Manage data cleansing, to ensure data is client friendly. Data Cleansing / Data Matching: Manage manual cleansing reports and pull data. Working alongside account teams, clients, dealers and internal teams to assist with lead event and source launches (testing the data), ensuring it comes through correctly. Help trouble-shoot data matches between various reports.Running and Modifying SQL Queries. Correcting issues with the data when it does not match. Report Preparation: Running Monthly Industry data for Monthly and Quarterly Client Reviews. Preparing and sending the data for other client monthly and ad hoc reports. Prepare data-driven reports and insights for clients highlighting key findings and recommendations. Support the account teams with various client reports. Skills and Qualifications: 1+ years of relevant experience or education in data analysis. Hyper attention to detail. Organized PowerPoint. A Master at Excel and experience with pivot tables. SQL experience with data manipulation preferred. Graphic design and or Photshop skills are appreciated. Experience with data visualization and Power BI. Interest in AI, and data automation. Interest in the automotive industry."
BCJobs,Entry-Level Data Analyst (Rebel Apparel),"Key Responsibilities: Collect, clean, and analyze data to generate actionable insights. Prepare reports and presentations of findings, clearly communicating complex data to stakeholders. Assist in identifying trends and patterns in datasets. Collaborate with cross-functional teams to understand data needs and deliver solutions. Contribute to the development and improvement of data collection and analysis processes. Stay updated with industry trends and best practices indataanalysis. Requirements: Bachelor’s degree in Data Science,Statistics, Mathematics, Computer Science, Economics, or a related field. Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.Proficiency in data analysis tools such as Excel, SQL, Python, or R (experience with statistical software is a plus). Ability to work effectively in a team environment and independently when needed. Excellent communication and presentation skills."
BCJobs,Entry-Level Data Analyst (Rebel Apparel),"Key Responsibilities: Collect, clean, and analyze data to generate actionable insights. Prepare reports and presentations of findings, clearly communicating complex data to stakeholders. Assist in identifying trends and patterns in datasets. Collaborate with cross-functional teams to understand data needs and deliver solutions. Contribute to the development and improvement of data collection and analysis processes. Stay updated with industry trends and best practices indataanalysis. Requirements: Bachelor’s degree in Data Science,Statistics, Mathematics, Computer Science, Economics, or a related field. Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy.Proficiency in data analysis tools such as Excel, SQL, Python, or R (experience with statistical software is a plus). Ability to work effectively in a team environment and independently when needed. Excellent communication and presentation skills."
Ericsson,Data Professional (Entry Level),"In this role you will work with an outstanding local and international team while interacting closely with our customers, influence and drive complex, innovative projects, overcome barriers and address sophisticated technical challenges in application development. What you will do: Define how we instrument, prioritize, and store data that powers AI/ML solutions. Design and implement scalable data models, predictive models andmachine learning algorithms. Evolve and optimize our data and data pipeline architecture, including data flow & collection for multi-functional teams, and integrate them with the company-wide architecture. Work closely with other team members to ‘digitize’ & ‘automate’ various reports (i.e. financial/progress tracking/planning etc.) Gather requirements from team-members to design and implement various data dashboards, templates, KPI scorecards. Evaluate and improve existing reports/visualizations. Create & maintain data repositories. The skills you bring: Bachelor’s degree in Business, IT, Engineering, Mathematics, Statistics, or a related discipline. Practical experience in building data pipelines and models. Familiarity with key data engineering technologies such as Python and SQL, with exposure to tools and languages like Spark and Java. Exposure to batch and streaming data architectures. Knowledge of various database structures, including relational and NoSQL databases. Proficiency in MS Excel and basic experience with data visualization tools like Power BI. Introductory experience with machine learning frameworks such as PyTorch, TensorFlow, or Keras. Understanding of cloud-native architectures and services, with basic knowledge of platforms like AWS, Azure, or GCP."
Applicantz,Data Scientist,"Candidate needs to have a combination of cloud infrastructure skills (particularly on AWS), strong proficiency in Python and JavaScript, expertise in NLP, and advanced knowledge of ML frameworks like PyTorch and HuggingFace. Additionally, they must have practical experience in using LLMs to build robust AI applications, including a solid understanding of agentic frameworks, RAG workflows, and prompt engineering.By leveraging these tools and skills, the team will be well-equipped to develop an AI code generation platform that enhances engineering productivity, ensures code consistency, and embeds Autodesk’s security and compliance standards.Technical Tools and Skills Required1. Cloud Infrastructure: AWS• The project will be built on Amazon Web Services (AWS), leveraging its robust cloud infrastructure for scalability, security, and reliability. Team members should be proficient in using AWS services such as EC2, S3, Lambda, RDS, and DynamoDB. Familiarity with AWS tools like CloudFormation or Terraform for Infrastructure as Code (IaC) is also essential to ensure automated and consistent deployment of resources.2. Programming Languages: Python/JavaScript• Python is required for developing core components, including AI andmachine learning models,data processing pipelines, and backend services. Proficiency in Python libraries such as NumPy, Pandas, and scikit-learn is valuable.• JavaScript is needed for frontend development, particularly for building user interfaces and integrating with APIs. Familiarity with frameworks like React or Vue.js will help in developing responsive and dynamic UI components for the platform. 3. Natural Language Processing (NLP) Experience:• Team members should have experience in Natural Language Processing (NLP) to understand and interpret various input sources like LUCID diagrams, API specifications, and FIGMA designs. This includes expertise in text preprocessing, entity recognition, and language modeling techniques, which are crucial for extracting relevant information and generating code. 4.Experience in Agentic Frameworks, Retrieval-Augmented Generation (RAG) Workflows, and Prompt Engineering: • Knowledge of Agentic frameworks (frameworks that enable autonomous agent behavior in AI systems) is essential to develop sophisticated AI models that can autonomously generate code based on input data.• Proficiency in Retrieval-Augmented Generation (RAG) workflows is required to ensure that the AI models can retrieve relevant information dynamically and generate accurate code snippets. Understanding how to implement RAG will help in building an efficient and context-aware AI platform.• Prompt engineering skills are critical for designing and optimizing the prompts used to interact with the underlying large language models(LLMs). This involves crafting effective prompts that guide the model to generate the desired outputs, improving both accuracy and relevance.5. Proficiency in Machine Learning Frameworks: PyTorch, HuggingFace• The team must be adept in using popular ML frameworks like PyTorch for building and trainingdeep learning models. PyTorch is highly flexible and well-suited for experimentation and custom model development.• Familiarity with the HuggingFace ecosystem, particularly its Transformers library, is crucial for working with pre-trained large language models (LLMs) and fine-tuning them for specific tasks. Experience with HuggingFace’s tools will enable the team to leverage state-of-the-art models and accelerate the development process. 6.Must-Have Experience: Building Applications on Top of Large Language Models(LLMs)• Team members should have hands-on experience in creating applications that utilize Large Language Models(LLMs) like GPT-3, GPT-4, or other transformer-based models. This includes knowledge of integrating LLMs into applications, handling model inputs and outputs, and optimizing their performance for specific use cases.• Practical experience in deploying LLM-based applications in a production environment, managing their scalability, latency, and cost-effectiveness, is also critical to ensure the success of the project."
Refonte Learning,AI & Data Scientist Study & Internship,"Responsibilities: Collaborate with cross-functional teams to develop, deploy, and maintain AI-driven solutions. Assist in collecting, cleaning, and analyzing data from various sources to derive actionable insights. Contribute to the design and implementation of scalable data pipelines for model training and deployment. Support the integration of machine learningmodelsinto production systems using DevOps best practices.Participate in designing and optimizing CI/CD pipelines for continuous integration, testing, and deployment of AI applications on cloud platforms. Assist in infrastructure provisioning, configuration, and monitoring using cloud services such as AWS, Azure, or Google Cloud.Work closely withdatascientists and engineers to understand business requirements and translate them into technical solutions.Research and experiment with emerging technologies and tools in AI, Data Science, DevOps, and Cloud domains. Collaborate on documentation efforts to ensure knowledge sharing and best practices across the team. Projects You Will Work On:- Multi Cloud AI Infrastructure Configuration, Automation and Deployment- Full Stack AI DevOps & Development- Generative AI model, Large Language Models and Foundations models to transform input to output. NB: Input can be text, images, audios or videos; Output can also be text, images, audios or videos- Finance Fraud Detection: Develop advanced fraud detection algorithms leveraging financial data analysis.- Recommender System: Contribute to personalized recommendation systems, enhancing user experiences across platforms.- Sentiment Analysis: Explore sentiment analysis to extract insights from textual data, shaping user sentiment understanding.- Chatbots: Engage in intelligent chatbot development, revolutionizing customer interactions and support.- Image/Audio Video Classification: Push boundaries with multimedia technology by working on image and audio video classification projects.- Text Analysis: Uncover hidden patterns in textual data through sophisticated text analysis techniques.Roles & Responsibilities:- Collaborate with our esteemed AI & data science experts to collect, clean, and analyze extensive datasets, honing skills in data preprocessing and visualization.- Contribute to the development of predictive models and algorithms, employing cutting-edge machine learning techniques to solve real-world challenges.- Work closely with team members to design, implement, and evaluate experiments, fostering a collaborative and innovative environment.- Stay updated with the latest industry trends and best practices in data science, applying newfound knowledge to enhance project outcomes. Qualifications:- Currently pursuing any degree showcasing a strong commitment to continuous learning and professional growth.- Exceptional written and verbal communication skills, vital for effective collaboration and articulation of complex ideas.- Demonstrated ability to work both independently and as part of a cohesive team, highlighting adaptability and strong teamwork capabilities."
Jerry,Staff Data Scientist (User Growth),"How you will make an impact: Define, understand, and test levers to drive profitable and scalable user acquisition across our paid and organic channels. Design, run, andanalyzeA/B experiments across different channels, extract key insights, share learnings and continue iterating. Build key reports, dashboards, and predictive models to monitor the performance of our paid ads and marketing channels, communicate analytical outcomes to our teams, and make recommendations on next steps. Transform and refine raw production data for analytical needs. Preferred experience: Bachelor’s degree in a quantitatively or intellectually rigorous discipline. A few years of consulting experience (MBB preferred). Prior experience with customer acquisition or performance marketing. High level of comfort with SQL and Python (or similar ML programming language)"
Microsoft,Research Intern - Agent Systems for AI Infrastructure,"Responsibilities: Research Interns put inquiry and theory into practice. Alongside fellow doctoral candidates and some of the world’s best researchers, Research Interns learn, collaborate, and network for life. Research Interns not only advance their own careers, but they also contribute to exciting research and development strides. During the 12-week internship, Research Interns are paired with mentors and expected to collaborate with other Research Interns and researchers, present findings, and contribute to the vibrant life of the community. Research internships are available in all areas of research, and are offered year-round, though they typically begin in the summer. Additional Responsibilities: Development and Implementation: Design LLM-based agent systems for AI infrastructure. Implement prototypes and conduct experiments to test and validate them. Research and Analysis: Conduct thorough research on emerging trends in agent systems for AI software and hardware infrastructure.Collaboration: Work closely with cross-functional teams, including hardware engineers, software developers, anddatascientists, to integrate your ideas with existing and future agent projects. Documentation and Reporting: Prepare detailed documentation of simulations, methodologies, and findings. Present results and insights to team members and stakeholders. Innovation and Problem-Solving: Identify challenges and bottlenecks in AI infrastructure and propose innovative solutions.QualificationsRequired Qualifications: Currently enrolled in a bachelor's, master's, or PhD program in Computer Science, Electrical Engineering, Machine learning, Mathematics, or a related field. Preferred Qualifications: Proficient in programming languages such as C/C++ or Python. Familiar with fundamental concepts related to LLM, prompt engineering, and LLM-based agents. Experience in AI for systems and/or systems for AI.Passionate about addressing real-world large-scale infrastructure problems using AImodels. Experience with AI infrastructure or LLM would be a plus.Proficient analytical and problem-solving skills and communication skills, both written and verbal"
Amazon,"Machine Learning Engineer MLOps/MLaaS, ICC AI Center of Excellence","Key job responsibilities: You own the development and operationalization of solutions deployed in production. You work across multiple teams to integrate our solutions with products owned by our partners. You help design model experimentation processes and frameworks in synergy with our scientists. You help the team grow and cultivate best practices in software development, MLOps, and experimentation. A day in the life: Almost everyday offers new challenges and opportunities for growth. Where one day will offer implementation of experimentation tooling, the next day may be focused on our operational excellence in maintaining our code base. Later in the week, you may sort technical challenges with our partners to help them enrich their products with ourmodels. On some days or weeks, you may watch over our products and stand ready to intervene and provide support to partners consuming ourmodels.If you are not sure that every qualification on the list above describes you exactly, we'd still love to hear from you! At Amazon, we value people with unique backgrounds, experiences, and skillsets. If you’re passionate about this role and want to make an impact on a global scale, please apply! Basic Qualifications: 2+ years of non-internship professional software development experience1+ years of non-internship design or architecture (design patterns, reliability and scaling) of new and existing systems experience. Experience programming with at least one software programming language. Bachelor's degree in computer science or equivalent. 1+ years experience and knowledge in MLOps, in deploying, operationalizing, and maintaining scalable AI/ML-solutions in production.Preferred Qualifications: 2+ years of full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations experienceMaster's degree in computer science or equivalent. Experience in machine learning, data mining, information retrieval, statistics or natural language processing"
"InterDigital, Inc.",AIML Research Engineer,"The candidate will have a solid theoretical background in artificial intelligence, neural networks andmachine learning. The candidate will preferably have some practical experience with AI/ML model design and training as well as in applying AI/ML techniques in specific domain expertise e.g., imaging, video, large languagemodelsand/or wireless. The candidate will conduct research with focus on fundamental aspects of deploying AI/ML techniques in wireless systems, develop innovative solutions addressing the challenges of advanced deployments of AI/ML in future wireless systems with focus on the design of solutions applied to physical radio layer techniques (PHY Layer 1), medium access control (MAC Layer 2) and/or resource control functions (RRC Layer 3) as well as novel applications for specific use cases such as joint communication and sensing, mobility optimizations or the likes. The candidate will contribute to the 3GPP RAN AI/ML team: By carrying out research on fundamental and/or applied AI/ML techniques targeting the evolution and/or design of the physical radio layer for 5G-Advanced radio technologies, for 6G radio access and beyond ;By conducting evaluation, analysis, and development of innovative AI-driven solutions applicable in L1/PHY, L2/MAC and/or L3/RRM such as feedback compression, channel state prediction, beam management, mobility and RRM management, positioning and/or sensing ;By supporting 3GPP standardization activities through the preparation of AI/MLmodels, simulations, and contributions in Radio Access Network (RAN) Working Groups (WG). Qualifications/Requirements B.Eng., M.S., or Ph.D. in Computer Science,Statistics, Electrical Engineering, Applied Math or other relevant discipline (or similar / equivalent)0-5 years of relevant experience in the field of machine learning with theoretical understanding of various training methods + neutral networks. In-depth knowledge in one or more of the following areas: machine learning, deep learning, data mining, optimization. Hands on experience with ML frameworks such as Pytorch, Tensorflow. Strong analytical, technical, innovative and leadership skills. Ability to thrive in a dynamic, innovative, collaborative and team-oriented work environment.Dedicated, result oriented professional with an attitude for getting the work done. Willing to take challenges in a fast paced and high-pressure environment. Experience with wireless communications and/or signal processing may be considered an advantage. Experience with GitHub/GitLab, Matlab, NS-3, C/C++ and/or Python may be considered an advantage."
RAD Intel,Machine Learning Engineer (NLP),"Objectives of this Role: Work with the Machine Learning team to build state-of-the-art AI solutions that leverage the power of NLP to unlock critical business and audience related insights and relationships. Develop new features and ML models in support of rapidly emerging business and project requirements. Assume leadership of new projects from conceptualization to deployment. Ensure application performance, uptime, and scale, maintaining high standards of code quality and thoughtful application design. Work with agile development methodologies, adhering to best practices and pursuing continued learning opportunities. Experience: Experienced in large scale (big) data analytics:Deep understanding of feature spaces and their internal relationships. Deep understanding of pattern recognition (unsupervised learning). Must possess strong and proven knowledge of NLP including: Understanding of latest sentence transformer / embedding models. Understanding of multi purpose NLP models(BERT, RoBERTa, ELECTRA, etc). Deep understanding of how your created features work, need to expand on why predictions are made and what needs to be done to change or follow your predictions. Time complexity management - most models/ algorithms should run in real time. Required Skills and Qualifications: Computer Science Master's degree preferred but Bachelor's will also be considered. Python, TensorFlow, Anaconda, Flask, Gunicorn, MySQL, Linux, Nignx, RESTful APIsProficient in Python (2 to 3 years). Proficient in using Jupyter Notebooks (1 to 2 years). Proficient in basic SQL and python database connectors (1 to 2 years). Experienced in Version Control, Git Flow model (1 year). Knowledge of RESTful API development (1 year). Knowledge in launching software or services, iterative development, understanding of Agile principles, high code quality / reusability, Code Design Patterns. Knowledge in writing well-defined requirements including features, user stories, and acceptance criteria using Jira Software. Good Skills to Have: Knowledge of “Textual Replacements” at a word / phrase / sentence level. Knowledge about Text (GPT, etc.) Generative Models. Knowledge of Linux Shell Scripting, SSH Tunnelling."
COMCO Controls,Junior Data Scientist,"Primary Responsibilities: Develop and implement advanced data models. Conduct predictive and prescriptive analytics to forecast trends and outcomes. Collaborate with cross-functional teams to integrate data-driven solutions into business processes. Communicate complex data insights and recommendations to stakeholders.Develop data collection plans and ensure integrity of data collected through all stages of acquisition and processing. Combine expert knowledge of statistical methodology and analysis with advanced programming skills to perform exploratory data analysis to uncover patterns in user's health and activities. Contribute to data quality control, model validation and model explainability investigation. Collect, process, and analyze large datasets to extract meaningful insights. Collaborate with cross-functional teams to understand business requirements and deliverdata-driven solutions.Createdatavisualizations and dashboards to communicate findings to stakeholders.Conduct A/B testing and other experiments to optimize business processes. Stay updated with the latest trends and advancements indatascience andmachine learning. Required Qualifications and experience:Bachelors in computer science,DataScience, or a related field, with a focus on databases anddataanalytics, or equivalent combination of education, training, and experience is required. 2-4 years of previous related job experience. Experience with tools and systems on Microsoft BI Stack, including SSRS and TSQL, Power Query, Power BI. Experience in Microsoft Fabric and Azure DevOps would be considered an asset. Knowledge of database fundamentals such as multidimensional database design, relational database design. Creating complex SQL queries. Understanding of software development architecture as well as technical aspects.Analyzing data in Microsoft Access databases, Excel sheets and summarizing data with pivot tables. Proficiency in programming languages (e.g., Python, R). Ability to handle complex datasets and derive actionable insights. Extensive experience performing data analysis and strong data visualization skills. Ability to convert business requirements into technical specifications and to analyze complex datasets and derive actionable insights. High degree of trust handling confidential information. Adaptable and willing to learn new techniques. Ability to adapt quickly to change and work under pressure with tight deadlines. Self-starter who can initiate, prioritize, and organize projects and decisions to meet deadlines consistently. Strong interpersonal skills with the ability to work independently and within a team environment, taking direction from others and providing guidance and expertise to numerous internal cross-functional groups and external stakeholders. Excellent written and verbal communication skills to present findings to non-technical stakeholders."
Oeson,Data Science Intern,"Projects You Will Work On:- Finance Fraud Detection:Develop advanced fraud detection algorithms leveraging financial data analysis.- Recommender System: Contribute to personalized recommendation systems, enhancing user experiences across platforms.- Sentiment Analysis: Explore sentiment analysis to extract insights from textual data, shaping user sentiment understanding.- Chatbots: Engage in intelligent chatbot development, revolutionizing customer interactions and support.- Image/Audio Video Classification: Push boundaries with multimedia technology by working on image and audio video classification projects.- Text Analysis: Uncover hidden patterns in textual data through sophisticated text analysis techniques. Roles & Responsibilities:- Collaborate with our esteemed data science experts to collect, clean, and analyze extensive datasets, honing skills in data preprocessing and visualization.- Contribute to the development of predictive models and algorithms, employing cutting-edge machine learningtechniques to solve real-world challenges.- Work closely with team members to design, implement, and evaluate experiments, fostering a collaborative and innovative environment.- Stay updated with the latest industry trends and best practices indatascience, applying newfound knowledge to enhance project outcomes. Qualifications:- Currently pursuing any degree showcasing a strong commitment to continuous learning and professional growth.- Exceptional written and verbal communication skills, vital for effective collaboration and articulation of complex ideas.- Demonstrated ability to work both independently and as part of a cohesive team, highlighting adaptability and strong teamwork capabilities."
Darkhorse Emergency,Data Analyst / Consultant,"What you'll be doing: Work at Darkhorse Emergency is diverse and ever-changing. We implement our proprietary software and solve interesting and challenging business problems for a wide variety of fire and emergency clients. Every client is different. The interpretation of their data and unique needs requires strong analytical skills. You will work to understand our clients'data and bring it into our proprietary software and provide them with an in-depth analysis that highlights root causes and opportunities. You will work with a team of analysts and take part in the project life cycle from start to finish, including: Gathering and interpreting Fire and EMS data.Our data can come from different sources, including client tools and open databases, as well as flat files. Bringing data together in our databases will be a big part of what you do here. Data cleaning and summary analysis. Data is rarely what you want it to be. Cleaning and understanding your inputs are the first required step in any quantitative project. Implementing our Darkhorse Emergency Platform. Matching clients' unique data to the fields required for our platform is a crucial step, and requires business context as well as technical savvy. Validating findings and stories in the data. Do your findings pass the smell check? A mix of experience, expertise and creativity is required to validate your findings and distinguish the right signals from data errors. Creating stunning data visualizations. Using tools like QGIS, Jupyter Notebooks or even Excel, you will work with the data to create visually engaging visualizations and maps to clarify our findings. Creating succinct presentations and reports. Often times our analyses need to be translated into a report that is readable, actionable, and isn't going to collect dust on the shelf. We believe less is more, and we take the time to make it succinct.Translating client needs into achievable solutions. Using your understanding of the industry and the client's unique challenges, you will provide our clients with actionable recommendations. Understanding our clients means that we can confidently approach their business problems. Brainstorming creative approaches. Theory and practice never align, so creativity is a must in planning a solution. Improving our tools and processes. As a growing company, we must continually search for the better way. We expect you to spend a good portion of your time on improving processes. Maintaining client relationships. As a SaaS company, You will become the trusted go-to person for our clients. What we look forWe believe the right person is not defined by years of experience or education. We look for the right people at all levels of their career. Here are some of the things that will make you stand out: You are a quick learner. The tools we will use next year have not been invented yet. People thriving on continuously learning and growing do well here.You are a wizard withdata, and comfortable using some combination of Excel, SQL, and Python. You ideally have some experience using GIS tools like QGIS to visualize data geographically. You genuinely enjoy problem-solving and finding patterns. The harder the challenge, the harder it is to put it down until you crack it.You enjoy working with clients, learning about their values and challenges, and use our tools to make a difference.You are able to focus on the most critical parts. In a haystack ofdata, you are able to find the needle that can pinpoint issues or root causes.You can communicate well. Having the correct analytical solution is one thing, but you should also be able to develop your solution methodically and explain the whole thing to your grandmother in the end.You are motivated and comfortable immersing yourself indataand understanding the message behind it. You can be skeptical about your findings if they don't seem correct. You're detail-oriented but can see the big picture.You are creative in your ways of interpreting and analyzing the data. When one avenue leads to a dead-end, you can use different data sources to come to an outcome that satisfies the business need"
Invoke Co.,Data Engineer (Part-Time Contractor),"Responsibilities: Work alongside our development team to deliver high quality products. Collaborate with the product and strategy teams on data requirements and use cases. Research & stay up to date on modern technologies. Assist in the design, development, and maintenance of data pipelines and ETL processes. Perform data sourcing, organization, cleaning and transformation tasks. Collaborate with technology team to ensure data quality and applicability for training machine learning models Develop scripts and tools for data processing and automation Troubleshoot and resolve data-related issues and inconsistencies. Support machine learning model training iterations. Requirements: Familiarity with Python. Experience with data modeling, data cleaning, and data transformation Basic understanding of cloud platforms (e.g., AWS, Azure, Google Cloud) and their data services. Solid understanding of machine learning principles and data requirements. Understanding of data privacy and security principles Strong analytical and problem-solving skills You’re curious to work in an Agile, Scrum-based environment with a team or on your own You’re excited at the opportunity to create quality user experiences You love to collaborate with all types of people including Design, User Experience and Strategy. Completed/Earning a Bachelor’s degree in Computer Science or a related field"
Accentrust,Data Engineer,"As a member of Accentrust, you will work on various aspects of the data engineering lifecycle, from data modeling and ETL processes to data warehousing and analytics. You will collaborate closely with our product team, engineers, and stakeholders to design and implement scalable data pipelines and infrastructure. You’ll also play a critical role in transforming raw data into valuable insights that drive business decisions. This role is ideal for someone with a strong foundation in data engineering who is eager to take ownership of impactful projects and work in a collaborative, fast-paced environment. Responsibilities:Design, develop, and maintain scalable ETL pipelines for data collection, transformation, and storage. Collaborate with cross-functional teams to understand data requirements and build efficient data models and warehouses. Develop, test, and optimize SQL queries and data pipelines to ensure efficient data processing and retrieval. Implement best practices for data governance, including data quality, integrity, and security. Work with cloud-based data platforms and tools to manage and process large-scale datasets. Monitor and troubleshoot data pipelines to ensure reliable data flow and real-time availability. Provide actionable insights by analyzing data and generating reports. Build and manage web scraping pipelines using tools such as Scrapy,Selenium, or BeautifulSoup to collect and process data from various sources. Stay informed about the latest trends in generative AI technologies (e.g., GPT, BERT, LLAMA) and understand how they can be leveraged in data engineering processes. Develop data visualizations using Tableau, Power BI, Matplotlib, or Seaborn to create dashboards and custom visualizations that support decision-making and data-driven insights. Embrace modern tools and workflows like Git, Docker, and CI/CD to enhance operational efficiency. Stay up-to-date with the latest trends and technologies indataengineering and apply them to improve our systems.Qualifications:Bachelor’s or Master’s degree in Computer Science,Data Science or a related field.At least 1 year of relevant work experience (including internships or project work as part of a Master’s program). Strong proficiency in programming languages like Python and SQL. Experience with ETL processes, data modeling, and data warehousing solutions. Familiarity with cloud-based platforms such as AWS, Google Cloud, or Azure. Familiarity with NoSQL and SQL databases (e.g., MongoDB, MySQL, PostgreSQL). Proficiency in using web scraping tools like Scrapy, Selenium, BeautifulSoup for data extraction from various websites. Experience with data transformation and analytics tools such as Pandas and NumPy. Basic understanding of generative AI models(e.g., GPT, BERT, LLAMA) and how they can be applied to data processing or analysis. Familiarity with Tableau, Power BI, Matplotlib, or Seaborn for data visualization and reporting. Strong problem-solving skills and ability to work independently or collaboratively within a team. Familiarity with Docker, CI/CD pipelines, and modern DevOps practices is a plus. Experience with machine learning pipelines or advanced analytics is a bonus"
TikTok,Machine Learning Engineer Intern (Data-TnS-Algo) - 2024 Start (BS/MS/PhD),"Responsibilities: Build industry-leading content safety systems for TikTok. Develop highly-scalable classifiers, tools, models and algorithms leveraging cutting-edge machine learning, computer vision and data mining technologies. Understand product objectives and improve trust and safety strategy and model's performance. Work with cross-functional teams to protect TikTok globally. Qualifications Currently pursuing a BS/MS/PhD degree in Software Development, Computer Science, Computer Engineering, or a related technical discipline.• Solid experience in at least one of the following areas: recommendation system,machine learning, pattern recognition, NLP,data mining, or computer vision. Experience in recommendation, search, advertising, or other related projects. Published papers in the top AI conferences or journals is a plus, including KDD, IJCAI, WWW, WSDM, ICML, NeurIPS, CVPR, ECCV, ICCV, ACL, etc. Competition experience in machine learning,data mining, CV, NLP, etc. Good understanding of data structures and algorithms. Passion for techniques and solving challenging problems.​"
theScore,"Data Analyst","As a key member of our Data Analytics team, you will:Help enhance the useability and adoption of a data exploration tool across the organization. Reduce query and model run times, simplify inter-DAG dependencies, and continuously improve on best practices and documentation. Liaise with analytics engineers to build curated data modelsand solve related issues. Assist team members in solving data issues related to both analysis and modeling. Develop a deep understanding of how users interact with theScore/ESPN Bet apps and websites. Complete analyses and present clear recommendations to business stakeholders. Identify opportunities to apply data modeling and advanced analytics techniques to drive insights. Peer review team's analyses to ensure data and insights shared are accurate and informative. About You: University degree in Computer Science,Statistics, Business, or related field. Strong knowledge of relational databases and SQL. Experience with data exploration tools such as: Looker, Mode, ThoughtSpot, Tableau, Power BI, and Periscope/Sisense. Experience with dbt or dbt Cloud. Experience with modern data warehouses such as: Redshift, BigQuery, Snowflake, and Synapse. A passion and curiosity for solving analytical problems using quantitative approaches. Ability to take complexdataand present it in a clear manner to cross-functional groups. Ability to focus in a fast paced environment and multitask. Excellent written and oral communications skills. Knowledge of analysis tools; R or Python packages preferred."
DataAnnotation,Data Engineer,"You will work with the chatbots that we are building in order to measure their progress, as well as write and evaluate code.To apply to this role, you will need to be proficient in either Python and/or JavaScript. Your role will require proficiency in at least one programming language (JavaScript, Python, C#, C++, HTML, SQL, or Swift) in order to solve coding problems (think LeetCode, HackerRank, etc). For each coding problem, you must be able to explain how your solution solves the problem Responsibilities: Come up with diverse problems and solutions for a coding chatbot. Write high-quality answers and code snippets. Evaluate code quality produced by AI models for correctness and performance. Qualifications Proficient in either Python and/or JavaScript. Excellent writing and grammar skills. A bachelor's degree (completed or in progress). Previous experience as a Software Developer, Coder, Software Engineer, or Programmer."
MethodHub,Data Engineer,"Skills -Synapse/PysparkNotebook/Pipeline development Data warehousing Experience must. ETL pipelines using Pyspark must. Bitbucket code management must. CICD knowledge must.Understanding and implementation knowledge on various Microsoft Azure Tools (ADF, DataBricks, Azure Synapse) preferred. Knowledge and understanding of MS Azure DevOps preferred. Should have experience in Agile methodology must. Retail experience is preferred."
StackAdapt,Data Engineer,"What you'll be doing: Design modular and scalable real time data pipelines to handle huge datasets. Understand and implement custom ML algorithms in a low latency environment. Work on microservice architectures that run training, inference, and monitoring on thousands of ML models concurrently. What you'll bring to the table: Have the ability to take an ambiguously defined task, and break it down into actionable steps. Have deep understanding of algorithm and software design, concurrency, and data structures. Experience in implementing probabilistic or machine learning algorithms. Interest in designing scalable distributed systems. A high GPA from a well-respected Computer Science program. Enjoy working in a friendly, collaborative environment with others"
Experis Canada,Data Engineer,"The ideal candidate will have excellent problem-solving skills, strong communication skills, and the ability to work independently as well as collaboratively in a team. What's the Job? Provide data and implementation support for strategic projects in the our clients Digital Innovation Team.Optimize code run time to meet critical business SLA. Write Python code to extract, clean, and preprocess data from various sources including web scraping to ensure quality and integrity of the data for use in model development. Work closely with modelers/data scientists to support their data transformation, analyses, and model coding activities. Design, develop, and maintain ETL pipelines and APIs to host complex algorithms in an Azure production environment. What's Needed? B.Sc. Degree or higher in Computer Science or Information Technology. 5+ years proven hands-on working experience in data engineering and/or software development, with a strong focus on Python. Expert in data manipulation on large datasets with advanced programming/scripting skills in Python. Building high-performance APIs using Python and Microsoft Azure. Implementing automated ETL data pipelines using Python hosted on Microsoft Azure."
Updata,ML Specialist,"Key Responsibilities: Conduct exploratory data analysis (EDA) and visualize data using various libraries. Build, deploy, and monitor machine learning models.Implement computer vision and time series analysis techniques in projects.Implement and manage MLOps workflows and pipelines on cloud platforms. Utilize big data services and manage data pipelines.Collaborate with teams to integrate research code into industrial applications. Qualifications: Strong experience with Python, including advanced features and real-world application development.Proficiency in cloud platforms, including MLOps, data storage, serverless functions, and cloud security.Experience with tools and techniques such as Scikit-learn, OpenCV, PyTorch, and TensorFlow. Active participation in open-source contributions, workshops, or webinars is a plus. Excellent problem-solving skills and a keen eye for detail.Strong communication skills, both written and verbal."
Electronic Arts (EA),Machine Learning Scientist,"Your Responsibilities: Contribute to the ML research strategy to create new player experiences, exploring frontier technologies to shape the future of Sports Gaming Work closely with the engineering team to support experiments with tooling and platforms, as well as data acquisition and management Share your results through presentations, papers, prototypes and compelling interactive demonstrations. Stay abreast of the latest advancements in relevant technologies and propose impactful projects to drive innovation Collaborate with game teams to understand their design goals and build strategies to discover new engaging experiences. Collaborate with a diverse range of partners, including research, engineering, and game development teams. Your Qualifications: Masters in Computer Science, mathematics or related field, with experience in research Experience with machine learning and familiarity with multiple ML techniques such as transformer models and diffusion models Technical background, experience working with both research engineering, and a proven track record of going from idea to implementation"
Updata,ML Specialist,"Key Responsibilities: Conduct exploratory data analysis (EDA) and visualize data using various libraries. Build, deploy, and monitor machine learning models.Implement computer vision and time series analysis techniques in projects.Implement and manage MLOps workflows and pipelines on cloud platforms.Utilize big data services and manage data pipelines.Collaborate with teams to integrate research code into industrial applications. Qualifications: Strong experience with Python, including advanced features and real-world application development.Proficiency in cloud platforms, including MLOps, data storage, serverless functions, and cloud security.Experience with tools and techniques such as Scikit-learn, OpenCV, PyTorch, and TensorFlow. Active participation in open-source contributions, workshops, or webinars is a plus. Excellent problem-solving skills and a keen eye for detail.Strong communication skills, both written and verbal."
Electronic Arts (EA),AI Data Scientist,"As a Data Scientist you will: Apply current and emerging techniques in deep learning, natural language processing and other machine learning areas.Collect, clean, manage, analyze and visualize large sets of data using multiple data platforms, tools and techniques.Work in partnership with game teams to integrate AI solutions into products and services. Optimize and fine-tune AI models for performance and scalability. Required Qualifications And Experience: Graduate degree in statistics, mathematics, computer science, or related field: Expertise in deep learning and fluency with machine learning frameworks and libraries (e.g. Scikit-Learn, PyTorch) Fluent in Python and good knowledge of SQLExposure to Cloud environments (e.g. GCP, AWS, or Azure) Exposure to devops tools and principles (Git, CI/CD, Docker) Excellent problem-solving skills and attention to detail. Ability to work collaboratively in a team environment Strong communication skills, both written and verbal"
Canonical,Python and Kubernetes Software Engineer - Data AI/ML & Analytics, "What your day will look like: Develop your understanding of the entire Linux stack, from kernel, networking, and storage, to the application layer Design, build and maintain solutions that will be deployed on public and private clouds and local workstations Master distributed systems concepts such as observability, identity, tracing Work with both Kubernetes and machine-oriented open source applications. Collaborate proactively with a distributed team of engineers, designers and product managers Debug issues and interact in public with upstream and Ubuntu communities Generate and discuss ideas, and collaborate on finding good solutions What we are looking for in you Professional or academic software delivery using Python Exceptional academic track record from both high school and university Undergraduate degree in a technical subject or a compelling narrative about your alternative chosen path Confidence to respectfully speak up, exchange feedback, and share ideas without hesitation Track record of going above-and-beyond expectations to achieve outstanding results Passion for technology evidenced by personal projects and initiatives The work ethic and confidence to shine alongside motivated colleagues Professional written and spoken English with excellent presentation skills Experience with Linux (Debian or Ubuntu preferred)  Excellent interpersonal skills, curiosity, flexibility, and accountability Appreciative of diversity, polite and effective in a multi-cultural, multi-national organisation Thoughtfulness and self-motivation  Result-oriented, with a personal drive to meet commitments  Hands-on experience with machine learning libraries, or tools. Proven track record of building highly automated machine learning solutions for the cloud. Experience with container technologies (Docker, LXD, Kubernetes, etc.) Experience with public clouds (AWS, Azure, Google Cloud) Working knowledge of cloud computing Passionate about software quality and testing Experience working on an open source project"
Primate Labs Inc., Machine Learning Software Developer, "Responsibilities As a Machine Learning Software Developer, your primary responsibility will be to investigate and develop machine learning benchmarks. The benchmarks include both computer vision AI models (e.g., Object Detection, Image Segmentation) and generative AI models (e.g., LLMs such as LLaMA, text-to-image models such as Stable Diffusion). The benchmark tests will target high-end mobile devices and desktop systems. You will also be responsible for communicating experimental results from the benchmark tests both internally to project stakeholders and externally to representatives from hardware companies. Qualifications The ideal candidate has the following background and skills: Experience with C++ and Python. Experience with at least one machine learning toolkit (e.g., TensorFlow, PyTorch). An understanding of neural networks and how they work. Strong written and oral communication skills. Able to work independently with minimal supervision. Able to conduct research and problem solve independently The following skills are nice to have but are not necessary: An understanding of generative AI models (e.g., LLMs, Stable Diffusion) Experience working with a variety of operating systems (e.g., Linux, Windows, macOS). Experience training deep learning models."
ThinkCX, Junior Data Engineer 2024, "Responsibilities: Mine large, complex sets of data sets for insights on customer behavior. Work with product and sales team to design meaningful client focused solutions. Work with data analytics team to engineer feature sets. Explore vast data sets for new device insight sources or product applications. Discover new uses to improve key model metrics. Optimize existing production models. Minimum Qualifications: Bachelor’s degree in a quantitative discipline (e.g., statistics, economics, computer science, data science, mathematics, physics, electrical engineering, etc.). Demonstrated effective written and verbal communication skills. Proven self starter and attention to detail. Applied experience with SQL and languages/software such as (Python, R, pandas, MATLAB etc.). Preferred Skills: Can translate data analysis into business recommendations Experience with cloud services like AWS, Azure, Google Cloud Applied experience with machine learning on large datasets Familiarity with data warehousing platforms like Amazon Redshift, Azure Data Warehouse Love solving problems and have analytical skills to boot Have a cool side project or a GitHub/Kaggle link? We're keen to see it!"
Homebase, Applied AI Scientist, "You will make an impact by Drive advancements in LLMs and other AI technologies, navigating a rapidly evolving landscape over the coming years. Leverage pre-existing models, train or fine-tune new ones, and oversee their deployment, scaling, inferencing, and monitoring. Tackle critical business problems across diverse domains with innovative AI solutions. Develop metrics to evaluate the end-user impact of ML models and ensure their alignment with business goals. Effectively communicate data science experiment results to non-technical stakeholders. Collaborate with data, platform, and product engineering teams to productionize models for real-time, batch, or streaming applications. Support Product, Engineering, and leadership teams with data-driven decision-making.You are a bar raiser, which means you come with A strong foundational understanding of ML principles. Experience working with LLMs in industry or academia. Adaptability to learn and evolve as newer AI models emerge. Proficiency in SQL and Python for data manipulation and model development. The ability to break down complex business problems into manageable components. Exceptional problem-solving skills and meticulous attention to detail. Strong communication and collaboration skills to work effectively with cross-functional teams."
CGI,AI Engineering Specialist,"Your future duties and responsibilities Design, implement, and optimize machine learning algorithms and models, including supervised, unsupervised, and reinforcement learning techniques. Collect, clean, preprocess, and transform large datasets from various sources to ensure they are suitable for model training. Train AI models on large datasets, optimizing hyperparameters to enhance model accuracy and performance. Deploy trained AI models into production environments, ensuring they are scalable, reliable, and maintainable. Continuously monitor the performance of deployed models, retraining and updating them as necessary to adapt to new data and conditions. Develop and maintain the necessary infrastructure, including data pipelines, model training frameworks, and deployment platforms, to support AI development. Work closely with software engineers and IT teams to integrate AI solutions with existing systems and applications. Collaborate with cross-functional teams, including data scientists, data engineers, product managers, and business stakeholders, to align AI projects with business objectives. Stay informed about the latest advancements in AI and machine learning, experimenting with new tools, techniques, and technologies to drive innovation. Implement and adhere to ethical guidelines and principles, ensuring AI solutions are fair, transparent, and accountable, and mitigating biases in models."
CGI, Machine Learning Engineering Specialist, "Your future duties and responsibilities Develops and implements machine learning models and algorithms tailored to specific use cases and business objectives. Collects, preprocesses, and cleanses large volumes of structured and unstructured data from various sources, ensuring data quality and integrity. Engineers relevant features from raw data to improve model performance and accuracy, employing techniques such as feature scaling, transformation, and selection. Trains machine learning models using appropriate algorithms and techniques, optimizing hyperparameters to achieve desired performance metrics. Evaluates model performance using metrics such as accuracy, precision, recall, F1 score, and ROC curves, iteratively refining models based on feedback and experimentation. Deploys trained machine learning models into production environments, ensuring scalability, reliability, and efficiency, and integrating models with existing software systems and workflows. Collaborates with cross-functional teams, including data scientists, software engineers, business analysts, and domain experts, to align machine learning initiatives with business objectives. Visualizes and interprets model outputs and insights, communicating findings effectively to stakeholders through dashboards, reports, and presentations. Stays updated with the latest advancements in machine learning research and techniques, experimenting with new algorithms, libraries, and tools to drive innovation and improve model performance. Documents the development process, including methodologies, code, experiments, and results, to ensure reproducibility, knowledge transfer, and compliance with regulatory requirements and data privacy standards."
Capital One, Associate Data Scientist Full-Time 2025, "To give you an idea of what they do, take a look belo Using machine learning and other data mining techniques to get insights from massive datasets, such as identifying fraud rings Understanding what problems create consumer calls through data mining interaction records Analyzing online banking patterns Mining vast amounts of data to identify trends in credit risk Using a combination of quantitative analysis and statistical methods to develop strategic insights and recommendations to optimize products or programs Adapting the latest technologies in modelling and data mining to design scientific tests that optimize dollars spent on new initiatives and marketing channels Building statistical models to form the foundation of business decisions and regularly monitoring the performance of those models so corrective action can be taken What would be great for you to have: Pursuing a degree in Mathematics, Statistics, Computer Science, Engineering, or another quantitative discipline Experience analyzing and manipulating large data sets using tools such as SQL or Python through co-op or school programs Experience with statistical analysis and data mining using tools such as R, SAS, Matlab, Stata, or SPSS through co-op or school programs Experience building and maintaining data pipelines Experience in a Linux/Unix environment, with Git, AWS, or APIs through co-op or school programs is considered an asset Excellent verbal and written communication skills"
BGIS, Jr. Data Scientist - AI and Advanced Analytics, "Key Duties & Responsibilities Analysis and Modelling Utilize your educational background in a quantitative field, such as computer science, mathematics, or engineering, to analyze complex datasets and develop predictive models. Apply your expertise in analytics to solve real-world problems, using machine learning, artificial intelligence, linear programming, generative AI, NLP/Large Language Models (LLMs) or other data science techniques. Design, build, and maintain robust feature engineering, data pipelines and ETL processes, ensuring data accuracy and accessibility for analysis and modeling. Create optimization models for strategic resource allocation and operational efficiency. Best practices and Continuous Learning Showcase your aptitude for continuous learning by staying up to date with the latest advancements in data science and technology. Maintain a strong understanding of data warehousing, cloud technologies, and big data solutions, constantly updating and improving our data and analytics infrastructure. Solutions and Business Strategy Harness your passion for problem-solving to identify opportunities for improvement and drive data-driven decisions. Collaborate with cross-functional teams to ensure the successful implementation of data-driven solutions. Demonstrate strong communication skills, effectively translating technical insights into actionable business strategies. Communication Collaborate closely with data scientists and business analysts, providing them with clean, structured data outputs from your analytic work to facilitate business decision making. Knowledge & Skills Bachelor's degree or higher in a quantitative field (Computer Science, Mathematics, Engineering, etc.). Work experience in data analytics, machine learning, AI, or data science is a nice to have. Understanding of delivering actionable insights and solutions from complex data and analytic models. A natural curiosity and enthusiasm for tackling challenging problems. Excellent communication skills with the ability to convey technical concepts to non-technical concerned parties. Proficiency in data manipulation and programming languages such as Python, R, or SQL. Familiarity with data visualization tools and techniques [Power BI, Tableau, Python libraries). Exposure and growing familiarity with LLMs/Generative AI and potential applications."
HelloFresh, Data Engineer, "You will : Work together in a cross-disciplinary team of Data Scientists, Data Analysts, and Data Engineers to evolve our data pipeline, warehouse, and workflow systems to be more resilient, extensible, and maintainable Partner with the analytics and data science teams to provide the data and tools they need to solve complex problems Identify improvements in the flow of our data, including designing tests Work with new technologies (both self-managed and services) for moving, transforming, modeling, and storing data and make recommendations Build and maintain complex and scalable ETL pipelines Collaborate with various teams to design a data analytic solution and data model that meets the needs of the business You are: Collaborative: you can work hand in hand with stakeholders, data scientists, engineers, and other data professionals to identify needs and opportunities for improved data management and delivery. Sense of ownership: you take responsibility for your projects and pride in your work. Strategic & critical thinking: you get the smaller and the bigger picture. Creative critical thinker and problem solver with meticulous attention to detail Ability to function in a fast-paced environment and have the flexibility to change course based on business needs or environmental conditions At a minimum, you have: Bachelors in a STEM subject. E.g. Computer Science, Data Science, Math, etc At least two years two years experience designing and building highly scalable data pipelines using Airflow & Databricks At least two years of professional experience in Data Warehouse/Data Lake. Past experience in Marketing Analytics is a plus. Experience with Python, Spark, Databricks, DBT, Terraform, Snowflake, AWS (EMR, EC2, S3, etc.) Strong SQL, strong knowledge of databases, and experience in developing data pipelines. Solid knowledge of Python (packages: PySpark/ Pandas & Numpy). Experience with Snowflake & Databricks is a plus. Technical skills, including architecture/design, data modeling, and developing services An understanding of database design and ETL practices Excellent verbal and written communication skills with the ability to effectively advocate technical solutions to engineering teams and business audiences"
Ambyint, Data Scientist, "What you"ll do Combine engineering principles and domain knowledge with data science techniques to solve complex challenges in artificial lift optimization Develop, operationalize, and maintain advanced machine learning models that focus on increasing production efficiency and reducing downtime Analyze large, complex datasets from IoT sensors and industrial systems to identify anomalies and opportunities for operational improvements Communicate complex data insights and deliver technical presentations to internal teams, explaining model functionality, results and recommendations for improvement Collaborate as a part of a cross-functional team to understand business problems, identify requirements and create data-driven solutions Qualifications Bachelor's degree or higher in engineering discipline, Mechanical or Petroleum preferred Demonstrated experience with oil and gas production operations such as artificial lift design/installation/optimization, production surveillance, failure identification, or reservoir engineering Knowledge of data science and machine learning methods, ideally within industrial, IoT, or energy sectors Familiarity with SCADA data, IoT data pipelines, and industrial control systems Proficiency in Python with a solid understanding of machine learning libraries (e.g., scikit-learn, TensorFlow, PyTorch) Experience working with large-scale datasets, SQL, and distributed computing platforms Strong foundation in statistics, optimization, and predictive modeling Excellent problem-solving skills and the ability to work both independently and in a collaborative environment"
TD, Data Scientist I, "Job Functions Document Business Intelligence requirements for our Business partners; Document business rules and validate data mappings; Perform impact analysis Liaise with different teams within Platform and Technology or with other internal partners to ensure data availability and accuracy Collaborate with all members of the Agile team to achieve successful implementations Flexibility to support other team members Communicate business needs to the teams responsible to define the business rules and development, so that they can design and propose solutions. Support related teams in the investigation and design of business solutions; Collaborate in the selection of the best solutions; Present and validate with the business partners the alternative(s) of proposed solution(s). Manage stakeholder expectations Must be able to understand different operational systems: systems functioning of related frontend systems or other backend databases what they contain (understanding available data, investigating data, etc ...). Must show strong adaptability to discuss with different stakeholders in terms of expertise, training and hierarchy. Skills At least 2 years of relevant experience in Business Intelligence projects/Reporting. Good knowledge in data extraction and manipulation (Databricks, SQL or equivalent) Good communication: Bilingual (English and French). Good time management skills to support multiple initiatives of different complexity Demonstrate analytical skills and critical thinking Be quality conscious Can negotiate with business partners Work efficiently in a team setting. Assets Knowledge of the Insurance Industry Jira/Confluence, Agile methodology. Knowledge of Python, Tableau"
Movable Ink, Machine Learning Engineer, "Responsibilities: Generate insights into customer behavior and derive modeling ideas for improving our content recommender system Work with data engineers to define what additional customer data we might want to collect and help make it available in a format suitable for modeling purposes Create meaningful machine-learning features that improve our content recommender's performance measured through offline metrics and online a/b tests Build machine learning models and deploy them as part of our recommender system Qualifications: Master's degree or equivalent experience (2+ years) in a relevant field or industry Solid understanding of machine learning fundamentals High comfort level in Python or other programming language Familiarity with an ML stack such as typical scientific Python libraries (pandas, numpy, sklearn, xgboost) or deep learning frameworks (we use Pytorch) Familiarity with data analysis through SQL or a big-data processing framework such as Spark Ability to collaborate with technical partners you'll be working closely with other teams to determine requirements for your work and to make design decisions that affect our stack The idea of writing and deploying production code, and getting real-world feedback on your models excites you A desire to always be learning and contributing to a collaborative environment"
Spin Master, Data Scientist, "How will you create impact? Exploratory Data Analysis: Collect, clean, and preprocess large datasets from various sources, ensuring data quality and consistency. Perform EDA to uncover patterns, trends, and relationships within the data. Machine Learning: Design, develop, and implement machine learning models to address business challenges and improve decision-making processes by utilizing GCP's ML tools such as AutoML, BigQuery ML and TensorFlow to build scalable models. Continuously monitor and refine models to ensure accuracy and effectiveness. Deploy machine learning models on GCP using Vertex AI. Ensure models are optimized for performance, scalability, and cost-effectiveness. Experimentation and Testing: Design and execute A/B tests, experiments, and other statistical tests to measure the impact of different business strategies. Analyze experiment results and provide actionable recommendations to stakeholders. Collaboration and Communication: Work closely with data engineers, program manager, data visualization engineers and other stakeholders to understand business needs and translate them into technical requirements. Present results and findings to non-technical stakeholders in a clear and concise manner. Collaborate with data engineers to deploy and integrate machine learning models into production systems. Goolge Cloud Platform Utilization: Leverage GCP's suite of products, including BigQuery, Vertex AI, Cloud Storage and Power BI/Tableau to manage and analyze data. What are your skills and experience? Experience in data science, with a focus on using machine learning for higher accuracy and improved time to market on projects delivery. Bachelor's degree in data science and/or computer science and/or statistics and/or mathematics, or a related field. Experience in cloud platforms like Google or Azure or AWS. Technical Skills: Strong programming skills in Python, R, with experience in data science libraries such as pandas, scikit-learn, TensorFlow, or PyTorch. Proficiency in SQL for data querying and manipulation. Experience with data preprocessing, feature engineering, and model evaluation techniques. Experience with data visualization tools (e.g., Power BI, Tableau, Matplotlib) to create clear and impactful visualizations. Strong knowledge of statistical analysis, hypothesis testing, and machine learning techniques."
Loblaw Digital, Machine Learning Software Engineer, "What You'll Do Design, build, and maintain highly scalable, robust, and efficient cloud infrastructure using Google Cloud Platform (GCP) services, including Vertex AI, BigTable, BigQuery, and Cloud Composer. Develop automation and orchestration of ML pipelines, integrating data ingestion, feature engineering, training, and deployment processes. Collaborate with cross-functional teams to understand their needs and build solutions that improve platform usability, scalability, and the overall development experience. Optimize data processing pipelines and cloud resources to ensure low-latency, cost-effective operation. Implement monitoring, alerting, and failover strategies to ensure platform reliability. Stay updated with industry trends and best practices in cloud engineering, data engineering, and machine learning Does this sound like you? Customer-centric mindset: Passionate about delivering an exceptional experience for data scientists through a self-service platform, reducing friction in their workflows. Collaboration: Strong communication skills to work closely with cross-functional teams, including data scientists and engineers, to ensure platform features meet user needs and expectations. Problem-solving: Ability to identify and solve complex technical issues related to ML pipelines, cloud infrastructure, and scalability, ensuring an efficient and robust platform. Automation-first approach: Commitment to streamlining and automating processes for scalability and reliability, enabling data scientists to focus on experimentation and model development. Adaptability: Ability to quickly adjust to new technologies and evolving platform needs to keep the infrastructure cutting-edge and efficient. Ownership and initiative: Comfortable taking ownership of key platform components, driving innovation and improvements that benefit the platform's scalability and usability. Bachelor's or Master's degree in Computer Science, Engineering, or a related field. 2+ years of experience in software engineering with a focus on cloud infrastructure and/or data engineering. Hands-on experience with Google Cloud Platform services such as Vertex AI, BigTable, BigQuery, Cloud Composer, Cloud Storage, etc. Proficiency in one or more programming languages such as Python, Java, and SQL. Experience with orchestration tools such as Apache Airflow (Composer). Knowledge of CI/CD pipelines and DevOps tools for continuous integration and deployment. Familiarity with containerization and orchestration (Docker, Kubernetes). Strong problem-solving skills and attention to detail. Excellent communication skills and ability to work in a collaborative, fast-paced environment"
Medeloop, Machine Learning Engineer, "What You'll Do As a Machine Learning Engineer at Medeloop, you will be responsible for architecting AI-driven data pipelines, designing cutting-edge algorithms, and developing models to analyze complex healthcare datasets. You will collaborate closely with the product, engineering, and science teams to deliver machine-learning solutions that address critical business needs and drive innovation in healthcare. Key Responsibilities Design and build end-to-end AI solutions that manage and track large-scale healthcare datasets. Develop models and algorithms to analyze healthcare data, ensuring performance optimization. Work closely with customers to understand their requirements and deliver features that align with their needs. Stay up-to-date with industry trends and new technologies in machine learning and healthcare data management. Influence the direction and culture of Medeloop with your initiatives and feedback. Who You Are Strong foundation in theoretical machine learning principles and proficiency with machine learning frameworks such as PyTorch, Transformers, NumPy, and Pandas. Creative thinker with a passion for continuous learning and a deep curiosity to explore and understand machine learning research. Fast, results-driven engineer with a hands-on approach, eager to build and deliver solutions efficiently. Hands-on experience with cloud platforms (AWS, Azure, GCP, etc.), and experience working with large-scale healthcare datasets is a plus. Skilled with development tools and best practices, ready to challenge and overhaul existing features for superior solutions. Exceptional communicator, excited to wear multiple hats and work cross-functionally, adjusting swiftly to the shifting needs of the business while maintaining SOC2 and HIPAA compliance. Unafraid to challenge the status quo and driven by a motivation to innovate, build, and experiment with cutting-edge solutions."
Circle K, Data Scientist - Real Estate, "What You'll Do Develop advanced analytics and predictive models from design through implementation in the areas of real estate site selection forecasts and network optimization. Additional knowledge in areas including pricing and promotion, marketing, and merchandising would also be preferred Participate in the research of analytical methods to find or advance solutions to business problems Clearly and concisely explain complex analytical findings to non-analytical peers and business leaders Create analytic datasets in collaboration with data engineering teams that involves data querying, cleaning, transformation, and feature engineering. Define requirements and test cases for Data validation and monitoring Author programming code (e.g., SQL, Python, R, spark) to assemble and analyze data, following/establishing/enhancing organizational standards and coding practices including code management (i.e. Documentation, use ofGit Hub). Lead and participate in peer code reviews for QA/QC/standards compliance. Train/Mentor other team members, provide developmental support where necessary Follow defined project management processes. Actively participate in project plan development and agile ceremonies (development of backlog, sprint planning, daily standups, retros/reviews) and documentation. Knowledge, Skills And Other Qualifications Required Demonstrated knowledge of SQL, R and Python; Experience querying large datasets using SparkSQL or PySpark; Experience querying geospatial data preferred Experience with ML frameworks such as scikit-learn, Tensorflow, Keras, Pytorch, etc.; Exposure with software development practices, object-oriented principles and test automation; Exposure to version control systems such as Git - experience preferred Experience with cloud-based analytics environments (Azure, AWS or GCP); Experience applying operational research, statistical and machine learning techniques such as regression, time series forecasting, clustering, optimization, etc.; Exposure to agile methodologies using project planning and tracking management tools e.g., JIRA; experience preferred Strong problem-solving skills and ability to troubleshoot complex distributed systems; Strong interpersonal skills, including the ability to communicate the business benefits of analytics; Knowledge of commercial real estate preferred Availability to travel up to 10% of the time Hybrid schedule with a minimum three days in the office Education / Training Required Bachelor's or Masters degree required with a quantitative focus (Statistics, Business Analytics, Data Science, Math, Economics, etc.) Masters degree or Bachelors degree with 1+ year of experience in a data science/advanced analytics role. Experience with Geospatial analytics preferred."
Huawei Canada, Engineer - AI Infrastructure Software, "Responsibilities: Apply relevant AI infrastructure and software/hardware acceleration techniques to build & optimize our intelligent AI/ML systems that improve our products and experiences. Apply your distributed system experience to build & optimize the AI/ML infrastructure for scalability, performance and reliability. Set project technical goals and milestones, provide architecture and design of the AI system, and implement and benchmark the design. Collaborate with multiple teams to deliver the project. Apply in depth knowledge of how the AI/ML infrastructure interacts with the other systems around it. Work with other engineers / research scientists & improve the quality of engineering work in the broader team. Job requirements Bachelor's degree in Computer Science, Computer Engineering, relevant technical field, or equivalent practical experience. Specialized experience in one or more of the following machine learning/deep learning domains: Hardware accelerators architecture, GPU architecture, machine learning compilers, or ML systems, AI infrastructure, high performance computing, performance optimizations, or Machine learning frameworks (e.g. PyTorch), and SW/HW co-design. Experience developing AI-System infrastructure or AI algorithms in C/C++ or Python."
Huawei Canada, Machine Learning Researcher - LLM/RAG, "Responsibilities: Build the benchmark and testbed for advanced LLMs based on Transformer, Mamba. Build the testbed for retrieval-augmented generation (RAG) and optimize the efficiency of the database system. Work closely with the researchers, promptly implement and test the new algorithms and new architectures proposed by the researchers. Research and develop innovative DL architecture and algorithms for large language models (LLM). Contribute to the design, implementation, test, and maintenance of research and development frameworks. Job requirements What you'll bring to the team: Hold a Master or PhD (preferred) in computer science, software or electric engineering or related subjects. Deep understanding of fundamentals and state-of-the-art techniques in NLP and ML. Expertise in NLP using machine learning or deep learning. Strong coding skills mainly in Python, with a focused expertise in PyTorch. Excellent oral and written communication skills. You are curious, like to think outside the box and appreciate complex challenges."
Aspire Software, Data Analyst Engineer, "What your day will look like: Data Acquisition: Identify and payment datasets from various internal and external sources, ensuring data quality and compliance Data Cleansing: Cleanse and preprocess data to remove errors, inconsistencies, and duplicates, ensuring high data quality and reliability Data Transformation: Develop and implement data transformation pipelines to convert raw data into structured formats suitable for analysis and reporting frameworks Database Management: Manage data storage and retrieval systems, including databases, data warehouses, or data lakes Collaboration: Collaborate with data scientists, analysts, and other cross-functional teams to understand data requirements and deliver tailored solutions Documentation: Maintain clear and organized documentation of data pipelines, processes, and data sources Data Analytics: Generate insights, trends, and correlations from collected data Data Visualization: Generate dashboards and other interfaces to expose Analytics. Work with the development team to integrate these into merchant-facing products About You: Bachelor's or Master's degree in Data Science, Information Technology, Computer Science or a similar related field Proven experience in data engineering, data visualization, using languages and frameworks from Python, R or other similar technologies Proficiency in data manipulation and transformation using ETL tools Strong SQL skills for data querying and manipulation Strong Knowledge of data storage solutions like databases (SQL, NoSQL), data lakes, and data warehouses Understanding of the basics of payment processing, including knowledge of payment gateways, credit card processing, and e-commerce, is a plus Excellent problem-solving skills, attention to detail and ability to work independently Ability to work independently and manage multiple tasks simultaneously in a fast-paced, always-evolving environment Strong communication and teamwork skills Detail-oriented with strong organizational skills Fluent in English, both written and verbal, is essential"
Allstate Canada, Claims Intelligence Data Insights Analyst, "You will be accountable for supporting the achievement of ACG's short and long term Mission, Vision, and Strategic Objectives by: Report Re-Design Replace existing stakeholder reports with automated, interactive dashboards enabling powerful data exploration and proactive self-serve analytics Innovate and influence the design of new dashboards with a focus on simplicity and usability Optimize the data pipeline by improving existing scripts / data update processes Provision execution of reporting into data visualization tools by gathering requirements, performing data validation, UAT testing and communicating rollout of new dashboards Build on and improve our existing script and report documentation Stakeholder Initiative Data Support Consolidate analytical and data requirements on project initiatives Consult and provide input on current data libraries and reports to help foster the implementation/creation of new initiatives Determine the best way to collect and aggregate data to build a easily-maintainable data model / report to support the monitoring of stakeholder initiatives and support its success Provide analytical assistance to business partners in enhancing operational excellence and supporting sustainable profitable growth of the business with proactive analytics Ad-Hoc Analytics Respond to ad-hoc insights requests from our stakeholders by pulling the right data, presenting it in a simple and usable way to answer their questions and help them make decisions Simplify data and insights through best practices in data-storytelling and data visualization Qualifications Education or equivalent working experience: Post-Secondary Education, Year of Study 2nd year and above GPA: 3.0/4.0 and above (or equivalent) Skills/Compentencies Exceptional analytical and critical thinking skills Must be able to analyze complex data with high accuracy, be open minded towards change, anticipate client needs and work with others to identify and resolve issues Must be able to coordinate various priorities under time pressure and ensure alignment to corporate objectives and stakeholder priorities Strong communication (written and verbal), collaboration and relationship-building skills Demonstrates strong experience in using data to tell a story influencing business decisions and driving stakeholders to action Demonstrates competency and proficiency in data extraction and transformation, developing optimized data models for data visualization Technical Skills Experience using Power BI and/or Tableau Experience with SQL Extensive experience with Microsoft Suite with an emphasis on Excel and O365 Additional Preferred Skills Experience in the insurance industry, especially with claims data Post-Secondary degree in a quantitative study such as statistics, actuarial science, economics, computer science Experience with Tableau Prep, Python, or R Experience with SAS"
Berkeley Canada, Data Operations Analyst, "What will your typical day look like? Receiving sales data in Excel format from our distributors in a wide variety of shapes, sizes and levels of quality and consistency. Analysing, massaging and re-formatting said data to align with our database requirements. Adjusting and running SQL scripts to load sales data into our database. Adjusting and running SQL scripts to export data from our database into Excel-based reports. Working within a monthly financial close timetable. Identifying and actioning opportunities where diverse data sets and complex procedures could be aligned into fewer, simpler, more standardized forms. Documenting processes and workflows. Qualifications Characteristics of the successful candidate: A strong mind for tabular and relational data - data types, column layouts, relationships, CRUD operations. Intermediate-to-Advanced Excel skills - ability to sort, filter, pivot and re-shape Excel data with ease; ability to make effective use of formulas (text, arithmetic and conditional) and to quickly understand formulas written by others. Basic-to-intermediate SQL skills - ability to write basic SQL statements including joins, where clauses, inserts, updates, deletes; ability to understand scripts written by others and make minor adjustments to them to cater to changes in the underlying data. Superb consistency and attention to detail - ability to spot inaccuracies in one's own work and that of others, particularly in Excel and SQL. Excellent communication skills, particularly when issues arise that require escalation. A sense of urgency and a recognition of the importance of meeting financial reporting deadlines. Ability to document processes in a way that others can understand and follow, including the ability to communicate key contextual information that gives meaning to processes. A strong workflow mindset; ability to think about and document processes in terms of hand-offs and escalation points between different roles."
SynergisticIT, Junior Data Analyst/Engineer/Scientist - Remote, "For data Science/Machine learning Positions Required Skills Bachelors degree or Masters degree in Computer Science, Computer Engineering, Electrical Engineering, Information Systems, IT Project work on the technologies needed Highly motivated, self-learner, and technically inquisitive Experience in programming language Java and understanding of the software development life cycle Knowledge of Statistics, Gen AI, LLM, Python, Computer Vision, data visualization tools Excellent written and verbal communication skills Preferred skills: NLP, Text mining, Tableau, PowerBI, Databricks, Tensorflow"
J&M Group, Data Scientist / Machine Learning Specialist, "Key Responsibilities Independently work on end-to-end development of Machine Learning and Natural Language Processing models to derive insights from research publications, legal documents, regulatory requirements etc. Technical analysis and software development. Design and implement business solution in agile squads. Engage in Machine Learning project, which includes problem definition, Data Engineering, Machine Learning design and documentation for the model risk management and running all the needed Client tests to ensure reliability. Develop, maintain, and track detailed delivery plans. Strong Data analysis, processing, discovery skills. Skills Required Master's in mathematics, Statistics, Economics, Data Science, Machine Learning, Operational Research, Physics, and other related quantitative fields. At least 4 years of experience with design and implementation of machine learning, predictive analysis, data science, knowledge bases, recommendation systems, information retrieval. Strong understanding of the foundational concepts and applied experience in Machine Learning and model explain ability (ideally, a combination of excellent academic research and high-impact commercial projects). In depth understanding of common Machine Learning algorithms (e.g., for classification, regression, and clustering). Experience with LLM models and Open AI. In depth knowledge of advanced statistical theories, methodologies, and inference tools. Proven track record in some of the advanced topics such as Bayesian inference, hierarchical models, deep learning, Gaussian processes, and causal inference. Practical experience in preparing data for Machine Learning integrating with big-data platforms and high-performance computing ecosystems. Strong oral and written communication skills. Strong analytical and problem-solving skills. Skills Desired Modeling using vendor products such as: Microsoft AI Builder (Power Platform) and CO Pilot. Familiar with Azure and AWS framework Experience with non-English Natural Language Processing. Client Libraries H2O, Keras, Tensorflow Experience with Deep Learning"
Huawei Canada, Researcher - Embodied AI, "Responsibilities: Working closely with a team of experienced researchers to solve real world challenges in the domain of Embodied AI Developing state-of-the-art approaches for Embodied AI applications. Directions include, but not limited to, generative AI, representation learning, foundation models, reasoning, planning, data generation and augmentation, reinforcement learning, and low-level control. Translating mathematical problem definitions and model/solution specifications into efficient executable code Conducting evaluation and empirical studies using robotic platforms in both simulation and real-world Proposing high-impact intellectual properties (e.g., patents), and publishing or contributing to research papers in top-tier AI venues Job requirements What you'll bring to the team: M.Sc./ PhD degree in computer science or related fields (Exceptional candidates with Bachelor degree will be considered) Prior experience in sequence analysis and generative AI, in particular vision and language Proven research record in AI by having at least one paper as the first author in top tier venues, such as NeurIPS, ICML, ICLR, CVPR, ICCV, ECCV, ICRA Proficiency in Python programming language Experience with one or more mainstream deep learning frameworks e.g. PyTorch, TensorFlow Experience in robotics applications, in particular decision-making, planning and control, or experience with real robot and ROS is an asset Experience with using large-scale datasets and sensor data, or with use of various transformer architectures and diffusion models is an asset Experience with imitation and reinforcement learning algorithms is an asset"
Metaguest, Data Engineer, "As a Data Engineer at Metaguest, you will design and manage the data architecture that supports our AI-driven solutions for the hospitality industry. This is a hands-on role where you will create scalable data pipelines, optimize ETL processes, and ensure high data quality and reliability across the organization. You'll collaborate with cross-functional teams to enable data-driven decision-making and unlock new insights for our products. In this role, you will: Develop, test, and maintain efficient ETL processes to handle structured and unstructured data from various sources. Design and implement data storage solutions for performance, scalability, and reliability using cloud platforms like AWS, GCP, or Azure. Implement robust data validation, monitoring, and governance processes to maintain high levels of accuracy and consistency. Collaborate with Data Scientists, Machine Learning Engineers, and Product Teams to ensure data readiness for AI models and analytics tools. Work with APIs, relational databases, and big data technologies to consolidate data from Property Management Systems (PMS), CRM tools, and guest engagement platforms. Create systems that support real-time data processing for predictive analytics and personalized guest experiences. Enable business teams to access and analyze data through intuitive dashboards and visualizations. You'll thrive in this role if you: Have expertise in Python, SQL, and data pipeline tools like Apache Airflow or AWS Glue. Are experienced with big data technologies such as Spark, Hadoop, or Kafka. Have worked with cloud platforms like AWS (e.g., Redshift, S3), GCP, or Azure to implement scalable data systems. Understand the intricacies of ETL processes, database design, and data normalization. Are familiar with data lake architectures and implementing efficient data partitioning strategies. Have a strong grasp of API integrations, enabling seamless connectivity between systems. Understand the importance of data security and compliance, particularly for enterprise clients. Have a Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field (preferred)."
Validere, Data Consultant, "Let's Give You a Purpose Lead discovery with clients and internal stakeholders to uncover new business use cases and drive data strategy Work with clients to deliver end-to-end solutions leveraging Validere's software product and proprietary algorithms Develop and standardize best practices for data tooling and product configuration across disparate business use cases Configure and maintain automated data ETL pipelines to enable real-time insights What You'll Bring Along Growth mindset, passion for learning new things, restlessness First-principles thinking - you have the strong desire to understand any topic at a deeper level than most, and are unsatisfied with decisions based on convention or unjustified assumptions Ability to work in a fast-paced environment, formulate and test hypotheses quickly and efficiently Experience with Python and common data packages (NumPy, Pandas) Proficient communication and presentation skills Ability to work with disparate data sources (i.e SQL, CSV, APIs) and leverage a Python codebase Familiarity with leveraging standard API protocols and practices to query, manipulate and ingest data Bachelor's degree in engineering, business or another quantitative field (e.g. physics, math, statistics, etc.) Preferred: 1-2 years experience in a similar role (data analyst, data engineer, etc.) with a proven track record of taking ownership and driving projects to completion"
TD, Data Scientist I, "Job Functions Document Business Intelligence requirements for our Business partners; Document business rules and validate data mappings; Perform impact analysis Liaise with different teams within Platform and Technology or with other internal partners to ensure data availability and accuracy Collaborate with all members of the Agile team to achieve successful implementations Flexibility to support other team members Communicate business needs to the teams responsible to define the business rules and development, so that they can design and propose solutions. Support related teams in the investigation and design of business solutions; Collaborate in the selection of the best solutions; Present and validate with the business partners the alternative(s) of proposed solution(s). Manage stakeholder expectations Must be able to understand different operational systems: systems functioning of related frontend systems or other backend databases what they contain (understanding available data, investigating data, etc ...). Must show strong adaptability to discuss with different stakeholders in terms of expertise, training and hierarchy. Skills At least 2 years of relevant experience in Business Intelligence projects/Reporting. Good knowledge in data extraction and manipulation (Databricks, SQL or equivalent) Good communication: Bilingual (English and French). Good time management skills to support multiple initiatives of different complexity Demonstrate analytical skills and critical thinking Be quality conscious Can negotiate with business partners Work efficiently in a team setting. Assets Knowledge of the Insurance Industry Jira/Confluence, Agile methodology. Knowledge of Python, Tableau"
Advanced Tower Structural Solutions (ATSS), Specialist Data Analytics, "Key Responsibilities Analyze and manage large datasets to extract meaningful insights Develop and sustain reports that effectively convey data findings Create data visualizations that facilitate client decision-making Collaborate with clients to ascertain their data requirements and goals Ensure the accuracy and integrity of data throughout the analytical process Required Skills Strong analytical and problem-solving abilities Proficiency in data visualization tools such as Tableau or Power BI Experience with data manipulation and querying languages, including SQL Familiarity with statistical analysis methodologies Excellent communication skills for clear presentation of findings"
Gore Mutual Insurance, Data Engineer, "What will you be doing in this role? Developing Systems for Collecting and Storing Data Build data pipelines and technical components Automating Data Management Processes and Handling Data Security Automated data management processes. Follow security protocols and best practices to protect against potential security threats. Testing and Optimizing Data Pipelines Optimize data pipelines to ensure efficient data flow. Ensure that the data extracted from sources is accurate, complete, and usable. This might involve checking for missing values, inconsistent formats, or anomalies that could indicate errors. Test the efficiency and speed of data pipelines and databases. This can help identify bottlenecks and optimize performance. Verify that different components of the data infrastructure work together as expected. This includes checking that data flows correctly from sources to databases, and from databases to applications. Maintenance of Data Pipelines Regularly check the health and performance of the data processes. Identify and resolve issues including debugging code, optimizing queries, or adjusting configurations. What will you need to succeed in this role? Bachelor's or Master's degree in Computer Science, Data Engineering, Software Engineering or a related field. A minimum of 1-3 years of relevant experience as a data engineer is required. This includes experience in data engineering, data system development, or related roles. Good understanding of data structures, data modeling, and SQL. Exposure to Cloud based Services, preferably Microsoft Azure. Understanding with software design patterns and test-driven development (TDD) Proficiency in one programming language, preferably Python, including a strong grasp of Object Oriented and Functional programming paradigms. Exposure to Apache Spark concepts and distributed systems, including data transformations, RDDs, DataFrames, and Spark SQL. Strong problem-solving and critical-thinking abilities. Strong communication and collaboration skills."
Oxford Properties Group, Data Engineer, "Key Responsibilities ETL Processes: Develop robust ETL processes to extract, transform, and load data from various sources into our systems, ensuring data accuracy and integrity. SQL Database Management: Maintain and optimize a SQL database containing a cost model, ensuring data availability, security, and performance. Code Reviews: Conduct thorough code reviews to ensure coding standards, best practices, and quality are upheld across the development team. Development Process Improvement: Collaborate with the data scientist and development team to establish and enhance development processes, including version control, unit testing, and continuous integration/delivery. Release Management: Oversee the release management process, coordinating with cross-functional teams to ensure smooth and timely deployment of software solutions. Data Science Support: Collaborate with the data scientist to implement development best practices, set up unit testing frameworks, and provide technical support for data science initiatives. Dashboard Development: Create visually appealing and insightful dashboards to enable data-driven decision-making within the organization. Power Apps Development: Design, develop, and maintain Power Apps solutions that streamline business processes and enhance user experiences. Technical Documentation: Document technical designs, procedures, and guidelines to facilitate knowledge sharing and onboarding. Qualifications And Skills Bachelor's degree in Computer Science, Engineering, or related field. 1 - 3 years of experience in software development, with a focus on ETL processes, SQL database management, and Power Apps. Strong SQL skills and experience in database design, optimization, and maintenance. Solid understanding of ETL principles and practices. Experience with version control systems (e.g. Git) and CI/CD pipelines. Familiarity with software development methodologies and best practices. Proficiency in Microsoft Power Apps development. Excellent problem-solving and analytical skills. Strong communication and collaboration abilities. Attention to detail and a commitment to producing high-quality work. Experience in dashboard development and data visualization tools is a plus. Previous exposure to supporting data science initiatives is an advantage."
W3Global, Data Engineer, "We are looking for a skilled Data Engineer and ETL Developer to design and manage data pipelines, ETL processes, and cloud-based data solutions. You will work with cutting-edge tools and platforms to ensure efficient data integration and processing Key Responsibilities: Develop and maintain ETL workflows using Ascend.io / Informatica / IICS. Automate and strong python knowledge to build the pipeline with Airflow Composer. Build scalable data pipelines on Google Cloud Platform (GCP). Optimize data storage and querying in BigQuery / DBMS. Write and optimize SQL queries for data extraction and analysis. Use Python to automate and streamline data processes. Collaborate with teams to meet data requirements and troubleshoot issues. Skills & Qualifications: Proficiency with Ascend.io, Informatica, Airflow Composer, and GCP services. Strong SQL and Python programming skills. Experience in data pipeline design and cloud data management, particularly in BigQuery. Solid problem-solving skills and ability to work collaboratively."
ServiceNow, Machine Learning Developer, "Build the best cloud-based AI/ML solutions to power intelligent enterprise services Collaborate daily with a team of like-minded developers, product managers and quality engineers to produce quality software Work with product owners to understand detailed requirements and own your code from design, implementation, testing and delivery of high-quality solutions to our users Qualifications To be successful in this role you have: Expertise in Java or Python, OOP, Design Patterns, time and space-efficient algorithms Experience building new products that use challenging algorithms Expertise in coding efficient, object-oriented, modularized and quality software Knowledge of core AI/ML techniques and algorithms Knowledge of unit testing, profiling, and code tuning"
Neighbourly Pharmacy, Data Engineer, "Responsibilities Design, build, and maintain a scalable, secure data infrastructure that supports cross-functional data needs across departments, ensuring efficient data pipelines and reliable access to clean data for analytics and decision-making purposes. Implement and uphold data governance and quality standards across the organization, creating automated processes for data extraction, transformation, and loading (ETL) to reduce manual workload, support compliance, and enhance data-driven insights Analyze existing data processes and workflows to identify areas for improvement. Collaborate with Finance and IT professionals to design and implement process enhancements. Develop and maintain documentation for data processes, including data flow diagrams and process descriptions. Monitor data processes to ensure compliance with data governance and quality standards. Assist in the development and execution of data quality checks and controls. Provide training and support to end-users on new data processes and tools. Participate in cross-functional projects to support data-driven decision-making across the organization. Stay informed on the latest industry trends and technologies in data process management. Qualifications & Skills Bachelor's degree in Information Technology, Data Science, Computer Science, or a related field. Proven experience in data analysis, process improvement, or a related role. Strong understanding of data management principles and best practices. Proficiency in data analysis tools and software, such as SQL, Excel, Python and process mapping tools. Excellent analytical and problem-solving skills. Strong communication and collaboration abilities. Detail-oriented with a focus on accuracy and quality."
NTT Data Inc., Junior Data Scientist and ML Engineer, "Key Responsibilities: Proactively supports the design, development, and programing methods, processes, and systems to consolidate and analyze unstructured, diverse big data sources to generate actionable insights and solutions for client services and product enhancement. Receives instructions to research, design, implement and deploy scalable data analytics vision and machine learning solutions to challenge business issues. Contributes to the design and enhancement of data collection procedures to include information that is relevant for building analytic systems. Ensures that data used for analysis is processed, cleaned and, integrally verified and build algorithms necessary to find meaningful answers. Contributes to the designing and coding of software programs, algorithms, and automated processes to cleanse, integrates and evaluates large datasets from multiple disparate sources. Takes direction from management/leadership to provide meaningful insights from large data and metadata sources; interprets and communicates insights and findings from analysis and experiments to product, service, and business managers. Supports the design of scalable and highly available applications leveraging the latest tools and technologies. Participates in creatively visualizing and effectively communicating results of data analysis, insights, and ideas in a variety of formats to key decision-makers within the business. Receives instructions to create SQL queries for the analysis of data and visualize the output of the models. Contributes to ensuring that industry standards best practices are applied to development activities. Knowledge and Attributes: Developing in data modelling, statistical methods and machine learning techniques. Ability to thrive in a dynamic, fast-paced environment. Quantitative and qualitative analysis skills. Desire to acquire more knowledge to keep up to speed with the ever-evolving field of data science. Curiosity to sift through data to find answers and more insights. Developing understanding of the information technology industry within a matrixed organization and the typical business problems such organizations face. Ability to translate technical findings clearly and fluently to non-technical team business stakeholders to enable informed decision-making. Developing ability to create a storyline around the data to make it easy to interpret and understand. Self-driven and able to work independently yet acts as a team player. Developing ability to apply data science principles through a business lens. Academic Qualifications and Certifications: Bachelor's degree or equivalent in Data Science, Business Analytics, Mathematics, Economics, Engineering, Computer Science or a related field. Relevant programming certification preferred. Agile certification preferred. Required Experience: Moderate level experience in a data science position in a corporate environment and/or related industry. Moderate level experience in statistical modelling and data modelling, machine learning, data mining, unstructured data analytics, natural language processing. Familiarity with programming languages (R, Python, etc.). Moderate level experience working with and creating data architectures. Moderate level experience with extracting, cleaning, and transforming data and working with data owners to understand the data. Familiarity with visualizing and/or presenting data for stakeholder use and reuse across the business."
Electric Mind, Data Engineer, "While working with teams in an Agile environment, you will be: Designing high quality data pipelines/architectures that are highly scalable and extensible Providing guidance on data modeling, analysis, visualization, and implementation of data solutions Estimating, tasking, and prototyping Collaborating with cross-functional team members on software features, design and implementation Participating in the end-to-end delivery of data consulting projects, ensuring completion on time, on budget, and meeting or exceeding client expectations Investigating, learning and applying new technologies and ?processes Clarifying requirements with team and client representatives 'Must Have' Skills/Experience Snowflake, Databricks, or Apache Spark experience Design and implementation experience with Azure Cloud Data Services and/or AWS Cloud Data Services Design and implementation experience with Master Data Management, especially customer and reference data for financial services Proficiency in SQL In-depth knowledge of data management, analytics, business intelligence, and related technologies Growth mindset and desire to learn Bachelor's Degree in: Data Science, Business Analytics or Computer Science (or equivalent education/experience) 'Nice To Have' Skills/Experience PowerBI or Tableau or data visualization experience Talend, Apache Airflow, AWS Glue, and Azure Data Factor experience Proficiency in Python or R Experience in data modeling or schema design Experience with Kafka or other data streaming tools Financial Services industry experience Sales, pre-sales, or consulting experience Exposure to and enthusiasm for Agile approaches Master's Degree in: Data Science, Business Analytics or Computer Science (or equivalent education/experience)"
J&M Group, Data Engineer, "Design, develop, and maintain data processing pipelines using Apache Spark. Collaborate with data engineers, data scientists, and business analysts to understand data requirements and deliver solutions that meet business needs. Write efficient Spark code to process, transform, and analyze large datasets. Optimize Spark jobs for performance, scalability, and resource utilization. Integrate Hadoop, Hive, Spring, Hibernate, Kafka, and ETL processes into Spark applications. Troubleshoot and resolve issues related to data pipelines and Spark applications. Monitor and manage Spark clusters to ensure high availability and reliability. Implement data quality and validation processes to ensure accuracy and consistency of data. Stay up-to-date with industry trends and best practices related to Spark, big data technologies, Python, and AWS services. Document technical designs, processes, and procedures related to Spark development. Provide technical guidance and mentorship to junior developers on Spark-related projects."
AXIS (AXIS Capital), Junior AI Engineer, "Collaborating with the Lead Data Scientist, data scientists, Innovation & Analytics Leads to support the design, development, and deployment of AI-driven models and solutions. Leading the deployment and infrastructure activities relating to AI-driven models and solutions. Assisting in gathering, processing, and analyzing large datasets to extract insights for machine learning and AI applications. Developing and optimizing machine learning models and Generative AI systems for real-world business applications. Deploying AI models and solutions to production environments, ensuring seamless integration with existing systems and monitoring performance. Building and maintaining scalable AI solutions, ensuring efficient model deployment and integration into business workflows. Supporting the end-to-end lifecycle of AI solutions, from data engineering to model training, evaluation, and deployment in production environments. Contributing to improving AI model performance and scalability through rigorous testing, monitoring, and optimization. Collaborating with stakeholders to ensure AI solutions align with business objectives and comply with regulatory requirements. Staying current with the latest trends and advancements in AI, ML, and GenAI technologies."
Bitstrapped, Machine Learning Engineer, "Responsibilities Develop, train, and deploy machine learning models on Google Cloud Platform (GCP), leveraging tools such as VertexAI, GenApp Builder, PalmApi and BigQueryML. Collaborate with cross-functional teams to gather requirements, define project goals, and design scalable machine learning architectures. Conduct data preprocessing, feature engineering, and model evaluation to ensure high-quality and reliable models. Implement and optimize machine learning algorithms and pipelines to handle large-scale text-based datasets. Explore and experiment with generative AI techniques, including large language models, to solve complex text-related problems. Monitor and maintain deployed models, ensuring performance, scalability, and reliability in production environments. Stay up-to-date with the latest advancements in machine learning, generative AI, and text-based models, and proactively propose innovative solutions to enhance existing systems. Requirements Bachelor's or advanced degree in Computer Science, Engineering, or a related field. Strong experience in machine learning engineering, with a focus on developing and deploying models in production environments. Proficiency in using Google Cloud Platform (GCP) tools and services for machine learning, such as Vertex AI, BigQuery, and TensorFlow. Solid understanding of deep learning architectures, natural language processing (NLP), and generative AI models, particularly in the text domain. Proficiency in Python and experience with relevant libraries and frameworks, such as TensorFlow, PyTorch, or Keras. Experience with data preprocessing, feature engineering, and model evaluation techniques for text-based datasets. Familiarity with MLOps practices, version control, and CI/CD pipelines for machine learning. Strong problem-solving skills and the ability to work on complex projects independently or collaboratively. Excellent communication skills, with the ability to convey complex technical concepts to both technical and non-technical stakeholders. Preferred Qualifications Experience with large language models, such as GPT-3, BERT, or Transformer-based models. Familiarity with cloud-based machine learning services and technologies, beyond Google Cloud Platform. Knowledge of scalable distributed computing frameworks, such as Apache Spark or Hadoop. Contributions to open-source machine learning projects or research publications in the field."
ATS Software, Machine Learning Engineer, "Development of Advanced ML Models: Take part in the development and training of advanced machine learning models, with a focus on large language models (LLMs) and optical character recognition (OCR) across various data modalities. Utilize Python and Pandas to make significant contributions to our interdisciplinary projects.   Optimization and MLOps Application: Assist in refining our codebase and adopting MLOps practices to enhance the machine learning lifecycle across modalities, from data preparation to deployment and monitoring. Develop and implement strategies to manage the machine learning lifecycle for seamless integration of AI and ML models within the data warehouse.   Interdisciplinary Model Integration: Collaborate with data engineers and data scientists to ensure data quality and model accuracy. text and vision, into our application ecosystem. Focus on enhancing performance and scalability under the guidance of senior leadership. Required Qualifications:   Deep Learning Expertise Across Modalities: Strong knowledge and experience with deep learning algorithms such as RCNN, LSTM, and Transformers, particularly applied to various modalities like text and vision.   Experience with Interdisciplinary CV and NLP Models: Demonstrated experience working with machine learning models in computer vision and NLP, showcasing the ability to tackle complex, interdisciplinary problems through innovative ML solutions.   MLOps Platforms Proficiency: Familiarity with MLOps platforms such as AWS SageMaker, Databricks, and Weights & Biases, with a competency in leveraging these tools to streamline the ML lifecycle across various data modalities.   Coding Proficiency and Best Practices: Experience in writing clean, efficient code following Object-Oriented Programming (OOP) principles and coding best practices, with a proven ability to navigate and contribute to complex codebases.   Proficiency in Python, SQL, and relevant data warehouse technologies (e.g., Snowflake, Redshift, BigQuery)."
Advanced Tower Structural Solutions (ATSS), Data Specialist, "Your expertise will be utilized in managing extensive datasets, uncovering trends, generating reports, and crafting compelling data visualizations. Key Responsibilities Analyze expansive datasets to extract meaningful trends Generate detailed reports tailored for client needs Create visually engaging data representations to clarify findings Collaborate effectively with team members to refine data-centric strategies Required Skills Proficient in data analysis tools and software, including Excel, SQL, and Python Strong analytical abilities and attention to detail Skilled in developing data visualizations and comprehensive reports Excellent problem-solving capabilities Strong written and verbal communication skills"
Advanced Tower Structural Solutions (ATSS), Data Specialist, "Your expertise will be utilized in managing extensive datasets, uncovering trends, generating reports, and crafting compelling data visualizations. Key Responsibilities Analyze expansive datasets to extract meaningful trends Generate detailed reports tailored for client needs Create visually engaging data representations to clarify findings Collaborate effectively with team members to refine data-centric strategies Required Skills Proficient in data analysis tools and software, including Excel, SQL, and Python Strong analytical abilities and attention to detail Skilled in developing data visualizations and comprehensive reports Excellent problem-solving capabilities Strong written and verbal communication skills"
Animo Tech, Data Analyst, "Responsibilities: Analyze Market Data: Evaluate market trends and conditions, interpreting large datasets to extract actionable insights. Analyze Sales Data: Perform in-depth analysis of sales patterns to identify opportunities for growth, while managing downside risks. Market Strategy Recommendations: Based on data analysis, propose strategies to optimize market positioning and enhance business performance. Hypothesis Testing: Conduct hypothesis tests using natural experiments to validate findings and improve decision-making. Statistical Modeling: Develop and apply statistical models to predict future sales trends and forecast outcomes. Active participation: Engage with Subject Matter Experts (SMEs) and stakeholders through interviews and field research to refine models, ensuring they align with business needs and identifying factors outside statistical scopes. Qualifications: A bachelor's degree in data analytics, computer science, statistics, information systems, or a STEM discipline with demonstrated knowledge in statistics. A high level of Python proficiency: Strong background in Python programming, particularly for data analysis and modeling. Strong experience in commonly used data libraries like Pandas, Scikit-learn (sklearn), and NumPy, etc."
CGI, Salesforce Data Analyst, "Attributes that define our ideal candidates will include: Hands on experience with Salesforce core product schemas, Salesforce Reports and Dashboards, CRM Analytics, Tableau, Salesforce Einstein. Hands-on experience writing complex SQL queries and working with relational databases such as Oracle, DB2, or Microsoft, SQL Server Hands-on experience building reports outside of Salesforce platform with PowerBI, Tableau, or other reporting and analytical tools Analytical curiosity, strong data-oriented skills, statistical programming or computer science background preferred Strong data management and data cleansing hands on experience, including managing large data volumes, performing ETL activities, and monitoring data quality Experience in a mature data governance environment, with practical exposure to data governance tools and processes at enterprise scale Experience working in highly secure environments, handling PII and PHI Knowledge of software development techniques and methodologies, with background in Agile environments Exceptional communication skills, with an ability to make advanced analytics concepts accessible and understandable to non-technical business users Highly detailed-oriented with exceptional organizational and follow-through skills, self-starter and able to execute without a lot of direction or oversight Ability to handle multiple priorities and deadlines Bachelor's degree in mathematics, business, statistics, economics, computer science or information systems (or equivalent combination of skill and experience) Your future duties and responsibilities: Solve some of the most challenging and impactful problems for healthcare, public sector, financial services, insurance and utilities by using artificial intelligence, machine learning, and natural language processing and big data Help create the platform, tools and APIs necessary to enable other teams to work with data Efficiently handle vast amounts of data from multiple sources and destinations, including relational and NoSQL databases as well as external systems, both in batch processing and real-time delivery Drive innovation into Analytics solutions and focus on humanizing enterprise software to achieve better customer experience and to enable data-driven business decisions Work on high priority initiatives using advanced analytics, predictive modeling and a variety of data sources to produce actionable business insights Provide guidance to a customer and project team with respect to technical feasibility, complexity, and level of effort required to deliver a solution Work closely with other team members to further develop metrics, KPIs, and insights that measure business performance improvements Assist in the development and delivery of pre and post sales POCs, presentations and proposals for client engagements"
Canadian Cancer Society, Data and Analytics (BI) Specialist, "What You'll Be Doing Dive into Data: Extract, clean, and transform large datasets from various sources, turning raw data into valuable insight. Uncover Trends: Analyze data to identify trends, patterns, and correlations, providing actionable insights that drive strategic decisions. Create Visual Masterpieces: Develop and maintain dynamic reports, dashboards, and visualizations using BI tools (e.g., Tableau, Power BI, Salesforce Reports). Ensure Data Excellence: Maintain data accuracy and integrity, ensuring that our decisions are always data-driven and reliable. Collaborate and Innovate: Work closely with stakeholders to develop and maintain data models, ETL processes, and data pipelines, fostering a culture of innovation. Predict the Future: Utilize predictive analytics and statistical models to support business planning and identify growth opportunities. Empower the Team: Train and support end users on BI tools, fostering a data-driven culture across the organization. QUALIFICATIONS: University or College degree in information technology or an equivalent combination of education and experience. Strong analytical skills with the ability to interpret complex data sets. Proficiency in BI tools such as Tableau, Power BI, and Salesforce reports. Strong organization skills and ability to manage multiple tasks. Experience with data modeling, ETL processes, and data pipeline development. Excellent communication and collaboration skills Creative and analytical thinker with strong problem-solving skills."
Bombardier, Data Engineering Analyst, "What are your contributions to the team? Develop or enhance current framework to ingest the data from the source and load into the Lakehouse Support development and maintenance activities on enterprise data platform (Data Warehouse, Lakehouse). Support the creation and maintenance of backend data solutions to be used by data analysts and data scientists across the enterprise. Support the creation and maintenance of data solutions (report, dashboard) for various areas of the business. Experience with troubleshooting and optimization of the current process Optimize data pipelines to meet business needs. Develop the code using Software Development Life Cycle (SDLC) best practices using DevOps Continuous Integration and Continuous Deployment (CICD) of data systems. Identify data quality issues and make recommendations for addressing root causes. How to thrive in this role? You hold a bachelor's degree in computer science, Statistics, Informatics, Information Systems or another quantitative field. You have some experience (internship or other) in a similar role. You have developed Power BI solutions which make use of DAX. You have experience working with API for developing simple applications. You have knowledge of Agile / SCRUM project delivery, DevOps and CICD practices. You can create analytics solutions using SQL on relational databases. You have knowledge of modern data services on Microsoft (Power BI, Azure Data Factory, Synapse Analytics, Azure Data Lake Storage and Databricks) You have good knowledge of Object-oriented/object function scripting languages: Python, Java, C++, Scala, etc. You have good people skills and are a team player."
Pattison Food Group, Data Analyst, "You will be responsible for: Collaborating with stakeholders to understand business questions, processes, and KPIs. Formulating testable hypotheses aligned with business questions and conducting statistical experiments in a live environment. Writing SQL scripts to automate data pipelines and workflows. Using Python scripts in a Jupyter Notebook environment for exploratory data analysis, data wrangling, and statistical testing. Developing and maintaining dashboards and visualizations in Power BI to convey insights and report on KPIs and test results. Learning cloud-based analytics environments and big data tools to build analytics workflows on large datasets. Discovering unique and creative insights to identify new opportunities for driving business value. Becoming an expert in the relevant datasets within your analytics domain. Supporting the Data & Analytics with key projects. You have: A degree in a STEM discipline with 2 to 3 years of relevant experience in data analytics or data science. Intermediate Python coding skills. Experience in other programming languages is a considered an asset. Intermediate understanding of SQL. Experience using experimental design principles such as A/B and multivariate testing and working with business stakeholders to formulate testable hypotheses. Experience with visualization and reporting tools such as Power BI or Tableau. Working knowledge of cloud environments such as Azure, GCP, or AWS. Experience with big data tools such as Spark and Databricks is a considered an asset. You are: A confident communicator, both verbally and written, with strong interpersonal skills and the ability to communicate a story with data and insights to non-technical stakeholders. A problem solver with strong analytical and critical thinking skills. Effective in task management and delivering high-quality work. Capable of multitasking and thriving in a fast-paced environment. Self-motivated with the ability to work independently and collaboratively."
Mueller Water Products, Data Analyst, "Key Responsibilities Manage key client sites using the EchoShore product line: an industry leading Internet of Things (IoT) leak detection system. Perform acoustic analysis and large-scale data processing for leak detection and pipe condition assessment and provide reports to water utilities (customers) to help them manage their water distribution networks. Communicate the status of data analysis to Project Managers. Identify data anomalies and significant variances and flag them to senior staff. Assisting with the development of automatic software tools to improve productivity and efficiency of data analysis process. You will work closely with various departments (IT, software, sales) to implement these tools, understand their data needs and provide data-driven insights. Analyze data and work with Senior Data Analysts and Software Engineers to adjust analysis software for best outcomes including defining acceptance criteria Develop processes and procedures for accurate and efficient data analysis Monitor process performance through KPIs and strive for continuous improvement and further development of the leak detection center Prepare and update Standard Operating Procedure documents as required Build and maintain relationships with external leak detection customers Prepare and distribute reports to external customers regarding their leak assessment results Work in compliance with the MWP Code of conduct and all Environmental, Health and Safety policies, procedures and regulations Position Requirements Bachelor's degree in quantitative fields such as Engineering, Statistics, Mathematics, Computer Science, or a related field. Previous experience in data analytics Strong analytical skills, including data mining and statistical analysis Proficiency in data analysis tools and software (e.g., SQL, Excel, R, Python). The ability to perform large scale data frame processing using Pandas and NumPy is a plus. Database skills: good command of SQL queries. Understand database technologies: relational database (RDBMS), NoSQL, document-oriented, etc. Good command of numerical methods and associated tools: Scipy, Matlab, Octave, or other. Personal skills: creative, perseverant, detail-oriented, self-starter. Well developed communication skills, both verbal and written, with the ability to present analysis results and data insights to technical and non-technical stakeholders. Online asynchronous communication is required. Critical Competencies: technical learning, time management, problem solving. Position Assets Experience with acoustic, seismic, and vibration analysis is a plus. Knowledge of signal processing techniques and time-series analysis to identify anomalies, trends and other data patterns. Experience with GIS processing software such as ArcGIS, Geocortex, DMTI Spatial Design and implement instrumentation for data collection and analysis. Conduct experiments to test hypotheses and validate data models. Analyze experimental results and provide actionable insights. Working experience in an Agile environment. Familiar with issue and bug tracking using JIRA Familiar with Data Visualization tools such as Matplotlib, Plotly, etc"
BC Housing, Data and Process Analyst, "EDUCATION & EXPERIENCE: Bachelor's degree in data management, Information Systems, Business Administration or similar from a recognized post secondary institution. Sound experience in Data Analysis, Data quality Assurance, and Business Process Analysis. Some experience in Data Governance, policies, and procedures. Or an equivalent combination of education, training, and experience acceptable to the employer. KNOWLEDGE, SKILLS AND ABILITIES: Good knowledge of database systems and data warehousing concepts, Good knowledge of data quality improvement process, data dictionaries and repositories. Good knowledge of organizational system, ERP, or process management system Some knowledge of data governance principles including data governance framework, data analytics, data design and practices. Some knowledge of technical and user manual documentation. Familiarity with data visualization tools (e.g., Power BI). Strong communication, report writing, presentation and interpersonal skills. Strong research, analytical and problem-solving skills. Ability to assist with data investigations and recommend updates. Attention to detail and ability to collaborate and work independently. Ability to adapt to evolving technologies and methodologies. Ability to facilitate training on data governance, policies, and procedures."
Electronic Arts (EA), Data Analyst - Quality Verification, "You will define the data analytics roadmap for some of EA's largest titles. You will use your technical skills to develop data products that improve decision-making across the Quality Verification organization. You will guide best practices in tools and visual technologies to help evolve the project dashboards to provide essential data for project monitoring. You will develop scripts and queries to import and manipulate data from multiple sources. You will help us promote a data-driven culture and support data governance practices to improve our organization's data maturity. The Next Great Data Analyst Needs 2+ years of professional analytical experience, using data to improve processes. Programming and Database experience with advanced SQL experience and efficient in Python. Knowledge of different data software analytics and visualization tools such as Looker and PowerBI. Able to develop data modelling and warehousing solutions. Able to plan and prioritize multiple concurrent projects. Able to foster working relationships with partners."
Services SFT, Data Analyst, "The role we are offering you: Perform analysis on large data set, extract insights and communicate various stakeholders with the objective of improving the overall business performance. Interact with customers around the world Develop and maintain data management tools to help with every-day decision planning Create tools and build dashboards to facilitate reporting with relevant KPIs for different stakeholders Collaborate with Digital Product Managers to explore new product ideas or enhancements Possibility to work on simulator related software Requirements We are looking for someone who: Bachelor's degree in computer science, Management Information Systems, or equivalent experience Proficiency creating SQL queries, Use of PowerBI or other data visualization tools Comfort cleaning and analyzing large data sets with appropriate data technologies (e.g., Python, Databricks, SAS, R) Experience in data warehouse design and implementation with an emphasis on data preparation Strong experience in analytical business decision-making, data analytics, and statistics Strong analytical skills with the ability to discover patterns in data and figure out puzzling data relationships"
Exadel, Data Scientist / Analyst, "Qualifications Bachelor's degree in Computer Science, Software Engineering or a relevant work experience. Strong SQL/data skills. Programming skills in Python or equivalent to implement ML Experience with financial and capital markets data. Solid understanding of statistical and machine learning techniques. Excellent problem solving and analytical skills. Strong communication and presentation skills. Ability to work independently and collaboratively in a fast-paced environment. Nice To Have Experience in big data technologies (e.g. Spark, AWS Glue, Redshift, etc.) Familiar with AWS data services (e.g. S3, RDS, Redshift, Glue, etc.) Responsibilities Data Acquisition and Cleansing - source and catalog data from multiple sources, cleanse, preprocess and validate data to ensure accuracy and consistency. Data Analysis and Modelling - Employ statistical and ML techniques to extract meaningful insights from complex datasets. Model Implementation and Validation - Collaborate with tech teams to deploy data feeds, models into production, ensuring scalability and reliability. Data Catalog - Maintain an Overall data catalog for the team."
Snaplii, Data Analyst - Informatics and Systems, "You Will: Develop and implement databases, data collection systems, data analytics, and other strategies that optimize statistical efficiency and quality. Interpret data, analyze results using statistical techniques, and provide ongoing reports. Identify, analyze, and interpret trends or patterns in complex data sets. Work closely with management to prioritize business and information needs. Locate and define new process improvement opportunities. Design and create data reports and reporting tools to help business executives in their decision-making process. Collaborate with engineering and product development teams to improve product and data systems. Ensure adherence to data privacy laws and policies. Required Qualifications and Skills: Bachelor's degree in Informatics, Computer Science, Statistics, or a related field. Proven working experience as a Data Analyst or Business Data Analyst. Technical expertise regarding data models, database design development, data mining, and segmentation techniques. Strong knowledge of and experience with reporting packages (Business Objects etc.), databases (SQL etc.), programming (XML, Javascript, or ETL frameworks). Knowledge of statistics and experience using statistical packages for analyzing datasets (Excel, SPSS, SAS, etc.). Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy. Adept at queries, report writing, and presenting findings. Excellent verbal and written communication skills. Preferred Skills: Experience with data visualization tools (Tableau, Power BI). Knowledge in machine learning and predictive modeling is a plus."
Orennia, Data Engineer, "What You'll Do Analyze, design, develop, test, review, document and troubleshoot data pipeline / ELT solutions against multiple structured and unstructured data sources. Support our team of analysts through developing requirements and delivering solutions. Develop code to scrape public websites for data and perform ELT processes. Maintain, monitor, and support production ELT processes and respond to error and emergency issues. Who You Are You have excellent knowledge and experience with Big Data concepts like data lakes, data warehouses, ELT strategies, and best practices. You have a strong understanding of relational and dimensional data modeling. You possess strong analytical and problem-solving skills. You have experience with DBT and SQL, and are proficent with Python. You have extensive experience with cloud-based data processing and warehousing technologies (Databricks, Snowflake, etc) You have experience with Lean and Agile development methodologies (such as Kanban or SCRUM). You are comfortable in entrepreneurial, self-starting, and fast-paced environment, working both independently and with our highly skilled teams. You have experience with other Big Data processing technologies and cloud services (AWS, GCP, Snowflake, Hive, Hadoop, MS SQL, etc.). You have experience with JIRA and similar organizational tools. You have experience building web-scraping tools against publicly available datasets (considered an asset). You have experience with GIS/geospatial data processing, integration, and analysis is (considered an asset). You have experience building or supporting data visualizations (considered an asset). Deep intellectual curiosity with a results-focused relentless pursuit of answers. Ability to work in a fast-paced start-up environment, embrace change and ambiguity."
Intelliswift Software, Data Engineer, "Key Responsibilities: Provide production support for data pipelines and applications using Python, ADF, Azure Databricks, and Kafka. Monitor, troubleshoot, and resolve issues in a timely manner to ensure system reliability and performance. Support Kstreams consumers and understanding and debugging Java and Scala code. Collaborate with development and operations teams to implement fixes and improvements. Maintain documentation of support activities and solutions. Ensure data quality and integrity across various data sources. Qualifications: Proficiency in Python. Experience with Azure Data Factory (ADF). Strong knowledge of Azure Databricks. Advanced SQL skills. Experience with Kafka. Ability to read and understand Java and Scala code. Nice-to-Have Skills: Experience with Spring Boot. Knowledge of Scala. Familiarity with UI development. Qualifications: Bachelor's degree in Computer Science, Engineering, or a related field. Experience in production support or a related role. Strong problem-solving skills and attention to detail. Excellent communication and teamwork abilities."
Scotiabank, Data Engineer Internship, "In this role, you will: Perform data exploration on different data sources and model training Productionalize/Operationalize advanced analytics models following the Bank standards. ModelOps plays a key role as a liaison with other teams (Security, Privacy, Data Office, Analytics, Technology, among others) Automate data pipelines and trigger actions/events to streamline business processes and models enablement Enable mechanisms for Data Scientist and stakeholders to monitor the performance of a Model Monitor the run time of the models, work closely with Data Scientist on continuous optimization Monitor data pipelines, liaise with vendors, and source systems groups in the event of failures/maintenance Migrate data pipelines and models to new analytics platforms Experiment & learn! Requirements Do you have the skills and requirements that will enable you to succeed in this role? - We'd love to work with you if: You have knowledge/experience in: Big Data Technologies (Hive/Beeline, HDFS, Sqoop, Spark) Cloud platforms - GCP, Azure or AWS MinIO Data Storage, Airflow, Trino DB2, SQL Server Python (PyHive, PySpark), R, SQL-Shell (Bash, Korn) & general Linux/Unix CI/CD tech stack (BitBucket, Jenkins, Artifactory)"
Autodesk, Data Developer, "Responsibilities Data Pipeline Construction: Design, build, and maintain scalable data pipelines to support continuous data flow and analytics Data Modeling: Develop and implement effective data models and schemas to ensure data integrity and accessibility Database Management: Oversee and optimize database systems (SQL, NoSQL) ensuring high performance and availability Data Integration: Integrate data from various sources, ensuring consistency, quality, and reliability ETL Processes: Design and manage Extract, Transform, Load (ETL) processes to move data between systems Data Governance: Implement and enforce data governance policies to ensure data security, privacy, and compliance Collaboration: Work closely with data scientists, analysts, and stakeholders to understand data requirements and deliver solutions Performance Tuning: Enhance and tune database performance for efficient processing and querying of large datasets Documentation: Maintain comprehensive documentation for data processes, models, and architecture Platform Mindset: Partner with internal platform team on the following tasks Minimum Qualifications Bachelor's degree in computer science, Information Technology, or a related field. A master's degree is a plus a Proficiency in programming languages such as Python, Java, or Scala Strong knowledge of SQL and experience with relational databases (e.g., MySQL, PostgreSQL) and NoSQL databases (e.g., MongoDB, Cassandra) Knowledge of big data technologies and frameworks (e.g., Kafka, Flink, Parquet, Iceberg, etc.) ETL Tools: Skilled in using ETL tools like Apache Airflow Experience with cloud services from AWS, Azure, or Google Cloud, including data-related services like AWS Glue, EMR, Redshift, and S3 Experience with data warehousing solutions like Snowflake, Redshift, or BigQuery Understanding of data modeling, data architecture, and ETL processes Experience with version control systems (e.g., Git) and continuous integration/continuous deployment pipelines Relevant certifications in big data technologies, cloud platforms, or database management are a plus Preferred Qualifications Experience in customer journey analytics and personalization Familiarity with machine learning concepts and collaboration with data science teams Knowledge of real-time data processing and streaming architectures Experience with containerization tools like Docker and orchestration platforms like Kubernetes"
Amazon, Data Engineer 1, "Basic Qualifications 1+ years of data engineering experience Experience with data modeling, warehousing and building ETL pipelines Experience with one or more query language (e.g., SQL, PL/SQL, DDL, MDX, HiveQL, SparkSQL, Scala) Experience with one or more scripting language (e.g., Python, KornShell) Preferred Qualifications Experience with big data technologies such as: Hadoop, Hive, Spark, EMR Experience with any ETL tool like, Informatica, ODI, SSIS, BODI, Datastage, etc."
Triunity Software Inc., Data Engineer, "Job Responsibilities: Create and maintain optimal data pipeline architecture Implement data products curated by our Chief Data Office, as well as custom data models for fit for use. Ensure data quality and integrity across various data sources and systems to ensure data accuracy, completeness, and reliability. Optimize data pipelines for performance and scalability. Provide technical support to promptly resolve escalated incidents/outages. Develop and document a detailed solution design, impart your subject matter expertise throughout life cycle. Take business, Enterprise Architecture, system performance and development standards requirements, then develop functional, technical and user interface designs for an application and/or system. Find ways to keep costs low, help come up with strategic solutions to support cost effectiveness and enhance stakeholder experience. Conduct code reviews to address quality, standards compliance, reusability and ease of maintenance, Operational Readiness Reviews, and support gating and review signoffs for solution design. Ensure design leverages existing reusable components, traces back to business requirements, and that new modules are designed with reusability in mind. Keep up to date with the latest industry trends and technologies related to data engineering. Skills and Experience Required: Required: 10+ years of relevant experience in a related field of job function. Experience with: Java, Spring, Experience with big data tools: Hadoop, HDFS, ADLS, ADF, Spark, Kafka, Databricks, Dremio etc. Experience with relational SQL and NoSQL databases, including Cassandra. Experience designing production grade, scalable applications and microservices. 5+ years of Capital Markets experience. Experience working on Agile Teams Desired: Experience in Python and/or Scala"
Compunnel Inc., Data Engineer, "8+ years of experience in Data Engineer with AWS, Glue, Lambda, SQL, Python, Dovps, Redshift. Must have working knowledge in designing and implementing data pipelines on any of the cloud providers (AWS is preferred). Must be able to work with large volumes of data coming from various sources. Perform data cleansing, data validation etc Hands on ETL developer who is good at python, SQL. AWS services like glue, glue crawlers, lambda, red shift, athena, s3, EC2, IAM, Monitoring and Logging mechanisms- AWS cloudwatch, setting up alerts. Deployment knowledge on cloud. Integrate CI/CD pipeline to build artifacts and deploy changed to higher Environments. Scheduling frame works Airflow, AWS Step functions Excellent Communication skills, should be able to work collaboratively with other teams"
Banting AI, Data Engineer, "Key Responsibilities Design, build, and maintain robust data pipelines for secure ingestion, transformation, and storage of EHR, imaging, and genomic data. Ensure all data processes comply with regulatory standards, including HIPAA and GDPR. Conduct data validation and quality assurance to maintain the integrity of ingested data. Collaborate with data scientists and machine learning engineers to optimize data flows and accessibility for model training and testing. Monitor and manage database performance and data storage solutions to support scalability. Requirements Bachelor's or Master's degree in Computer Science, Engineering, or a related field. 3+ years of experience in data engineering, with a focus on healthcare or clinical data preferred. Proficiency in SQL and experience with data storage and processing solutions (e.g., AWS, GCP, or Azure). Strong skills in ETL and data pipeline tools (e.g., Apache Airflow, Talend, dbt). Familiarity with data privacy and compliance standards (HIPAA, GDPR). Excellent problem-solving skills and ability to work collaboratively in a cross-functional team."
Viggle, Machine Learning Engineer, "Key Responsibilities: Train and deploy video generation models at scale. Design and implement data processing pipelines to drive model accuracy and efficiency. Finetune and optimize neural networks for real-time performance. Research and apply state-of-the-art algorithms. Implement and monitor deployment pipelines, ensure model reliability, scalability, and efficiency. Qualifications: 3+ years in machine learning, specifically in deploying and optimizing models in production environments; experience in consumer products is a plus. Strong experience with deep learning frameworks (e.g., TensorFlow, PyTorch). Proficient in video processing, computer vision, and generative models. Proficient in Python and familiar with efficient model deployment using Docker and Kubernetes. Proficient in large-scale data processing. Proficient in model optimization, including quantization, distillation, and latency reduction. Familiar with cloud platforms (e.g., AWS, GCP) and distributed computing frameworks. Strong communication and collaboration skills, with a passion for solving complex challenges and pushing the boundaries."
Tactable, Data Engineer, "Responsibilities: Work with proprietary tools and technologies including time series databases, job scheduling, cloud storage, containers/images, batch schedulers, and ETL tools Onboard and integrate new data sources Migrate existing data pipelines to new architectures Break down large tasks into manageable components and drive them to completion Lead from a technical perspective and support a team of data engineers with mentoring and guidance Design and maintain automation of workflows and processes to boost team efficiency and enforce standardization Write excellent documentation for yourself, your team, as well as our clients Required Core Skills: 5+ years of experience in software development Proficiency in Python or similar programming languages (TypeScript, Java, C#, etc.) Proficiency with data processing frameworks such as PySpark and Pandas Proficiency with data storage, including relational and non-relational databases Demonstrated ability to handle complex tasks and projects independently Strong problem-solving skills, including the ability to research and troubleshoot effectively Other Skills: Degree in Computer Science, Engineering, or equivalent industry experience Experience with data workflow management tools Strong communication and teamwork skills Strong time management skills and ability to manage multiple workstreams"
DML Capital Group Incorporated, Software/Data Engineer, "Key Responsibilities: Develop, maintain, and enhance ETL systems, primarily using Python. Manage and troubleshoot our Django + PostgreSQL + Celery-based applications. Deploy and manage applications in a Docker-based, on-premises environment. Collaborate with the team to deliver technical onboarding of clients, ensuring seamless integration and technical support. Write and maintain high-quality technical documentation, including user guides, process documentation, and internal engineering documents. Handle bug reports and work proactively with internal teams to ensure timely resolution. Provide technical insights and contribute to the improvement of engineering practices. Required Skills and Qualifications: 3-5 years of experience in software development or data engineering with a focus on Python-based ETL pipelines. Proficiency with Django, PostgreSQL, Celery, and Docker. Familiarity with managing and maintaining on-premises infrastructure. Strong problem-solving skills, with a proactive and collaborative mindset. Experience writing technical documentation and communicating complex technical details in an understandable manner. Ability to manage multiple tasks and priorities in a fast-paced environment. Excellent communication skills for client-facing interactions, including technical onboarding. Detail-oriented and able to debug, troubleshoot, and resolve technical issues effectively. Preferred Qualifications: Experience working in a hybrid or on-premises environment. Familiarity with DevOps practices and automation. Experience in handling technical client onboarding or customer-facing support in a technical capacity."
Blastwords Inc., BI (Business Intelligence) Data Engineer, "Key activities and deliverables: Design, build and improve ETL (extract, transform and load) pipelines in support of data aggregation and movement Implement processes to efficiently maintain data security Ensure data is ingestible and usable in reporting visualization tools such as Tableau, Looker, Meta, or PowerBI Qualified candidates will have: 3-5+ years of work experience in data science, engineering or analytics roles Bachelor degree or higher in Computer Science, Engineering or Math, or equivalent experience Experience with AWS (Amazon web services) offerings, including S3, EC2 and EMR Development ability in Python and an Object Oriented programming language Strong database experience, leveraging event-driven data Experience handling large data volumes Drive to thrive in a complex environment with a hands-on approach to problem solving Preferred candidates will have: Masters or PhD in relevant field or equivalent experience Practical experience with agile development practices and egoless programming Programming skill with Java and/or GoLang Experience with Hadoop Ecosystem including Spark, HDFS, Hive, Kafka, Sqoop and Hbase Experience with Luigi pipelines for moving data between HDFS (Hadoop Distributed File System) and RDBMS (Relational Database Management System) Experience in mobile or social analytics and mobile/social game industries (though interesting perspectives from other industries are also appreciated) Current knowledge on the latest technologies in data science, BI and/or data management"
Wisedocs, Machine Learning Engineer, "Responsibilities As a member of our Engineering team, your primary responsibilities will include: Designing and implementing machine learning models to analyse and interpret large datasets of medical and insurance documents, enhancing our platform's decision-making and efficiency. Developing robust, scalable backend services in Python, ensuring seamless integration with our AI/ML components and front-end systems. Collaborating with other technical stakeholders and leaders to actively work towards the design and implementation of systems. Ensuring the reliability and scalability of ML systems, implementing best practices in data engineering and model lifecycle management. Staying abreast of the latest developments in AI/ML technologies and Azure services, continually seeking ways to optimize our platform and workflows. Conducting rigorous testing and validation of ML models, ensuring accuracy, efficiency, and alignment with business objectives. Coordinating with cross-functional teams to understand requirements and translate them into effective AI solutions within our platform. Other duties and responsibilities will be assigned as projects develop, adjust and mature. Qualifications Have a minimum of 3 years of experience in software development, with a strong focus on Python programming. Possess professional working experience with LLMs, RAG, transformers, BERT. Having demonstrated a solid foundation in data structures, algorithms, and software engineering principles, with a track record of developing high-quality, maintainable code. Should be familiar with recent model developments and frameworks, i.e. LlaMa, Mistral, GLINER, TGI, etc. Proven Machine Learning industry experience, preferably at scale. Prior professional experience with LlamaIndex is preferred. Prior experience with transfer learning and model modifications. Excellent problem-solving skills, while exhibiting high enthusiasm to tackle complex challenges in AI/ML development and deployment. Are enthusiastic about working in a fast-paced, innovative environment, contributing to a team that aims to make a significant impact in the medical and insurance tech space. Possess good understanding on leveraging Azure's AI and machine learning services to deploy, monitor, and maintain ML models, ensuring high availability and performance."
Cozey, Data Engineer, "What you'll be doing Implement and own the end-to-end data stack and process at Cozey Set-up and manage ETL/ELT processes for data collection, transformation, and integration from various sources. Manage our data warehouse to ensure it meets organizational needs. Managing integrations to our BI and Analytics systems We'd love to hear from you if you have 5+ years' experience in owning our new data stack. This includes reporting needs and data ingestion to transformation and reporting. Proficiency with ELT tools such as Fivetran, AWS Glue, and Apache Airflow. Advanced skills in Python and SQL for data manipulation, analysis, and automation. Expertise in BI and visualization tools, such as PowerBI. Understanding of data governance principles, data quality management, and regulatory compliance (e.g., GDPR, CCPA). Experience with AWS, including services like S3, and Redshift. Experience with integrations like Google Tag Manager and Marketing platforms."
Teranet Inc., Big Data Engineer, "What You'll Be Doing Participate in planning with business product owners, data analysts and identify tasks for the data analytics team. Design and develop programs for setting up data pipelines, curate data for the enterprise-wide usage, prepare data models for specific use case. Develop test objectives, test plan and success criteria (connectivity, data replication, auto fail-over, peak load performance etc.). Work with infrastructure, security, and networking teams to ensure connectivity requirements are met for data pipelines sources and targets. Tuning of data ingestion and replication to meet performance targets. Configure the CDC framework as required to create daily/weekly/monthly data snapshots within acceptable performance targets. Design and implement technology best practices, guidelines, and repeatable processes. Create design, test plan, and confluence documentation. Able to self-direct, prioritize and perform assignments with minimum supervision About You 5+ years of experience with Hadoop, Hive, Spark, Python, Bash, Linux Deep Hive/Spark/SQL knowledge, development, and testing experience Expert Python/Spark/Shell development and coding best practices skills Extensive experience in developing ETLs and processing large datasets Experience with Airflow data pipeline implementation Excellent day-to-day working knowledge of Git with exposure to Azure DevOps Experience building data models to support BI data visualizations using Tableau Knowledge of CDC based data ingestion setup, preferably using HVR Familiarity with AWS services such as S3, EKS, and Kubectl is highly beneficial Experience with Terraform, and CI/CD tools is a plus Excellent written and verbal communication skills"
Movable Ink, Data Engineer Business Intelligence & Analytics, "Responsibilities: Partner with other Data Engineers and other stakeholders to model ingested data into consumable objects tied to business/stakeholder need Implement and test data pipelines Document data lineage for transformation of data as it flows from source to target Create comprehensive data products with well-defined SLAs Partner with Analysts to build dashboards and reports in a BI tool Partner with Software Engineers to build in-platform reports Work with team to help administer various aspects of our data platform Occasionally participate in Show and Tell events to demonstrate recent work Qualifications: 3+ years experience as a Data Engineer for a SaaS or SaaS-like company with a focus on Cloud-based Data Warehouse platforms (e.g. AWS Redshift, Google BigQuery, Snowflake). We use AWS Redshift. Experience with data modeling in a data warehouse environment (Kimball, Inmon, DataVault). We use Kimball and DataVault 2.0 Experience with creating code-based data pipelines (we use Python and SQL) and orchestrating them with tools such Apache Airflow, Luigi, Dagster. We use Airflow. Experience building data pipelines from structured, unstructured, and semi-structured data. Experience working with version control tools (e.g. git). We use Git/Github. Experience working within the Continuous Integration / Continuous Delivery (CI/CD) paradigm. We use Gitlab CI/CD. Experience working with Linux/BSD shell commands. Our Linux-based servers run Ubuntu Linux, and our employee computers are Macbook Pros. Experience with at least one Enterprise BI Tool (e.g. Tableau Server, QlikSense, etc.). We use Tableau/Tableau Server on Linux. Experience building data pipelines from APIs published by third party SaaS platforms. Some of the third-party SaaS platforms we need to extract data from are: Salesforce.com, Zendesk, Netsuite, Shortcut. Strong documentation skills and in particular has experience documenting source-to-target mappings (STMs). Experience working in an Agile environment"
Definity, Data Engineer, "The Data Engineer is a critical role responsible for designing, building, and maintaining the data infrastructure and pipelines that power data-driven insights and applications. This role requires expertise in Google Cloud Platform (GCP) and its data services to effectively manage and process large-scale datasets. Key Responsibilities: Data Ingestion and Processing: Design and implement robust data pipelines to collect, clean, transform, and store massive volumes of data using GCP services like Dataflow, Pub/Sub, and Cloud Storage. Data Warehousing and Analytics: Build and optimize data warehouses on Google BigQuery for efficient data analysis and reporting. Data Modeling: Design and implement scalable data models to support business intelligence and machine learning applications. ETL/ELT Development: Develop and maintain Extract, Transform, Load (ETL) or Extract, Load, Transform (ELT) processes using GCP tools. Performance Optimization: Continuously monitor and optimize data pipelines and queries for performance and cost-effectiveness. Collaboration: Work closely with data scientists, analysts, and other stakeholders to understand data requirements and deliver data solutions. Required Skills: Google Cloud Platform (GCP): Strong proficiency in GCP data services, including BigQuery, Dataflow, Pub/Sub, Cloud Storage, and DataBricks. Data Engineering Tools: Experience with Apache Beam, Apache Airflow, or similar data pipeline orchestration tools. Programming Skills: Proficiency in Python or Java for data processing and pipeline development. Data Modeling: Expertise in data modeling techniques and schema design. SQL: Strong SQL skills for data querying and analysis. Problem Solving: Ability to analyze and solve complex data engineering challenges. Qualifications: Bachelor's or Master's degree in Computer Science, Data Engineering, or a related field. 5+ years of experience in data engineering, with a focus on Google Cloud Platform. Experience with large-scale data processing and distributed systems. Preferred Skills: Google Cloud Professional Data Engineer Certification. Experience with machine learning and data science workflows. Knowledge of data visualization and reporting tools. Strong communication and collaboration skills. Understanding of AI This Data Engineer role, with its emphasis on Google Cloud Platform expertise, is ideal for individuals passionate about building scalable and efficient data solutions to drive business insights and innovation."
Amazon, Big Data Engineer, "You Will Develop and enhance current data models and pipelines to support emerging business needs. Develop next generation architecture to support near real time analytics and operational use cases. Drive continuous improvements to data quality, compliance, processing times, cost and SLAs for existing pipelines Collaborate with Data Scientists, ML Engineers to develop machine learning features that feed into our next generation Forecasting and ML Models. Keep up to date with advances in big data technologies and run pilots to design the data architecture to scale with ever-growing data volumes and emerging use cases Basic Qualifications 4+ years of data engineering experience Experience with data modeling, warehousing and building ETL pipelines Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets Experience with SQL Experience in at least one modern scripting or programming language, such as Python, Java, Scala, or NodeJS Preferred Qualifications Experience with AWS technologies like Redshift, S3, AWS Glue, EMR, Kinesis, FireHose, Lambda, and IAM roles and permissions Experience with non-relational databases / data stores (object storage, document or key-value stores, graph databases, column-family databases) Experience providing technical leadership and mentoring other engineers for best practices on data engineering Experience with data audit and reconciliation strategies. Experience with data pipeline orchestration frameworks, such as Airflow. Experience with streaming data processing technologies."
IT Connex, Big Data Developer, "Looking for a Big Data developer with extensive experience with Pyspark to build/optimize large scale data processing system Qualifications 6-8 years of experience in big data development using PySpark and Spark Ecosystem Extensive experience with PySpark/Python Solid understanding of Hadoop, Hive and HDFS Proficiency in SQL writing and Query optimization"
DRW, Data Engineer, "Responsibilities Manage large scale data projects, from design to implementation Create and manage ETL and data pipelines Continuously manage, clean and validate historical and live data for downstream ML tasks Combine large datasets from a variety of sources Design, build and maintain dashboards to visualize historical and live data Build scalable and performant software using software engineering best practices Drive data projects and lead new initiatives Collaborate with stakeholders from different departments Qualifications A bachelor's or a master's degree in computer science, software engineering or equivalent 3+ years of experience building software systems Proven track record of working with large datasets (ingesting, cleaning, preprocessing, analyzing and validating data) Excellent programming and scripting skills (Python, SQL, Go or equivalent) Experience working with timeseries structured data or streaming data Experience with relational databases, time series databases Strong understanding of software engineering principles and best practices Strong problem-solving and analytical skills Excellent communication and collaboration skills Experience with any of these technologies is an asset: Redis, RabbitMQ, GraphQL, Kafka, ELK, Docker, AMQP"
GHGSAT, Data Engineer, "Key Responsibilities: Monitor and manage the health and performance of data infrastructure. Design and implement ETL processes, leveraging tools like Airflow, DBT Maintain comprehensive documentation of data models, architecture, and APIs. Proactively troubleshoot and optimize data products to enhance performance and integrate best practices throughout the data development lifecycle. Work closely with data scientists, data analysts, and other engineering teams to understand data requirements and ensure the availability and reliability of data services. Research and implement new sources of structured and unstructured data to unlock new analytical capabilities Support the development of CI/CD pipelines and automation tools using GitLab to ensure quick and reliable data deployment across multiple environments. Required Qualifications: Bachelor's degree in computer science, Engineering, or a related field. Advanced degree or relevant certifications are a plus. 3+ years of hands-on experience in a data engineering or analytics related role. Proficient with using Python or similar programming languages to solve problems with data and maintain data infrastructure. Proficient with PostgreSQL. Hands-on experience with query optimization, database design, schema versioning, and running migrations within a PostgreSQL environment (or with a similar RDBMS such as MySQL). Demonstrated experience developing and maintaining data pipelines with tools such as Airflow and DBT. A solid understanding of data modeling and ETL principles, and experience with implementing these in practice. Experience in supporting data modeling initiatives and collaborating closely with Analytics Engineers and Data Scientists. Strong problem-solving skills with the ability to troubleshoot and resolve complex issues. Experience designing and building data models using data warehousing concepts to enable access to data across multiple domains. Excellent communication skills with the ability to collaborate effectively with cross-functional teams. Excellent written communication skills, with the ability to craft clear and detailed documentation. Nice to have: Experience working as a data analyst or demonstrated experience applying data in new ways to solve complex problems in a business or scientific domain. Experience building robust, production grade data pipelines. Experience working with geospatial data and postGIS. Knowledge of other programming languages or data tools. Relevant certifications in cloud platforms (AWS, Azure, GCP) or terraform."
The Brattle Group, Data Engineer, "Some of the day-to-day responsibilities of this role include: Developing processes and pipelines for cleaning, validating, and uploading data to tabular formats ex: SQL Server, Azure Databricks, or PowerBI, in collaboration with IT and project teams Writing reproducible scripts (Python preferred) and defining processes that will classify, clean, and visualize unstructured text data Researching and implementing new tools and technologies to improve or introduce capabilities Loading and supporting internal applications used for eDiscovery and structured text extraction Liaise with clients and Brattle IT to determine ideal mode of delivery of digitized data/information from client to Brattle project teams Maintaining an internal repository of scripts for common work to be used throughout the firm Grow internal knowledge base by developing and presenting training courses to consulting staff Keeps abreast of new tools, software, and technologies and make recommendations to project teams for future deployment THE CANDIDATE Bachelor's Degree in Computer Science, Computer Engineering, or related field 3-4 years of relevant professional experience, preferably in a professional services firm Experience working as a Data Scientist or in an applied data science role Proficiency with Python, SQL, and utilizing SQL in data solutions Experience with cloud computing services (Microsoft Azure or AWS) Experience with Generative AI or other Machine Learning applications such as predictive modeling, classification, and clustering Experience with R is a plus Data visualization skills is a plus Excellent requirements gathering and project delivery skills Outstanding verbal and written communication skills Strong technical communication skills and experience writing for non-technical clients Ability to estimate and convey project timelines, challenges, and milestones Ability to work collaboratively and maintain a holistic view of project goals Entrepreneurial orientation with an interest in owning research projects"
Saltworks Technologies: Industrial Water + Lithium, Data Engineer, "Responsibilities: Design, build, and deploy robust data management systems and software applications for internal company use. Translate project and business requirements into data schemas and software functional requirements. Create reliable and secure connections between enterprise software systems and data sources by leveraging APIs and setting up data pipelines. Implement ETL processes to transform raw data into consumable formats. Follow best practices for data schema design, coding, documentation, and version control (ensuring clarity, efficiency, reliability, and maintainability). Participate in ongoing system maintenance and optimization, ensuring system resilience, scalability, and security. Communicate design choices, modifications, and challenges to other teams and individuals with varying technical backgrounds. Contribute to team development by mentoring junior developers and improving in-house practices and standards. Stay up-to-date on industry trends and emerging technologies. You will be a great fit if you have: A bachelor's degree in Computer Science, Software Engineering, or equivalent. 5+ years of professional experience in Data Engineering. Expertise in relational databases, specifically Microsoft SQL Server, including data schema design, developing entity-relationship diagrams, query writing, and stored procedures. Proficiency in coding and DevOps tools, especially Python, node.js, Git, and Docker. Excellent understanding of SDLC principles and best practices. Impeccable attention to detail and a sense of urgency in project completion. Strong interpersonal skills, with the ability to work effectively within a team. Excellent problem-solving and troubleshooting/debugging skills. Ability to manage multiple projects simultaneously. Experience with cloud-based technologies (AWS/Azure). Experience with time series data is an asset. Bonus: experience with data visualization tools such as PowerBI and Tableau."
Scribd Inc., Data Architect/Principal Data Engineer, "Required Skills 7+ years of experience in data engineering, with a strong background in data architecture, data modeling, and data management, building and scaling robust data systems for complex business domains. Expertise in Scala or Python, with a deep understanding and hands-on experience in Spark for designing, optimizing, and scaling large-scale data processing pipelines, and proficiency in at least one SQL dialect. Experience with data lake technologies (e.g., Databricks, Delta Lake), data storage formats (Parquet, Avro), query engines (such as Photon, Spark SQL), and both real-time streaming and batch processing, or equivalent technologies and frameworks. Desired Skills Experience and working knowledge of streaming platforms, typically based around Kafka. Strong grasp of AWS data platform services and their strengths/weaknesses. Hands on experience in implementing data pipelines for data ingestion and transformation to support analytics and ML pipelines Strong experience communicating asynchronously using collaboration tools like Jira, Slack, etc. Experience using automation and CI/CD tooling like Git, GitHub,Docker,Jenkins, Terraform, etc. Experience developing standards for database design and implementation of various strategic data architecture initiatives around data quality, data management policies/standards, data governance, privacy and metadata management Working experience integrating with BI frameworks like Qlik, ThoughtSpot, Looker, Tableau, etc."
ecobee, Staff Data Engineer, "How You'll Make an Impact: Cross-Domain Problem Solving: Lead the design and implementation of scalable data pipelines and systems for complex problems that require detailed understanding across multiple domains (e.g., data, machine learning, IoT, cloud infrastructure). These problems will often come with high levels of ambiguity, incomplete data, and evolving requirements. Architectural Impact: Contribute to ecobee's system architecture with designs that have been battle-tested, resulting in significant, long-lasting impact within a specific domain. Solutions are expected to integrate elegantly with ecobee's broader enterprise architecture and align with company-wide standards. Enterprise-Wide Architecture: Start to think beyond individual components or domains, considering ecobee's broader architectural strategy. Collaborate with principal engineers and directors to ensure designs complement the company's vision. Technical Proposals: Propose technical solutions and strategies that have a significant impact on ecobee's data ecosystem. These solutions should drive improvement in the scalability, performance, and resilience of the company's products and services. Component Ownership: Take end-to-end ownership of full components within your domain of expertise, ensuring that their design, implementation, testing, deployment, and operations meet high standards. These components will likely interact with systems in other domains, requiring careful consideration of cross-team dependencies. System Operations & SLAs: Define and track SLAs for the components you own, ensuring they meet operational excellence standards and contribute to the system's overall reliability. Maintainability & Scalability: Systematically consider maintainability in designs and implementations, with a focus on ensuring systems can scale to support ecobee's growing data needs. Mentor & Lead: Actively mentor engineers across the organization, helping them achieve concrete technical and professional goals. Drive knowledge-sharing initiatives through code reviews, technical talks, and training sessions. Cross-Team Collaboration: Facilitate and guide technical discussions across squads, ensuring decisions are aligned with ecobee's strategic goals. You'll help foster an inclusive environment where all team members feel heard and respected. Technical Expertise Development: Participate in bar-raiser groups that focus on elevating engineering standards across ecobee, including leading post-mortem reviews, design sessions, and code reviews. Challenging Best Practices: Continuously review existing processes, best practices, and rituals across ecobee's engineering organization. Propose and implement improvements that enhance efficiency, collaboration, and quality. Delivery Metrics & Quality: Educate teams on key software delivery metrics and help track progress. Ensure that the team's testing approaches align with accepted frameworks, and work to close gaps in quality metrics. Documentation & Knowledge Sharing: Foster a culture of documentation and transparency within the team and across stakeholders, ensuring that key processes and decisions are well-documented and accessible. Forward-Thinking Design: Anticipate future data challenges, such as scalability and security concerns, and propose strategies to avoid roadblocks. You'll look for opportunities to improve existing solutions and identify novel approaches that haven't been tried before. Technology Evaluation: Stay ahead of industry trends by evaluating and recommending new technologies that align with ecobee's goals in data engineering, machine learning, and IoT. Domain-Wide Impact: Your work will have a measurable impact across multiple teams within the Data Engineering & Machine Learning Services group. This impact will often have significant customer implications, driving improvements in performance, scalability, and product capabilities. Economic Thinking & Risk Management: Drive a culture of thoughtful decision-making, balancing technical innovation with practical constraints like time, cost, and risk. Work closely with partner teams to prioritize capabilities that will deliver the highest business impact. Proactive Issue Resolution: Anticipate blockers and delays in projects before they require escalation. Proactively work to resolve these challenges by engaging with stakeholders and partner teams. What You'll Bring to the Table: 10+ years of experience in data/software engineering, with a proven track record of owning and delivering complex, cross-domain projects at scale. Extensive experience in building and maintaining scalable data pipelines and architecture using tools like Apache Spark, Kafka, and Airflow. Expertise in cloud data platforms (AWS, GCP, or Azure), with a strong focus on distributed systems, cloud managed open source frameworks and services, and IoT data integration. Solid understanding of end-to-end data systems, from ingestion to machine learning model deployment and inference. Expertise in data security, data governance, and compliance regulations relevant to the industry. Extensive experience in data architecture, database design and data engineering methodologies across multiple industries, with at least 5 years in a technical leadership role. Ability to solve problems that span multiple domains, including data engineering, machine learning, IoT, and cloud infrastructure. A deep understanding of how these domains interact is essential. Experience with real-time data processing, analytics platforms, and machine learning integration is highly valued. Proven ability to mentor and guide engineers, from juniors to senior engineers, across multiple teams. Experience facilitating technical discussions and driving consensus. Demonstrated ability to lead cross-functional initiatives and work effectively across squads. A strategic mindset, with the ability to think ahead about potential roadblocks and design systems that can scale and evolve with ecobee's needs. Experience driving large technical initiatives from ideation through implementation, with a focus on creating systems that deliver high business impact. Demonstrated track record of contributing to new processes and practices within engineering teams. You're comfortable challenging the status quo and driving improvements. Experience with software delivery metrics and ensuring that teams follow best practices in testing, code quality, and maintainability."
CGI, Data Engineer, "We are seeking a highly skilled Data Engineers to join our dynamic team. In this role, you will be responsible for managing and optimizing the data infrastructure, implementing data governance frameworks, and developing real-time data streaming solutions. This role requires expertise in a variety of data management tools and technologies, including Collibra, Apache Iceberg, Jira, Kafka, Microsoft Power BI, and Snowflake. If you are passionate about data and have a strong background in data architecture and AI/ML, we would love to hear from you. Join this exciting new initiative as a Data Practitioner. Your future duties and responsibilities As a Data Engineer your responsibilities will be: Implement and manage data governance with Collibra. Utilize Apache Iceberg for data lake management. Coordinate projects using Jira and Jira Align. Design and maintain real-time data streaming solutions with Apache Kafka. Develop and maintain dashboards and reports using Microsoft Power BI. Implement data integration workflows using Apache NiFi and Snaplogic. Administer and optimize Oracle Database systems. Use SAP PowerDesigner for data modeling. Manage and optimize data warehousing solutions with Snowflake. Develop data processing scripts and applications using Python. Apply AI/ML techniques for data analysis. Ensure high data quality and integrity. Implement and maintain data security protocols. Optimize the performance of data systems and applications. Create and maintain comprehensive documentation for data processes and systems. Provide training and support to team members on data tools and best practices. Collaborate with cross-functional teams to understand data needs and deliver solutions. Stay updated with the latest industry trends and technologies. Required Qualifications To Be Successful In This Role 3-5+ years of proven experience with the listed technologies and tools. Data Governance Center (Collibra) Iceberg (Apache) Jira (Atlassian) Jira Align (Atlassian) Kafka (Apache) Microsoft Power BI NiFi (Apache) Oracle Database SAP PowerDesigner Snowflake Snowflake Data Warehouse Lakehouse Snaplogic Python AI/ML Collibra Strong understanding of data governance, data management, and data integration. Excellent problem-solving skills and attention to detail. Excellent communication and interpersonal skills. Ability to work independently and in a team-oriented environment. Strong opinions, lightly held Willingness to own, research, and argue your points of view, bring new ideas to the table, but deliver according to consensus and expectations. Required Level of Education (include certifications): University or Technical Diploma in IT discipline with equivalent experience. Experience with Cloud data services (Azure, GCP, AWS) and/or Snowflake is desired. Relevant certifications for this role will be nice to have. Soft Skills: Excellent communicator Well organized Detail orientated Team Player"
Haventree Bank, Data Scientist/Machine Learning Engineer, "Major Duties & Responsibilities: Credit Risk Modeling: Develop, enhance, and validate statistical and machine learning models for credit risk assessments, loan default prediction, and portfolio risk optimization. Fraud Detection: Design machine learning models to detect and mitigate fraud in real-time, supporting direct-to-consumer business initiatives. Geospatial Analysis: Conduct geospatial modeling and work with raster and vector data to address climate-related risks and optimize lending and operational strategies. Model Deployment: Collaborate with IT and relevant stakeholders to deploy machine learning models, ensuring scalability and integration with existing systems. Data Analysis and Insights: Analyze large, complex datasets to extract actionable insights, focusing on business priorities like customer behavior, risk mitigation, and operational efficiency. AutoML and Tool Usage: Utilize advanced data science and analytics platforms (e.g., Alteryx, DataRobot, or similar) to automate workflows, streamline model development, and support seamless deployment of predictive models. Model Monitoring: Monitor and fine-tune deployed models to ensure accuracy, fairness, and performance over time. Independent Research and Innovation: Independently explore and implement cutting-edge methodologies from academic research, open-source communities (e.g., GitHub, Kaggle), and industry best practices to drive team innovation and stay ahead of trends. Stakeholder Collaboration: Partner with internal teams, including Risk Management, IT, and Business Operations, to align model outputs with strategic goals. Qualifications & Experience: Degrees, Diplomas & Certifications: A Master's degree in a quantitative field (e.g., Statistics, Actuarial Science, Data Science, Financial Engineering, or Analytics), or a Bachelor's degree with 3+ years of hands-on experience in data science, machine learning, or a related field requiring rigorous statistical knowledge and techniques. Years and Range of Experience Required to Perform the Job: Strong experience with statistical modeling and machine learning techniques, including regression, decision trees, clustering and probabilistic Bayesian model. Proficiency in programming languages like Python or R, with experience with relevant libraries (e.g., scikit-learn, xarray, PyBN, GeoPandas, Rasterio, pyproj). Hands-on experience with cloud platforms like AWS, including working with data lakes and scalable data processing tools such as Databricks. Proficiency with tools like Alteryx and Dataiku for automating analytics workflows and enabling machine learning applications. Experience in geospatial data analysis, including working with QGIS and integrating raster and vector data to solve real-world business problems, such as climate risk assessment and geographic optimization. Familiarity with data visualization tools (e.g., Tableau, Power BI) or libraries (e.g., Matplotlib, ggplot2) to effectively communicate insights. Experience in synthesizing data, including generating synthetic datasets for model development. Strong foundational understanding of Artificial Intelligence and Analytic Process Automation (APA) to identify and implement opportunities for streamlining workflows and accelerating AI adoption. Exceptional analytical and problem-solving skills, with the ability to translate complex business problems into actionable technical solutions. Ability to independently research new techniques and implement solutions using diverse resources. Excellent communication skills, with the ability to articulate their findings and recommendations clearly to both technical and non-technical stakeholders. Knowledge of credit risk or insurance frameworks and regulatory requirements (e.g., Basel, IFRS 9, IFRS 17), with the ability to apply actuarial science concepts to credit and other emerging risk domains is preferred. Experience deploying ML models in production environments, including containerized solutions such as Docker or equivalent is preferred. Knowledge of frameworks like Flask for creating APIs to serve machine learning models is preferred. Experience in fraud detection models and techniques, including anomaly detection and supervised/unsupervised learning methods is preferred."
Cognizant, Data Engineer, "In this role, you will Responsible for the day-to-day development process and leading all aspects of the technical design implementation, data profiling, and data analysis. Develop a deep understanding of systems and processes to extract insights from existing data, using rigorous project management subject area to ensure the highest possible quality of delivery while managing team capacity. Deploy, Run, and Debug the ETL engine running the scripts built by the ETL Engineering team. Conduct business due diligence activities to identify analytics opportunities for transformation. Ensure data integrity and security within the Snowflake environment, following industry regulations and standards. What You'll Need To Succeed (required Skills) Demonstrated ability working in Data Warehousing and proven experience with DBT core or DBT cloud. Relevant experience of a minimum of 5 years in Data Vault 2.0 implementation experience. Ability to work in a fast-paced, fast paced environment and mentor team members. Prior experience in managing high volume and high complexity data projects across multiple delivery location in virtual environment with excellent client interpersonal skills. Strong problem-solving and conceptual thinking abilities."
Compunnel Inc., AWS Data Engineer, "8+ years of experience in Data Engineer with AWS, Glue, Lambda, SQL, Python, Dovps, Redshift. 1 .Must have working knowledge in designing and implementing data pipelines on any of the cloud providers (AWS is preferred). Must be able to work with large volumes of data coming from various sources. Perform data cleansing, data validation etc 2. Hands on ETL developer who is good at python, SQL. AWS services like glue, glue crawlers, lambda, red shift, athena, s3, EC2, IAM, Monitoring and Logging mechanisms- AWS cloudwatch, setting up alerts. 3. Deployment knowledge on cloud. Integrate CI/CD pipeline to build artifacts and deploy changed to higher Environments. 4. Scheduling frame works Airflow, AWS Step functions 5. Excellent Communication skills, should be able to work collaboratively with other teams"
Electronic Arts (EA), Data Engineer III, "What You Will Do Collaborate with cross-functional stakeholders, including analyst, governance, project management, and data scientists, to comprehend data needs and ensure alignment with business goals. Design, build, and launch efficient and reliable data pipelines to move and transform data effectively, handling both large and small amounts. Possessing the capability to interpret and construct code, as well as provide insightful feedback during code reviews, focusing on elements such as scalability, performance, and adherence to best practices. Ensure scalability and efficiency by designing and implementing complex features, including multi-layered data workflows. Deploy data quality checks in collaboration with quality engineering and data governance teams to uphold high-quality data standards. Uphold a high-quality code base by writing and reviewing performant, scalable, and well-tested code to deploy effectively using source control products such as GitLab/GitHub. Define and manage SLAs for all data sets in allocated areas of ownership. Utilize job scheduling tools like Airflow and GitLab Runners to streamline processes. Maintain data engineering and architecture best practices and standards within the team and the wider organization, fostering a culture of quality, innovation, and experimentation. Take ownership of the end-to-end data engineering component of the product. Identify, design, and implement internal process improvements in collaboration with data architects, including automating manual processes and optimizing data delivery. Create and maintain technical documentation. Qualifications BS/MS in Computer Science, Engineering, or a related field with 8+ years of data engineering experience. A must experience with Snowflake, Git, DBT & Airflow 7+ years of experience in ETL orchestration and workflow management tools. Experience using containerization technologies such as Docker or Kubernetes. Proficiency in programming languages such as Python (preferred) & SQL Strong understanding of data modeling, warehousing, and building ETL/ELT/EtLT pipelines. Knowledge or Experience in Data mesh and Data Lake House Designs. Excellent written and verbal communication skills, ability to simplify and synthesize complex constructs."
Lumenalta, Data Engineer - Databricks, "Requirements 3+ years experience in a data engineering role using Python; ideally, you have delivered business-critical software to large enterprises You are comfortable manipulating large data sets and handling raw SQL Experience using technologies such as Pyspark/AWS/Databricks is essential Experience creating ETL Pipeline from scratch"
Lumenalta, Data Engineer - Databricks, "Requirements 3+ years experience in a data engineering role using Python; ideally, you have delivered business-critical software to large enterprises You are comfortable manipulating large data sets and handling raw SQL Experience using technologies such as Pyspark/AWS/Databricks is essential Experience creating ETL Pipeline from scratch"
Movable Ink, Principal Data Engineer, "Responsibilities: Partner with internal operations teams to identify, collect, and integrate data from various business systems, ensuring comprehensive and accurate data capture Design, implement, and maintain robust data pipelines that feed data into our Data Platform, ensuring high performance, scalability, and reliability Ensure data pipelines adhere to best practices and are optimized for performance and scalability Conduct thorough testing of data pipelines to validate data accuracy and integrity Monitor data pipelines, troubleshoot any issues that arise, and make improvements to these issues where applicable Establish and track SLAs for data processing and delivery, ensuring timely and reliable access to data for all users Become a mentor for less experienced team members, and establish patterns and practices that can be followed to increase quality, accuracy, and efficiency of solutions produced by the team Work with other teams in order to ensure access to data corresponds with company policies, and ensure data access, processing, and storage is in compliance with regulatory (e.g. GDPR, CCPA, etc.) requirements Qualifications: 12+ years of professional experience in data engineering, software engineering, database administration, business intelligence, or related field with 8+ years of that experience as a Data Engineer with a focus on cloud-based Data Warehouse platforms (Redshift, Snowflake, Firebolt, BigQuery). We currently use Redshift. Elite-level understanding on how to work with and optimize multi-petabyte, mission-critical databases, ensuring high availability, performance, and reliability informed by a strong understanding of database internals Elite-level proficiency with Python and SQL languages, and significant experience building robust data pipelines with these languages Elite-level proficiency in using, deploying and managing at least one data pipeline orchestration tool/framework such as Apache Airflow, Prefect, etc. We currently use Apache Airflow. Significant experience in building solutions that comply with regulatory requirements such as GDPR and CCPA Significant experience in designing and implementing solutions that can support both batch and real-time data consumption models Significant experience in building solutions that implement data security best practices within an AWS environment Significant experience in providing technical leadership, setting best practices, and successfully driving the adoption of new technologies and methodologies within a fast-moving organization Significant data modeling experience spanning more than one data modeling paradigm (e.g. Data Vault, Kimball/Ross, Inmon) Experience working in an Agile/Scrum environment, has experience working with technical managers and product owners/manager to break down high-level requirements into actionable cards Experience working with streaming platforms such as Apache Kafka and Apache Pulsar Excellent communication skills for effective collaboration with cross-functional teams"
ONxpress Transportation Partners, Data Engineer, "Job Summary As a Senior Developer with DevOps expertise, you will be responsible for designing, developing, and maintaining cloud-based applications on the Azure platform. You will also implement and manage DevOps practices to enhance our software development lifecycle, ensuring efficient, reliable, and scalable solutions. Responsibilities Design, develop, and maintain cloud-based applications and services on the Azure platform. Implement and manage DevOps practices, including continuous integration (CI), continuous delivery (CD), and infrastructure as code (IaC). Automate deployment, monitoring, and management of cloud infrastructure and applications. Collaborate with development, operations, and QA teams to streamline the software development lifecycle. Monitor application performance and implement optimizations for scalability and reliability. Develop and maintain comprehensive documentation for cloud architectures, processes, and procedures. Troubleshoot and resolve issues related to cloud infrastructure and applications. Stay updated with the latest Azure services, DevOps tools, and industry best practices. Provide technical leadership and mentorship to junior team members. Qualifications Education & Experience Bachelor's degree in computer science, Information Technology, or a related field. 10 years experience as an Azure Cloud Developer, with strong expertise in DevOps practices. Proficiency in programming languages such as C#, Python, or Java. Experience with Azure services, including Azure DevOps, Azure Functions, Azure App Services, Azure Kubernetes Service (AKS), and Azure Logic Apps. Strong knowledge of CI/CD pipelines, automation tools (e.g., Jenkins, GitHub Actions), and IaC tools (e.g., Terraform, ARM templates). Experience with containerization and orchestration technologies (e.g., Docker, Kubernetes). Familiarity with monitoring and logging tools (e.g., Azure Monitor, Log Analytics, Prometheus, Grafana). Experience with other cloud platforms (e.g., AWS, Google Cloud Platform). Knowledge of security best practices in cloud environments. Required Skills Excellent problem-solving and analytical skills. Strong communication and collaboration skills. Ability to manage multiple tasks and projects simultaneously. Preferred Qualifications Azure certification(s) (e.g., Microsoft Certified: Azure Developer Associate, Azure DevOps engineer Expert)."
Definity, AI Data Engineer, "Key Responsibilities: Data Ingestion and Processing: Design and implement robust data pipelines to collect, clean, transform, and store large volumes of data from diverse sources. Data Preparation: Preprocess and transform data into formats suitable for machine learning algorithms. Feature Engineering: Develop and select relevant features that improve the performance of machine learning models. Data support for Models: Collaborate with Data Scientists and ML Engineering Professionals, and other cross-functional teams to integrate AI solutions into systems and processes. This includes optimizing AI solutions to handle large volumes of data efficiently. Scalability: Design and implement scalable data architectures to support the growing demands of AI applications. Performance Optimization: Continuously optimize data processing and model inference for efficiency and speed. Required Skills: Strong Programming Skills: Proficiency in Python or Java, or Scala. Data Engineering Tools: Experience with Big Data (Big query , Hive), Experience in processing unstructured and structured data , Understanding of Call center data, media data processing, Clickstream , Realtime data processing. Experience with ETL tools like Databricks or equivalent. Cloud Platforms: Expertise with GCP cloud environment Machine Learning Frameworks: Familiarity with TensorFlow, PyTorch, vertex AI or other ML frameworks. Data Modeling: Understanding of data modeling techniques and database design. Data Pipelines: Experience with data pipeline orchestration tools like Apache Airflow. Problem Solving: Ability to analyze and solve complex data engineering challenges. Qualifications: Bachelor's or master's degree in computer science, Data Science, or a related field. Experience with machine learning and AI applications. 5+ years in Technology and Data engineering or a related role 3+ years of experience in cloud. Desired Skills: Experience with MLOps (Machine Learning Operations) practices. Knowledge of containerization technologies like Docker and Kubernetes. Familiarity with data visualization tools. Strong communication and collaboration skills. Experience working with Google cloud platform. Extensive Data engineering experience Understanding and familiarity with AI models"
SecurityScorecard, Machine Learning Engineer, "Responsibilities: Model Development: Design, train, and optimize machine learning models and algorithms. Data Pipeline Creation: Build and maintain scalable data pipelines to preprocess, clean, and transform raw data for analysis and model training. Model Deployment: Implement and manage models in production environments, ensuring scalability, reliability, and performance. Research and Experimentation: Stay updated on the latest machine learning techniques, tools, and frameworks to enhance model accuracy and efficiency. Collaboration: Work closely with data scientists, software engineers, and product teams to understand requirements and integrate ML solutions into products. Performance Monitoring: Continuously monitor, evaluate, and fine-tune models post-deployment to maintain accuracy and robustness. Documentation: Create clear and concise documentation for models, processes, and systems to support team collaboration and knowledge sharing. Required Qualifications: 3+ years of experience or equivalent demonstrable skills in ML Engineering, Data Science or related discipline. Bachelor's or Master's degree in Computer Science, Engineering, Mathematics, Physics, or a related field. Strong programming skills in Python. Experience with machine learning frameworks such as PyTorch, TensorFlow, or Scikit-learn. Proficiency in data manipulation and analysis using tools such as Polars, Pandas, NumPy, or SQL. Solid understanding of algorithms, statistics, and data structures. Experience with cloud platforms (AWS, Azure, GCP) and containerization (Docker, Kubernetes). Knowledge of CI/CD pipelines and version control systems (e.g. Git). Familiarity with Linux/Unix command line tools. Preferred Qualifications: PhD degree in Computer Science, Engineering, Mathematics, Physics or a related field. Hands-on experience with LLMs, RAG, LangChain, or LlamaIndex. Experience with big data technologies such as Hadoop, Spark, or Kafka."
SecurityScorecard, Machine Learning Engineer, "Responsibilities: Model Development: Design, train, and optimize machine learning models and algorithms. Data Pipeline Creation: Build and maintain scalable data pipelines to preprocess, clean, and transform raw data for analysis and model training. Model Deployment: Implement and manage models in production environments, ensuring scalability, reliability, and performance. Research and Experimentation: Stay updated on the latest machine learning techniques, tools, and frameworks to enhance model accuracy and efficiency. Collaboration: Work closely with data scientists, software engineers, and product teams to understand requirements and integrate ML solutions into products. Performance Monitoring: Continuously monitor, evaluate, and fine-tune models post-deployment to maintain accuracy and robustness. Documentation: Create clear and concise documentation for models, processes, and systems to support team collaboration and knowledge sharing. Required Qualifications: 3+ years of experience or equivalent demonstrable skills in ML Engineering, Data Science or related discipline. Bachelor's or Master's degree in Computer Science, Engineering, Mathematics, Physics, or a related field. Strong programming skills in Python. Experience with machine learning frameworks such as PyTorch, TensorFlow, or Scikit-learn. Proficiency in data manipulation and analysis using tools such as Polars, Pandas, NumPy, or SQL. Solid understanding of algorithms, statistics, and data structures. Experience with cloud platforms (AWS, Azure, GCP) and containerization (Docker, Kubernetes). Knowledge of CI/CD pipelines and version control systems (e.g. Git). Familiarity with Linux/Unix command line tools. Preferred Qualifications: PhD degree in Computer Science, Engineering, Mathematics, Physics or a related field. Hands-on experience with LLMs, RAG, LangChain, or LlamaIndex. Experience with big data technologies such as Hadoop, Spark, or Kafka."
SecurityScorecard, Machine Learning Engineer, "Responsibilities: Model Development: Design, train, and optimize machine learning models and algorithms. Data Pipeline Creation: Build and maintain scalable data pipelines to preprocess, clean, and transform raw data for analysis and model training. Model Deployment: Implement and manage models in production environments, ensuring scalability, reliability, and performance. Research and Experimentation: Stay updated on the latest machine learning techniques, tools, and frameworks to enhance model accuracy and efficiency. Collaboration: Work closely with data scientists, software engineers, and product teams to understand requirements and integrate ML solutions into products. Performance Monitoring: Continuously monitor, evaluate, and fine-tune models post-deployment to maintain accuracy and robustness. Documentation: Create clear and concise documentation for models, processes, and systems to support team collaboration and knowledge sharing. Required Qualifications: 3+ years of experience or equivalent demonstrable skills in ML Engineering, Data Science or related discipline. Bachelor's or Master's degree in Computer Science, Engineering, Mathematics, Physics, or a related field. Strong programming skills in Python. Experience with machine learning frameworks such as PyTorch, TensorFlow, or Scikit-learn. Proficiency in data manipulation and analysis using tools such as Polars, Pandas, NumPy, or SQL. Solid understanding of algorithms, statistics, and data structures. Experience with cloud platforms (AWS, Azure, GCP) and containerization (Docker, Kubernetes). Knowledge of CI/CD pipelines and version control systems (e.g. Git). Familiarity with Linux/Unix command line tools. Preferred Qualifications: PhD degree in Computer Science, Engineering, Mathematics, Physics or a related field. Hands-on experience with LLMs, RAG, LangChain, or LlamaIndex. Experience with big data technologies such as Hadoop, Spark, or Kafka."
ClearVision Technologies Inc., Computer Vision Developer, "Duties & Responsibilities: Development of Machine Vision algorithms that are efficient and robust. Working with other Software and Mechatronics developers to produce a comprehensive quality assurance solution. Testing of algorithms in an industrial environment. Required Experience: 2+ years of Machine Vision programming experience. 2+ years working in industry. 2+ years of either C++ or C# experience. 2+ years of OpenCV experience. 2+ years of Machine Learning. Required Competencies: Ability to create reliable machine vision algorithms that work well under varying lighting and imaging conditions. Ability to use creative ideas to design fast and well-optimized algorithms. Ability to create and train datasets to produce robust models that can be deployed to customer sites. Preferred Competencies: Visual Studio. Git, source code revision control. Lighting and imaging understanding. Preferred Personal Skills: Good communication skills. Good troubleshooting and problem solving. Customer oriented. Education: Bachelor's degree in engineering- or science-related field. Master's degree preferred."
GPTZero, Machine Learning Engineer, "Responsibilities Design, train, and fine-tune state-of-the-art language models Develop AI agents combined with retrieval-augmented language models Build efficient and scalable ML training and inference systems Stay up-to-date with the latest literature and emerging technologies to solve novel problems Work closely with product and design teams to develop intuitive applications that create societal impact Qualifications 3+ YOE in PyTorch/Transformers Experience pushing the cutting-edge in machine learning Self-starter (pitch, plan, and implement as a project owner in a fast-paced team) Highly motivated to make positive societal impact Ability to wear multiple hats and be a leader as our team grows Visa for work in Canada Bonus: strong open-source portfolio publications at top-tier ML venues experience working in an early-stage startup environment understanding of how machine learning models fail in the wild"
Flight Centre, Data Engineer II, "Key Responsibilities Data Pipeline Development: Build, maintain, and optimize data pipelines using Azure Data Factory to extract, transform, and load (ETL) data from various sources into Azure Data Lake and Azure SQL. Data Lakehouse Architecture: Design and implement scalable data lakehouse architectures leveraging Azure Synapse Analytics, Azure Data Lake Storage, and Azure SQL to enable efficient data storage, processing, and analytics. Data Governance: Establish and enforce data governance policies and standards to ensure data quality, security, and compliance. Performance Optimization: Identify and resolve performance bottlenecks within data pipelines and architectures to improve query response times and system efficiency. Cloud Migration: Assist in migrating existing data systems and applications to the Azure cloud, leveraging cloud-native services and best practices. Collaboration: Work closely with data scientists, analysts, and business stakeholders to understand their data requirements and translate them into technical solutions. Documentation: Create and maintain clear and concise documentation for data pipelines, architectures, and processes. Experience & Qualifications Bachelor's degree in Computer Science, Data Engineering, or a related field. 3-5 years of experience as a Data Engineer or Architect. Strong proficiency in Azure technologies, including Azure Data Factory, Azure Data Lake, Azure SQL, and Azure Synapse Analytics. In-depth knowledge of data lakehouse concepts and best practices. Experience with data modeling, ETL processes, and data warehousing. Proficiency in SQL, Python, or other programming languages. Excellent problem-solving, analytical, and troubleshooting skills. Ability to work independently and as part of a team. Strong communication and interpersonal skills. Preferred Qualifications Certification in Azure Data Engineering or related fields. Experience with cloud-native data processing frameworks (e.g., Spark, Databricks). Familiarity with data virtualization and data mesh concepts."
OnDeck Fisheries AI, Machine Learning Engineer, "What you'll be doing This means on top of tweaking our standard vision methods and engineering ML and data pipelines, you'll get to work on cutting-edge open-domain vision methods and even contribute to original research and papers submitted to conferences. Your responsibilities will encompass: Developing state-of-the-art machine learning models that can reason about visual data and retrieve relevant answers from external knowledge sources. ML engineering of systems that ingest large volumes of visual data, and allow finding complex sequences of events or objects never seen in training data. Working on the entire ML lifecycle, from conducting research and developing innovative models to productionizing them and quantifying their real-world improvements. Contributing to automated model lifecycle management, which takes models to production, handling large volumes of video footage while undergoing updates smoothly. You will have significant ownership over mission-critical development, propelling us as a first-to-market at scale. You'll also get to work on cutting edge applied ML research and even publish your work at top conferences. In return, we're looking for commitment to and excitement for OnDeck's journey at the forefront of ocean tech, climate tech and open-domain computer vision. Minimum Qualifications 2+ years of full-time, non-internship work experience in applied research and ML engineering for production environments Demonstrated ability to implement novel machine learning literature Proficiency in programming and implementing machine learning workflows using Python (experience with C, C++, CUDA, and JavaScript/TypeScript is an asset) Proficiency with PyTorch, TensorFlow, and other modern machine learning frameworks/tools Experience in serving ML models (especially for computer vision), cloud/edge development and optimizing model performance Comfortable in Unix/Linux environments, distributed and parallel systems, and doing data engineering Strong technical communication skills in English, both written and verbal Enthusiasm for building software and doing applied research that revolutionizes automated visual reasoning Authorized to work in Canada (work permit or other). Preferred Qualifications Master's or Ph.D. in Computer Science, Statistics, Engineering, or a related field Experience in startups or high-impact roles in smaller organizations Knowledge of containerization and orchestration for large-scale deployment (Docker, Kubernetes) Proficiency with MLOps and cloud infrastructure (e.g. AWS EC2, EKS) Experience in setting up and using CI/CD tools (e.g., GitHub Actions, AWS CodePipeline) Up-to-date knowledge in computer vision research Contributions to research in deep learning and computer vision applications, such as peer-reviewed conference papers at NeurIPS, ICML, ICLR, CVPR, or journal papers at JMLR. Ability to develop accessible technologies"
Quantiphi, Machine Learning Architect, "Role & Responsibilities Lead AI/ML Initiatives Drive key AI and ML projects leveraging AWS Bedrock Amazon Q, and Quicksight to support internal departments, including actuarial and finance teams. ML Architecture Design: Design and develop scalable ML architectures that integrate LLMs (Large Language Models) with AWS Kendra for advanced document retrieval, and implement RAG (Retrieval-Augmented Generation) for handling complex files. Team Leadership & Development: Build and mentor a high-performing AI/ML team, focusing on internal capability building while backfilling key roles as necessary. Chatbot Development: Lead the deployment of chatbots like "Sun Life Asks" for internal document queries, ensuring sophisticated handling of complex files. Data Analytics & Dashboards: Use Amazon Q and Quicksight to develop AI-powered dashboards that provide insights for actuarial and finance teams, enhancing data-driven decision-making. Collaboration with Stakeholders: Partner with cross-functional teams to design AI/ML solutions tailored to business needs, including actuarial and finance departments. Troubleshoot & Innovate: Address technical challenges related to the integration of AWS Bedrock LLMs with Amazon Kendra and other AI/ML services, focusing on real-time, scalable solutions. Stay Current: Stay updated with cutting-edge advancements in the Gen AI and ML domains, particularly within the AWS ecosystem, and ensure best practices are applied. Required Skills: Eligibility for Reliability Security Clearance Bachelor's or Master's degree in Computer Science, Data Science, Machine Learning, Artificial Intelligence, or a related technical field Minimum of 5+ years of experience in Machine Learning, AI, or related fields Proven expertise in AWS AI/ML tools: Bedrock, Amazon Q, Quicksight, Kendra Experience with LLM (Large Language Model) integration, including hands-on experience with RAG for document retrieval Experience with Natural Language Processing (NLP) and chatbot development Strong background in developing ML architectures that scale to meet business needs Advanced proficiency in Python, SQL, and AI/ML frameworks such as TensorFlow, PyTorch, or Hugging Face. In-depth knowledge of AWS services like SageMaker, Lambda, and Redshift. Familiarity with DevOps for AI/ML models, including monitoring and scaling ML models in production environments. Strong problem-solving skills and the ability to translate business challenges into technical solutions"
Octav, Senior Data Engineer, "Key Responsibilities Design and implement ETL pipelines to transform raw data into clean, structured datasets. Collaborate with the Principal Software Engineer to improve data extraction methods, storage solutions, and transformation logic. Work with SQL and NoSQL databases to ensure efficient querying, indexing, and partitioning strategies. Optimize existing AWS infrastructure (e.g., EC2, S3, RDS, Lambda) to support seamless scaling, low latency, and high availability of our data services. Build automated data validation, cleansing, and enrichment routines to maintain high data quality. Assist in integrating machine learning models into production pipelines, ensuring consistent and accurate data feeds for model inference and retraining. Implement metrics collection and monitoring tools to track data pipeline performance, data latency, and data quality KPIs. Collaborate with the Principal Software Engineer to refine metrics and identify opportunities for optimization. Work closely with backend and frontend engineers, data scientists, and product managers to ensure data availability aligns with project requirements. Support the Principal Software Engineer and CTO in estimating resource needs and allocating technical resources efficiently. Advocate for best practices in data security, governance, and compliance within the organization. Requirements Startup readiness flexibility, initiative, and a strong work ethic is a must. Experience: 5+ years as a Data Engineer or in a similar role, focusing on data pipeline architecture, ETL workflows, and database optimization. Proficiency in Python and/or TypeScript. Strong SQL skills for complex queries and performance tuning. Experience with AWS services (e.g., EC2, S3, RDS, Lambda) and familiarity with managed data orchestration frameworks (e.g., Apache Airflow, AWS Step Functions). Experience working with data ingestion frameworks or messaging systems (e.g., Apache Kafka) is a plus. Familiarity with integrating machine learning pipelines and tools such as TensorFlow or PyTorch into data workflows is highly desirable. Soft Skills: Excellent communication, strong problem-solving abilities, and a willingness to collaborate and learn from senior team members."
Clutch, Staff Data Engineer, "What You'll Do Lead the development, testing, and maintenance of complex data management solutions that support business goals and drive decision-making processes at scale. Architect, design, and implement sophisticated ETL/ELT processes to manage complex data transformations, ensuring efficiency, reliability, and scalability of data pipelines. Proactively identify and resolve critical data quality issues, implementing data governance practices and leading regular audits to maintain data accuracy and integrity across multiple data sources. Optimize and evolve data integration processes, applying best practices for performance, scalability, and security to meet growing business demands. Collaborate closely with data architects and other senior stakeholders to design and implement data transformations that align with evolving business requirements and future-proof data architecture. Lead the adoption and implementation of modern data frameworks, including data lakes, data warehouses, and cloud-based architectures, to enhance business intelligence and analytics capabilities. Champion the use of DevOps tools (e.g., Git, GitHub Actions, Docker) for code versioning, deployment automation, and ensuring continuous integration, delivery, and real-time monitoring of data pipelines. Document and standardize data definitions, processes, and solutions, driving the establishment of clear data standards and ensuring cross-team communication and knowledge sharing. Ensure data solutions adhere to the highest standards for security, scalability, and reliability, guiding teams in following industry best practices and company policies. What We're Looking For Bachelor's or Master's degree in Computer Science, Mathematics, Information Systems, or a related technical field. 5+ years of experience in data engineering, data architecture, or a related field, with demonstrated leadership experience in complex data initiatives. Advanced programming skills in languages such as Python, TypeScript, and JavaScript, with a deep understanding of software engineering principles. Expert-level SQL knowledge, with a proven track record of writing and optimizing complex queries and database performance for large-scale systems (e.g., PostgreSQL, MySQL, SQL Server). Extensive experience with modern database technologies, including both relational databases (e.g., Oracle, PostgreSQL, AWS Aurora) and NoSQL databases (e.g., MongoDB, Cassandra, DynamoDB), as well as cloud-based data solutions (e.g., Amazon Redshift, Google BigQuery, Snowflake). In-depth experience with ETL/ELT tools such as Apache Airflow, Talend, or Informatica, and a demonstrated ability to design efficient data flows for large datasets. Hands-on experience with cloud platforms (AWS preferred) such as AWS, GCP, or Azure, and deep knowledge of their data services (e.g., AWS Glue, AWS S3, SageMaker, Azure Data Factory, GCP Dataflow). Familiarity with data warehousing and big data technologies, including Hadoop, Spark, Kafka, for both real-time data streaming and batch processing. Strong understanding of DevOps methodologies, with hands-on experience in using tools like Docker, Kubernetes, Datadog, Terraform, and GitHub Actions for managing and optimizing data pipeline deployments. Proven ability to lead and mentor junior engineers, driving innovation and best practices in data engineering."
Great Canadian Entertainment, Data Accountability Engineer, "Key Accountabilities Builds and maintains data pipelines to move, transform, and load data from various sources to a centralized repository. Optimizes data infrastructure and pipelines for speed, scalability, and cost-effectiveness. Designs, publishes, documents, monitors, secures, and analyzes Application Programming Interfaces (APIs). Creates ETL (Extract, Transform, Load) processes and data ingestion pipelines to clean, transform, and prepare data for analysis. Designs, optimizes, and maintains databases for efficient data storage and retrieval. Manages data warehouses or data lakes to ensure accessibility and reliability of data. Develops and maintains data models and schemas that support analytics and reporting. Ensures data completeness, integrity, and security through validation, monitoring, and governance practices. Normalization of data to eliminate duplications and ensure single source of truth. Works closely with stakeholders to understand data needs and provide access to relevant data. Creates documentation and provides support to help others understand and use the data infrastructure effectively. Appropriately protects the confidentiality, security, and integrity of the Association, employees, borrowers, and other stakeholders. Design, implement, and support business intelligence and problem-solving pipelines and applications using Google Cloud Platform (GCP), internal, and third-party tools. Direct development of complex, big data pipelines and reporting solutions with extremely low tolerances for data error. Lead design, data modeling, architecture, coding, performance tuning, testing, and support of problem-solving systems. Demonstrate ownership of the end-to-end Software Development Life Cycle (SDLC). Maintain the highest levels of development practices including technical design, solution development, systems configuration, test documentation/execution, issue identification and resolution, writing clean, modular, and self-sustaining code. Lead projects in a multi-stakeholder and dynamic environment to achieve team and project Objectives and Key Results. Ensures compliance with licensing laws, health and safety and other statutory regulations. Manages other initiatives as required Education And Qualification Requirements Post-Secondary education in information systems or suitable combination of education and experience. Ability to exceed internal and external customer expectations through timely, effective, and service oriented communication. Experience with GCP and data transformation. Minimum 3 years of application and systems support experience."
Lyft, Machine Learning Engineer, "Responsibilities: Partner with Engineers, Data Scientists, Product Managers, and Business Partners to apply machine learning for business and user impact Perform data analysis and build proof-of-concept to explore and propose ML solutions to both new and existing problems Be able to make effective tradeoffs between model accuracy and its productization complexity and runtime performance. Develop statistical, machine learning, or optimization models Write production quality code to launch machine learning models that can scale well to serve millions of requests per day Evaluate machine learning systems against business goals Participate in code reviews, design reviews, production on-call support and incident triaging process Write well-crafted, well-tested, readable, maintainable code Experience: B.S., M.S., or Ph.D. in Computer Science or other quantitative fields or related work experience 3+ years of Machine Learning experience Passion for building impactful machine learning models leveraging expertise in one or multiple fields. Proficiency in Python, Golang, or other programming language Excellent communication skills and fluency in English Strong understanding of Machine Learning methodologies, including supervised learning, forecasting, recommendation systems, reinforcement learning, and multi-armed bandits"
S&P Global, Data Engineer, "Responsibilities And Impact Lead the design and implementation of EMR / Spark workloads using Python, including data access from relational databases and cloud storage technologies. Ensure performance, security, and reliability; coordinate with application teams to define database design. Collaborate with cross-functional teams to support data-driven initiatives. Mentor junior team members and promote best practices. Oversee SPARK maintenance and troubleshooting. Drive innovation by evaluating and integrating new technologies. Produce system design documents and participate in technical walkthroughs. Perform application and system performance tuning and troubleshoot performance issues. Effectively interact with global customers, business users, and IT staff. What We're Looking For Basic Required Qualifications: Bachelor's degree in computer science, Information Systems, or Engineering, or equivalent work experience. 8-10 years of IT experience in application support or development. Strong experience in managing EMR workloads including configurations. Experience with SPARK and NoSQL-related database technologies. Experience in working with multi-threaded, high-performance, low-latency messaging systems. Experience in AWS cloud-based technologies. Experience using system tools, source control systems, utilities, and third-party products. Experience with financial applications such as Index/Benchmarks, Asset Management, Portfolio Investment modeling, or trading systems. Excellent communication skills, with strong verbal and writing proficiencies. Additional Preferred Qualifications Proficiency in building data analytics solutions on AWS Cloud. Thorough understanding of replication strategies and implementation of HA protocols for 99.99% uptime. Experience with implementation of data governance and data lineage. Proficiency in implementing data models in SQL and NoSQL databases. Experience with microservice and serverless architecture implementation."
Lumenalta, Data Engineer - BigQuery/GCP - Mid Level, "Key Responsibilities: Data Pipeline Development: Design and build ETL/ELT data pipelines using BigQuery and other GCP services to ingest, process, and transform large datasets from multiple sources. Data Modeling & Architecture: Develop and optimize data models and schemas to support analytics, reporting, and machine learning requirements. Performance Optimization: Implement best practices for performance tuning, partitioning, and clustering to optimize data queries and reduce costs in BigQuery. Data Integration & Transformation: Collaborate with data scientists and analysts to design data solutions that integrate seamlessly with BI tools, machine learning models, and third-party applications. Data Quality & Governance: Establish and enforce data quality standards, data governance frameworks, and security policies for data storage and access on GCP. Automation & Monitoring: Automate workflows using Cloud Composer, Cloud Functions, or other orchestration tools to ensure reliable and scalable data pipelines. Documentation & Knowledge Sharing: Create comprehensive documentation for data pipelines, workflows, and processes. Share best practices and mentor junior data engineers. Required Qualifications: 3+ years of experience working as a Data Engineer, with a focus on GCP and BigQuery. Strong proficiency in SQL and experience in developing complex queries, stored procedures, and views in BigQuery. Hands-on experience with GCP services such as Cloud Storage, Dataflow, Cloud Composer, and Cloud Functions. Understanding of data warehousing concepts, dimensional modeling, and building data marts. Experience with ETL/ELT tools like Apache Beam, Dataflow, or dbt. Familiarity with scripting languages like Bash, Python or JavaScript for automation and integration. Proven ability to work with large datasets and cost-effectively optimize query performance. Excellent communication and interpersonal skills, with the ability to collaborate effectively with cross-functional teams. GCP Professional Data Engineer Certification is a plus. Preferred Skills: Experience with machine learning on GCP using Vertex AI or AI Platform. Knowledge of data governance and security best practices in a cloud environment. Experience working with real-time streaming data and tools like Pub/Sub or Kafka."
Lumenalta, Data Engineer - BigQuery/GCP - Mid Level, "Key Responsibilities: Data Pipeline Development: Design and build ETL/ELT data pipelines using BigQuery and other GCP services to ingest, process, and transform large datasets from multiple sources. Data Modeling & Architecture: Develop and optimize data models and schemas to support analytics, reporting, and machine learning requirements. Performance Optimization: Implement best practices for performance tuning, partitioning, and clustering to optimize data queries and reduce costs in BigQuery. Data Integration & Transformation: Collaborate with data scientists and analysts to design data solutions that integrate seamlessly with BI tools, machine learning models, and third-party applications. Data Quality & Governance: Establish and enforce data quality standards, data governance frameworks, and security policies for data storage and access on GCP. Automation & Monitoring: Automate workflows using Cloud Composer, Cloud Functions, or other orchestration tools to ensure reliable and scalable data pipelines. Documentation & Knowledge Sharing: Create comprehensive documentation for data pipelines, workflows, and processes. Share best practices and mentor junior data engineers. Required Qualifications: 3+ years of experience working as a Data Engineer, with a focus on GCP and BigQuery. Strong proficiency in SQL and experience in developing complex queries, stored procedures, and views in BigQuery. Hands-on experience with GCP services such as Cloud Storage, Dataflow, Cloud Composer, and Cloud Functions. Understanding of data warehousing concepts, dimensional modeling, and building data marts. Experience with ETL/ELT tools like Apache Beam, Dataflow, or dbt. Familiarity with scripting languages like Bash, Python or JavaScript for automation and integration. Proven ability to work with large datasets and cost-effectively optimize query performance. Excellent communication and interpersonal skills, with the ability to collaborate effectively with cross-functional teams. GCP Professional Data Engineer Certification is a plus. Preferred Skills: Experience with machine learning on GCP using Vertex AI or AI Platform. Knowledge of data governance and security best practices in a cloud environment. Experience working with real-time streaming data and tools like Pub/Sub or Kafka."
CGI, Generative AI Developer, "Your future duties and responsibilities Develop, test, and refine prompts for generative AI models to achieve specific outputs or solve specific problems. Experiment with model settings and parameters (e.g., temperature, token limits) to enhance performance. Build automation pipelines for prompt testing and model tuning. Assist in the design, development, and implementation of generative AI models using frameworks like LangChain and OpenAI. Collaborate with cross-functional teams to integrate AI models into applications, leveraging cloud platforms such as Azure OpenAI. Stay updated on the latest advancements in AI, generative models and machine learning technologies. Document best practices and prompt libraries for reuse across projects. Work with teams to align AI solutions with business needs and provide training and support to team members using AI tools. Required Qualifications To Be Successful In This Role Bachelor's or Master's degree in Computer Science, Data Science, or a related field. Equivalent practical experience may also be considered. 2+ years working with AI models, prompt engineering, or NLP solutions. Experience with enterprise web app development Strong programming proficiency with at least one year of experience in Python. Familiarity with LLMs like OpenAI's GPT, Anthropic's Claude, or similar tools. Knowledge of machine learning concepts and model fine-tuning. Strong problem-solving and creative thinking abilities. Effective communication skills to explain AI behavior and constraints. Detail-oriented with the ability to test and document processes."
Bitstrapped, Senior Data Engineer, "Responsibilities Design, implement, automate, and maintain enterprise data systems for our customers Implement data engineering technologies, primarily stream and batch processing systems with Apache Beam/Dataflow, Data Warehouses including BigQuery, queuing and message systems with Pub/Sub, Data Lakes with BigLake and Cloud Storage, application databases including SQL and Postgres, Redis, and Composer DAG-based workflow automation Show an ability to implement data system robustness including caches, clustering, partitioning, event windows, performant and cost efficient data warehouse queries, audit tables, data governance and security including RBAC and CBAC Capable of balancing 1-2 client projects with differing project priorities Write reusable and scalable code, prioritizing a test driven approach where possible Experience working with project plans and milestone based sprints, under the supervision of technical project management and principal architects Hands-on experience with relational and non-relational databases, as well as file types that include Parquet, Avro, csv Strong verbal communication skills Possess a desire to continuously learn and adopt new technologies Possess a desire to work in a fast paced environment Be capable of working independently, staying on task, and remaining productive with little supervision Minimum Requirements 5+ years experience as a data engineer or software engineer in a professional environment 5+ years of experience with data engineering duties including data ingestion, data processing, ETL/ELT, parallel computing, integration of sinks/taps, workflow orchestration Strong knowledge of databases and data storage Ability to architect, develop, and test code at an enterprise level, adhering to software engineering best practices Ability to Design data models with a strong understanding of modeling design patterns. Proficiency in at least one programming language. Has knowledge and experience working on distributed systems (monitoring, alerting, message contracts) Cloud Certifications - GCP Data Engineer certification preferred Bonus Experience in Data Science and Data Science Engineering Experience with the deployment of workloads to the cloud Experience interacting with other cloud infrastructure services including Cloud Functions, Kubernetes, or VertexAI is a nice to have Experience with DevOps CI/CD workflows using technologies such as Github, Gitlab, Jenkins, CircleCI, Bitbucket, and otherDevOps Technologies. Experience writing infrastructure as code using Terraform"
Lumenalta, Data Engineer - BigQuery/GCP - Senior, "Key Responsibilities: Data Pipeline Development: Design and build ETL/ELT data pipelines using BigQuery and other GCP services to ingest, process, and transform large datasets from multiple sources. Data Modeling & Architecture: Develop and optimize data models and schemas to support analytics, reporting, and machine learning requirements. Performance Optimization: Implement best practices for performance tuning, partitioning, and clustering to optimize data queries and reduce costs in BigQuery. Data Integration & Transformation: Collaborate with data scientists and analysts to design data solutions that integrate seamlessly with BI tools, machine learning models, and third-party applications. Data Quality & Governance: Establish and enforce data quality standards, data governance frameworks, and security policies for data storage and access on GCP. Automation & Monitoring: Automate workflows using Cloud Composer, Cloud Functions, or other orchestration tools to ensure reliable and scalable data pipelines. Documentation & Knowledge Sharing: Create comprehensive documentation for data pipelines, workflows, and processes. Share best practices and mentor junior data engineers. Required Qualifications: 5+ years of experience working as a Data Engineer, with a focus on GCP and BigQuery. Strong proficiency in SQL and experience in developing complex queries, stored procedures, and views in BigQuery. Hands-on experience with GCP services such as Cloud Storage, Dataflow, Cloud Composer, and Cloud Functions. Deep understanding of data warehousing concepts, dimensional modeling, and building data marts. Experience with ETL/ELT tools like Apache Beam, Dataflow, or dbt. Familiarity with scripting languages like Bash, Python or JavaScript for automation and integration. Proven ability to work with large datasets and cost-effectively optimize query performance. Excellent communication and interpersonal skills, with the ability to collaborate effectively with cross-functional teams. GCP Professional Data Engineer Certification is a plus. Preferred Skills: Experience with machine learning on GCP using Vertex AI or AI Platform. Knowledge of data governance and security best practices in a cloud environment. Experience working with real-time streaming data and tools like Pub/Sub or Kafka."
Affirm, Machine Learning Engineer II, "What You'll Do Use Affirm's proprietary and other third party data to develop machine learning models that predict the likelihood of fraud. These models will protect victims' identities from being stolen, prevent Affirm from incurring financial loss, and increase the trust that consumers and partners have in the Affirm ecosystem. Partner with the ML platform team to build fraud specific ML infrastructure Research ground breaking solutions and develop prototypes that drive the future of fraud decisioning at Affirm Implement and scale data pipelines, new features, and algorithms that are essential to our production models Collaborate with the engineering, fraud, and product teams to define requirements for new products Develop fraud models to maximize user conversion while minimizing fraud losses and data costs. What We Look For 2+ years of experience as a machine learning engineer or PhD in a relevant field Proficiency in machine learning with experience in areas such as gradient boosting, online learning, and deep learning. Domain knowledge in fraud risk is a plus Strong programming skills in Python Experience using large scale distributed systems like Spark and Ray Experience using machine learning frameworks such as scikit-learn, pandas, numpy, xgboost, and pytorch Excellent written and oral communication skills and the capability to drive cross-functional requirements with product and engineering teams The ability to present technical concepts and results in an audience-appropriate way"
Affirm, Machine Learning Engineer II, "What You'll Do Use Affirm's proprietary and other third party data to develop machine learning models that predict the likelihood of fraud. These models will protect victims' identities from being stolen, prevent Affirm from incurring financial loss, and increase the trust that consumers and partners have in the Affirm ecosystem. Partner with the ML platform team to build fraud specific ML infrastructure Research ground breaking solutions and develop prototypes that drive the future of fraud decisioning at Affirm Implement and scale data pipelines, new features, and algorithms that are essential to our production models Collaborate with the engineering, fraud, and product teams to define requirements for new products Develop fraud models to maximize user conversion while minimizing fraud losses and data costs. What We Look For 2+ years of experience as a machine learning engineer or PhD in a relevant field Proficiency in machine learning with experience in areas such as gradient boosting, online learning, and deep learning. Domain knowledge in fraud risk is a plus Strong programming skills in Python Experience using large scale distributed systems like Spark and Ray Experience using machine learning frameworks such as scikit-learn, pandas, numpy, xgboost, and pytorch Excellent written and oral communication skills and the capability to drive cross-functional requirements with product and engineering teams The ability to present technical concepts and results in an audience-appropriate way"
Affirm, Machine Learning Engineer II, "What You'll Do Use Affirm's proprietary and other third party data to develop machine learning models that predict the likelihood of fraud. These models will protect victims' identities from being stolen, prevent Affirm from incurring financial loss, and increase the trust that consumers and partners have in the Affirm ecosystem. Partner with the ML platform team to build fraud specific ML infrastructure Research ground breaking solutions and develop prototypes that drive the future of fraud decisioning at Affirm Implement and scale data pipelines, new features, and algorithms that are essential to our production models Collaborate with the engineering, fraud, and product teams to define requirements for new products Develop fraud models to maximize user conversion while minimizing fraud losses and data costs. What We Look For 2+ years of experience as a machine learning engineer or PhD in a relevant field Proficiency in machine learning with experience in areas such as gradient boosting, online learning, and deep learning. Domain knowledge in fraud risk is a plus Strong programming skills in Python Experience using large scale distributed systems like Spark and Ray Experience using machine learning frameworks such as scikit-learn, pandas, numpy, xgboost, and pytorch Excellent written and oral communication skills and the capability to drive cross-functional requirements with product and engineering teams The ability to present technical concepts and results in an audience-appropriate way"
VLink Inc, Machine Learning Developer, "Responsibilities: Lead the design, development, and deployment of advanced machine learning models and algorithms to solve complex business problems. Working with a team to build digital applications to integrate with ML solution solutions to the users. Mentor and guide junior engineers, providing technical expertise and leadership in ML projects. Collaborate with cross-functional teams to integrate ML solutions into various product features. Work with large datasets to identify trends, patterns, and insights that drive decision-making. Implement and optimize ML models for high availability, scalability, and performance in production environments. Stay up to date with the latest advancements in AI/ML technologies and apply them to improve our systems. Participate in continuous learning and knowledge sharing to ensure the team is always at the cutting edge of technology. Conduct code reviews, participate in technical design sessions, and provide mentorship to junior engineers. Troubleshoot and resolve issues related to ML models and data pipelines. Communicate complex technical concepts to non-technical stakeholders in a clear and concise manner. Develop and maintain comprehensive documentation for ML models and processes. Provide strategic input into project planning and execution, ensuring alignment with business goals. Required skills: Extensive experience in software development with at least 5-7 years of hands-on expertise in machine learning. Proven leadership skills with the ability to mentor and guide junior engineers. Proficiency in ML frameworks and libraries such as TensorFlow, PyTorch, and scikit-learn. Strong programming skills & experience writing clean code in languages like Python, Java, or similar. 1-2 Experience in Building digital applications using Django or SpringBoot frameworks Experience with data handling, preprocessing, and pipeline development, including proficiency in SQL/NoSQL databases. Knowledge of AI technologies, including Gen AI, LLMs, and semantic search. Strong understanding of security architecture and data privacy regulations. Excellent problem-solving skills and a passion for continuous learning and innovation. Effective communication skills, capable of presenting technical concepts to non-technical audiences. Ability to work collaboratively in a cross-functional team environment. Open-minded, receptive to new ideas, and always looking for a better way to do something. A passion for simplifying and automating work, making things better, continuous learning, open-ended problems, efficiency and helping others Knowledgeable about containers and orchestration (e.g. Docker, Kubernetes)"
Braze, Senior Data Engineer, "Responsibilities Lead the design, implementation, and monitoring of scalable data pipelines and architectures using tools like Snowflake and dbt Develop and maintain robust ETL processes to ensure high-quality data ingestion, transformation, and storage Collaborate closely with data scientists, analysts, and other engineers to design and implement data solutions that drive customer engagement and retention Optimize and manage data flows and integrations across various platforms and applications Ensure data quality, consistency, and governance by implementing best practices and monitoring systems Work extensively with large-scale event-level data, aggregating and processing it to support business intelligence and analytics Implement and maintain data products using advanced techniques and tools Collaborate with cross-functional teams including engineering, product management, sales, marketing, and customer success to deliver valuable data solutions Continuously evaluate and integrate new data technologies and tools to enhance our data infrastructure and capabilities Who You Are The ideal candidate for this role possesses: 5+ years of hands-on experience in data engineering, cloud data warehouses, and ETL development, preferably in a customer-facing environment Proven expertise in designing and optimizing data pipelines and architectures Strong proficiency in advanced SQL and data modeling techniques A track record of leading impactful data projects from conception to deployment Effective collaboration skills with cross-functional teams and stakeholders In-depth understanding of technical architecture and data flow in a cloud-based environment Ability to mentor and guide junior team members on best practices for data engineering and development Passion for building scalable data solutions that enhance customer experiences and drive business growth Strong analytical and problem-solving skills, with a keen eye for detail and accuracy Extensive experience working with and aggregating large event-level data Familiarity with data governance principles and ensuring compliance with industry regulations Prefer, but don't require, experience with Kubernetes for container orchestration and Airflow for workflow management"
AMD, Deep Learning Library GPU Software Developer - Performance, "Key Responsibilities The ideal candidate will be responsible for writing high-performance GPU kernels for AMD's Machine Learning and Deep Learning Library: MIOpen (https://github.com/ROCm/MIOpen) They will be porting and optimizing algorithms for new GPU hardware. Perform code reviews, build unit tests, author detailed documentation related to their work, and work with on-site and offshore teams to deliver the software solutions on schedule. They will play a key role in all phases of the software development including system requirements analysis, coordinating feature design and development across functional and organizational boundaries. Preferred Experience Strong programming skills in Python and C/C++. Experience with Hyper-V, and performance optimization techniques of virtualized environment for GPU computing is preferred. Experience or knowledge about deep learning primitives, fusion, and inference optimization is preferred. Experience using version control software such as Git. Strong understanding of Linux internals, Servers, and Debugging. Basic knowledge of software development lifecycle, SW practices including debugging, test, revision control, documentation, and bug tracking. Good teamwork and interpersonal skills required. Ability to work independently and within complementary teams. Demonstrate flexibility, strong motivation, and a proven track record of meeting results-oriented deadlines. Knowledge with deep neural network machine learning technologies and modern machine learning programming frameworks. Experience working with and developing virtualization containers and package managers for code deployment. Academic Credentials Bachelor's or Master's in Computer Science, Computer Engineering, or related subjects, or equivalent experience"
Amazon, Data Engineer Forecasting, "As a data engineer, you will need to design and develop high scalable ETLs with EMR based applications as well supporting them on Glue ETL or Redshift. Knowledge of big data architecture and design is a must. Key job responsibilities Create scalable data solutions using AWS and/or Amazon internal tools. Create and maintain business logic for data transformation and ingestion pipelines. Create and maintain datasets in Amazon data lake and internal data management systems using S3/Glue. Data investigation and analysis to understand the impact of changes on downstream customers' use cases and build transformation logic to ingest data for Forecasting data analysis. Provide quality data for Amazon downs team data consumers. A day in the life We not only provide data to machine learning models but also to our direct customers in BI and Data Science. We help them to get access to the data which they need for data analysis, working with them to address data quality issues and resolve data requirements for experiments, training and back testing models as well as data required for generating the forecast on regular basis. Basic Qualifications 3+ years of data engineering experience Bachelor's degree Knowledge of distributed systems as it pertains to data storage and computing Experience with SQL Preferred Qualifications Master's degree Knowledge of batch and streaming data architectures like Kafka, Kinesis, Flink, Storm, Beam"
Avance Consulting, Senior Data Engineer, "Job Description : 8+ years of experience as a Data Engineer with AWS, Glue, Lambda, SQL, Python, Redshift. Must have working knowledge in designing and implementing data pipelines on any of the cloud providers (AWS is preferred). Must be able to work with large volumes of data coming from various sources. Perform data cleansing, data validation, etc. Hands-on ETL developer who is good at Python, SQL. AWS services like glue, glue crawlers, lambda, red shift, athena, s3, EC2, IAM, Monitoring and Logging mechanisms- AWS cloudwatch, setting up alerts. Deployment knowledge on the cloud. Integrate CI/CD pipeline to build artifacts and deploy changes to higher Environments. Scheduling frameworks Airflow, AWS Step functions Excellent Communication skills, should be able to work collaboratively with other teams"
Lyft, Senior Data Engineer, "Responsibilities: Own the data architecture and pipelines that support the operation and reporting of Lyft's growth campaigns. Evolve data models and schemas in response to business needs and engineering best practices Coordinate with cross-functional partners to understand business problems, align on project prioritization, and determine solutions that balance speed versus long-term functionality Assess and improve systems tracking data quality and consistency Implement and maintain tools for self-service data pipeline management (ETL) Conduct advanced performance tuning for SQL and MapReduce jobs to improve data processing performance Experience: 5+ years relevant professional experience Experience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet) Proficient in at least one of the SQL languages (MySQL, PostgreSQL, SqlServer, Oracle) Good understanding of SQL Engine and able to conduct advanced performance tuning Strong skills in scripting language (Python, Ruby, Bash) 4+ years of experience with workflow management tools (Airflow, Oozie, Azkaban, UC4) Comfortable working cross-functionally to bridge Lyft's business goals with data engineering Strong communication and problem-solving skills."
Lumenalta, Data Engineer - Snowflake, "Requirements 3+ years experience in a senior developer role using Python; ideally, you have delivered business-critical software to large enterprises You are comfortable manipulating large data sets and handling raw SQL Experience using technologies such as Snowflake, AWS, and ETL pipelines is essential. Have extensive experience with data warehousing and working with scalability of large volumes of structured data Financial Services industry experience preferred English fluency, verbal and written Personality traits: Professional, problem solver, proactive, passionate, team player."
Weir Motion Metrics, Intermediate Machine Learning Developer, "Key Responsibilities: Design, implement and optimize proprietary machine learning algorithms based on runtime system requirements and constraints. Collect, sanity-check and preprocess large image datasets to ensure data quality and suitability for model training. Breakdown, implement, and supervise machine learning development lifecycle for assigned projects. Conduct code reviews and work closely with the software/hardware/cloud teams to deliver production grade code/algorithm. Collaborate with cross-functional teams, including data scientists, software engineers, and product managers, to define project requirements and deliver solutions that meet business goals. Take ownership of AI project development including improvement roadmap and monitoring strategy Safety First: Demonstrate 100% commitment to our zero harm behaviors in support of our drive towards developing a world class safety culture. Job Knowledge/Education and Qualifications: Master's Degree or higher in Computer Science or a related technology/engineering field. 3+ years of hands-on experience in developing and implementing and evaluating machine learning models and algorithms. In-depth knowledge of machine learning data structures and modeling, software architecture, libraries, and frameworks to create ML models that accomplish outlined goals. Solid understanding of statistical learning algorithms, including (self, semi) supervised, unsupervised and transfer learning and computer vision algorithms. Proficiency in Python development and version control. Basic familiarity with object- and service-oriented design, containerization, and complexity analysis. Solid Experience with TensorFlow, PyTorch and OpenCV. Familiarity with image processing algorithms and Reinforcement Learning (RL). Strong analytical, communication and presentation skills."
Autodesk, Machine Learning Ops Developer, "Responsibilities Collaborate with other engineers to develop scalable data pipelines and architectures with a focus on MLOps best practices for large language models (LLMs) Support tasks related to data collection, data analysis, content understanding, storage and processing Write code for model training, testing, and deployment Monitor and troubleshoot machine learning models to ensure accuracy and performance Perform requirements analysis, working with team members of different levels and documenting solutions Work with large-scale data sets and manage data flow between systems Organize and process large batches of text and geometric data Communicate your findings through quantitative data analysis and qualitative visuals and insights Minimum Qualifications MS in Machine Learning, Artificial Intelligence, Mathematics, Statistics, Computer Science, or a related field 3+ years of experience in machine learning engineering or related field Domain expertise in training deep neural nets, such as CNN and transformers and proficiency in least one deep learning framework, for example PyTorch, TensorFlow Experience with LLMs and related technologies, including frameworks, embedding models, vector databases, and Retrieval-Augmented Generation (RAG) systems Experience with data modeling, architecture, and processing using varied data representations including 2D/3D geometry Proficient in AWS cloud services and leveraging the SageMaker Studio platform for scalable data processing and model development Good understanding of fundamental CS algorithms and their scaling behaviors Excellent coding skills covering procedural as well as data-analytics oriented languages (such as Python) Ability to translate theoretical concepts into practical solutions and prototype implementations Preferred Qualifications Background in Architecture, Engineering, or Construction Practical experience in data preparation, hyper-parameter selection; acceleration techniques; and optimization methods Experience in parallel distribution of algorithms using platforms such as Spark or Hadoop Practical experience in developing high scale machine learning algorithms"
FICO, Staff AI Engineer, "What You'll Contribute Design and implement cutting-edge LLM-powered solutions for decision automation, fraud investigation, and process automation within FICO's platform. Develop sophisticated prompting strategies and Retrieval-Augmented Generation (RAG) architectures tailored to high-stakes applications. Apply advanced fine-tuning, in-context learning techniques, and custom evaluation frameworks to optimize AI model performance. Build robust model evaluation and governance systems, ensuring compliance and reliability across applications. Design and execute comprehensive backtesting methodologies to validate model predictions and performance. Implement A/B testing frameworks to evaluate the impact of various AI interventions on system performance. Monitor model performance and detect drift to maintain high accuracy and relevance over time. Research and integrate the latest AI techniques to keep FICO's platform at the forefront of innovation. Evaluate new models, architectures, and training approaches, experimenting with emerging technologies to enhance platform capabilities. Develop custom RAG implementations to enhance information retrieval within decision processes. Write production-quality code, implementing machine learning systems with attention to scalability, reliability, and security. Design scalable architectures that support FICO's analytics and decisioning solutions. Implement advanced monitoring techniques and system reliability checks to maintain operational excellence. What We're Seeking 5+ years of industry experience in software, ML or data engineering, with a history of managing complex, large-scale projects and driving strategic technical decisions. Proficiency in Python, Go, Java, or similar languages. Experience with PyTorch, JAX or similar deep learning frameworks. Proficiency in fine-tuning, model adaptation, and customization. Solid foundation in machine learning evaluation and testing methodologies, including the development of custom evaluation frameworks. Knowledge of backtesting methodologies to ensure reliability and robustness of model predictions. Proven expertise in applying modern Large Language Models (LLMs) and their applications to real-world problems preferred. Strong skills in prompt engineering and chain-of-thought prompting techniques. Experience with Retrieval-Augmented Generation (RAG) architectures and familiarity with vector databases preferred Experience with A/B testing frameworks. Expertise in model monitoring, drift detection, and governance to maintain compliance and operational integrity. Familiarity with compliance and regulatory technology, particularly in AI-driven decisioning environments Strong software engineering fundamentals, with experience in writing production-quality code and designing scalable architectures. Proven ability to mentor teams and collaborate across departments to deliver high-impact solutions. Experience in fast-paced environments, with an ability to adapt and drive impact in high-growth, high-stakes settings. Publications, patents, or open-source contributions that showcase thought leadership and technical expertise in AI and machine learning. Bachelor's or master's degree in computer science, or a related field with focus in Machine Learning."
GPTZero, Senior Machine Learning Engineer, "Responsibilities Design, train, and fine-tune state-of-the-art language models Develop AI agents combined with retrieval-augmented language models Build efficient and scalable ML training and inference systems Stay up-to-date with the latest literature and emerging technologies to solve novel problems Work closely with product and design teams to develop intuitive applications that create societal impact Qualifications 4+ YOE in PyTorch/Transformers 1st author in paper at top-tier conference or led a significant ML project in industry Experience pushing the cutting-edge in machine learning Self-starter (pitch, plan, and implement as a project owner in a fast-paced team) Highly motivated to make positive societal impact Ability to wear multiple hats and be a leader as our team grows Bonus: strong open-source portfolio publications at top-tier ML venues experience working in an early-stage startup environment understanding of how machine learning models fail in the wild"
Lumenalta, Data Engineer - BigQuery/GCP - Tech Lead, "Key Responsibilities: Data Pipeline Development: Design and build ETL/ELT data pipelines using BigQuery and other GCP services to ingest, process, and transform large datasets from multiple sources. Data Modeling & Architecture: Develop and optimize data models and schemas to support analytics, reporting, and machine learning requirements. Performance Optimization: Implement best practices for performance tuning, partitioning, and clustering to optimize data queries and reduce costs in BigQuery. Data Integration & Transformation: Collaborate with data scientists and analysts to design data solutions that integrate seamlessly with BI tools, machine learning models, and third-party applications. Data Quality & Governance: Establish and enforce data quality standards, data governance frameworks, and security policies for data storage and access on GCP. Automation & Monitoring: Automate workflows using Cloud Composer, Cloud Functions, or other orchestration tools to ensure reliable and scalable data pipelines. Documentation & Knowledge Sharing: Create comprehensive documentation for data pipelines, workflows, and processes. Share best practices and mentor junior data engineers. Required Qualifications: 7+ years of experience working as a Data Engineer, with a focus on GCP and BigQuery. Strong proficiency in SQL and experience in developing complex queries, stored procedures, and views in BigQuery. Hands-on experience with GCP services such as Cloud Storage, Dataflow, Cloud Composer, and Cloud Functions. Deep understanding of data warehousing concepts, dimensional modeling, and building data marts. Experience with ETL/ELT tools like Apache Beam, Dataflow, or dbt. Familiarity with scripting languages like Bash, Python or JavaScript for automation and integration. Proven track record managing teams and projects Proven ability to work with large datasets and cost-effectively optimize query performance. Excellent communication and interpersonal skills, with the ability to collaborate effectively with cross-functional teams. GCP Professional Data Engineer Certification is a plus. Preferred Skills: Experience with machine learning on GCP using Vertex AI or AI Platform. Knowledge of data governance and security best practices in a cloud environment. Experience working with real-time streaming data and tools like Pub/Sub or Kafka."
Lumenalta, Data Engineer - BigQuery/GCP - Tech Lead, "Key Responsibilities: Data Pipeline Development: Design and build ETL/ELT data pipelines using BigQuery and other GCP services to ingest, process, and transform large datasets from multiple sources. Data Modeling & Architecture: Develop and optimize data models and schemas to support analytics, reporting, and machine learning requirements. Performance Optimization: Implement best practices for performance tuning, partitioning, and clustering to optimize data queries and reduce costs in BigQuery. Data Integration & Transformation: Collaborate with data scientists and analysts to design data solutions that integrate seamlessly with BI tools, machine learning models, and third-party applications. Data Quality & Governance: Establish and enforce data quality standards, data governance frameworks, and security policies for data storage and access on GCP. Automation & Monitoring: Automate workflows using Cloud Composer, Cloud Functions, or other orchestration tools to ensure reliable and scalable data pipelines. Documentation & Knowledge Sharing: Create comprehensive documentation for data pipelines, workflows, and processes. Share best practices and mentor junior data engineers. Required Qualifications: 7+ years of experience working as a Data Engineer, with a focus on GCP and BigQuery. Strong proficiency in SQL and experience in developing complex queries, stored procedures, and views in BigQuery. Hands-on experience with GCP services such as Cloud Storage, Dataflow, Cloud Composer, and Cloud Functions. Deep understanding of data warehousing concepts, dimensional modeling, and building data marts. Experience with ETL/ELT tools like Apache Beam, Dataflow, or dbt. Familiarity with scripting languages like Bash, Python or JavaScript for automation and integration. Proven track record managing teams and projects Proven ability to work with large datasets and cost-effectively optimize query performance. Excellent communication and interpersonal skills, with the ability to collaborate effectively with cross-functional teams. GCP Professional Data Engineer Certification is a plus. Preferred Skills: Experience with machine learning on GCP using Vertex AI or AI Platform. Knowledge of data governance and security best practices in a cloud environment. Experience working with real-time streaming data and tools like Pub/Sub or Kafka."
E-Solutions, Data Engineer, "Ideal Candidate: An undergraduate or Master's degree in Computer Science or equivalent engineering experience 6+ years of professional software engineering and programming experience (Java, Python) with a focus on designing and developing complex data-intensive applications 3+ years of architecture and design (patterns, reliability, scalability, quality) of complex systems Advanced coding skills and practices (concurrency, distributed systems, functional principles, performance optimization) Professional experience working in an agile environment Strong analytical and problem-solving ability Strong written and verbal communication skills Experience in operating and maintaining production-grade software Comfortable with tackling very loosely defined problems and thrive when working on a team which has autonomy in their day to day decisions Preferred Skills In-depth knowledge of software and data engineering best practices Experience in mentoring and leading junior engineers Experience in serving as the technical lead for complex software development projects Experience with large-scale distributed data technologies and tools Strong experience with multiple database models ( relational, document, in-memory, search, etc ) Strong experience with Data Streaming Architecture ( Kafka, Spark, Airflow, SQL, NoSQL, CDC, etc ) Strong knowledge of cloud data platforms and technologies such as GCS, BigQuery, Cloud Composer, Pub/Sub, Dataflow, Dataproc, Looker, and other cloud-native offerings Strong Knowledge of Infrastructure as Code (IaC) and associated tools (Terraform, ansible etc) Experience pulling data from a variety of data source types including Mainframe (EBCDIC), Fixed Length and delimited files, databases (SQL, NoSQL, Time-series) Strong coding skills for analytics and data engineering (Java, Python, and Scala) Experience performing analysis with large datasets in a cloud-based environment, preferably with an understanding of Google's Cloud Platform (GCP) Understands how to translate business requirements to technical architectures and designs Comfortable communicating with various stakeholders (technical and non-technical) Experience with Airflow and Spark: Airflow: Proven experience in using Apache Airflow for orchestrating and scheduling workflows. Ability to design, implement, and manage complex data pipelines. Understanding of DAGs (also how to dynamically create them), task dependencies, and error handling within Airflow. Spark: Hands-on experience with Apache Spark for large-scale data processing and analytics. Proficiency in writing Spark jobs in Java (PySpak also fine as we're moving in that direction). Also, contains the ability to optimizie performance, and handling data transformations and aggregations at scale. Familiarity with GCP Services: BigQuery: Experience with Google BigQuery for running SQL queries on large datasets, optimizing queries for performance, and in general managing data warehousing solutions. Composer: Knowledge of Google Cloud Composer for managing and orchestrating workflows. Dataproc: Experience with Dataproc for managing and scaling Spark clusters, including configuring clusters, running jobs, and integrating with other GCP services. Proficiency in Python, Java, and SQL: Python: Strong foundation in Python, with experience in writing clean, efficient code and utilizing libraries such as Pandas and NumPy for data manipulation. Proficient in debugging, testing, and using Python for API interactions and external service integration. Java: Proficiency in Java, especially for integrating with data processing frameworks. Experience with Java-based libraries and tools relevant to data engineering is a plus. SQL: Experience in writing and optimizing complex SQL queries for data extraction, transformation, and analysis. Knowledge of Terraform (Optional but Preferred): Terraform: Familiarity with Terraform to automate the provisioning and management of cloud resources. Ability to write and maintain Terraform scripts to define and deploy GCP resources, ensuring infrastructure consistency and scalability."
Inworld AI, Staff Machine Learning Engineer, "Qualifications BA/BS degree or higher in Computer Science, Engineering, or a similar technical field. 6 years of experience with software development in one or more programming languages. 4 years of experience with applying machine learning algorithms in natural language processing domains. 1+ years of experience training LLMs (6B parameters and larger) such as NeoX, LLaMA, PaLM, etc. is considered a big plus. Deep knowledge of machine learning frameworks such as PyTorch, TensorFlow, or JAX. Responsibilities Explore and experiment with cutting edge ML techniques for applied NLP. Develop and test production-grade scalable generative machine learning models."
Lumenalta, Data Engineer - Databricks - Senior, "Key Responsibilities: Data Pipeline Development: Design and build ETL/ELT data pipelines using BigQuery and other GCP services to ingest, process, and transform large datasets from multiple sources. Data Modeling & Architecture: Develop and optimize data models and schemas to support analytics, reporting, and machine learning requirements. Performance Optimization: Implement best practices for performance tuning, partitioning, and clustering to optimize data queries and reduce costs in BigQuery. Data Integration & Transformation: Collaborate with data scientists and analysts to design data solutions that integrate seamlessly with BI tools, machine learning models, and third-party applications. Data Quality & Governance: Establish and enforce data quality standards, data governance frameworks, and security policies for data storage and access on GCP. Automation & Monitoring: Automate workflows using Cloud Composer, Cloud Functions, or other orchestration tools to ensure reliable and scalable data pipelines. Documentation & Knowledge Sharing: Create comprehensive documentation for data pipelines, workflows, and processes. Share best practices and mentor junior data engineers. Required Qualifications: 7+ years of experience working as a Data Engineer, with a focus on GCP and BigQuery. Strong proficiency in SQL and experience in developing complex queries, stored procedures, and views in BigQuery. Hands-on experience with GCP services such as Cloud Storage, Dataflow, Cloud Composer, and Cloud Functions. Deep understanding of data warehousing concepts, dimensional modeling, and building data marts. Experience with ETL/ELT tools like Apache Beam, Dataflow, or dbt. Familiarity with scripting languages like Bash, Python or JavaScript for automation and integration. Proven track record managing teams and projects Proven ability to work with large datasets and cost-effectively optimize query performance. Excellent communication and interpersonal skills, with the ability to collaborate effectively with cross-functional teams. GCP Professional Data Engineer Certification is a plus. Preferred Skills: Experience with machine learning on GCP using Vertex AI or AI Platform. Knowledge of data governance and security best practices in a cloud environment. Experience working with real-time streaming data and tools like Pub/Sub or Kafka."
Lumenalta, Data Engineer - Snowflake - Tech Lead, "Requirements 10+ years experience in a senior data engineering role using Python; ideally, you have delivered business-critical software to large enterprises You are comfortable manipulating large data sets and handling raw SQL Experience using technologies such as Snowflake, AWS, and ETL pipelines is essential. Have extensive experience with data warehousing and working with scalability of large volumes of structured data Proven track record managing teams and projects Personality traits: Professional, problem solver, proactive, passionate, team player."
Synechron, Senior Data Warehouse Engineer, "Responsibilities: Strong understanding or Snowflake on Azure Architecture, design, implementation and operationalization of large-scale data and analytics solutions on Snowflake Cloud Data Warehouse. Hands-on development experience with Snowflake features such as Snow SQL; Snow Pipe; Python; Tasks; Streams; Time travel; Zero Copy Cloning; Optimizer; Metadata Manager; data sharing; and stored procedures. Experience in Data warehousing - OLTP, OLAP, Dimensions, Facts, and Data modeling. Need to have working knowledge of MS Azure configuration items with respect to Snowflake. Developing EL pipelines in and out of data warehouse using combination of Data bricks, Python and Snow SQL. Developing scripts UNIX, Python etc. to Extract, Load and Transform data, as well as other utility functions. Provide production support for Data Warehouse issues such data load problems, transformation translation problems Requirements: You are: Minimum 10 years of designing and implementing an operational production grade large-scale data solution on Microsoft Azure Snowflake Data Warehouse. Including hands on experience with productionized data ingestion and processing pipelines using Python, Data bricks, Snow SQL Excellent understanding of Snowflake Internals and integration of Snowflake with other data processing and reporting technologies"
Zalando, Senior Principal Data Engineer, "We'd love to meet you if : You have at least 10 years of experience as an individual contributor data engineer, with demonstrated impact on a senior principal level (at Zalando, this is 2 steps beyond senior level on the individual contributor track) You bring a clear, audience-sensitive style to your verbal and written communication You have extensive experience with at least 2 of the following technologies AWS, GCP, Spark, and/or Databricks You are proactive, initiating process and technical improvements on your own, winning advocates along the way, with a high sense of ownership of the features built and supported by the team You can assess the severity/impact of bugs quickly to help set priority on the team You deliver or influence to ensure the quality of operational artifacts (e.g. post-mortems and playbooks) on the team and ensure the right software/engineering design approaches and methodologies are applied You understand requirements (performance, data, feature ownership) between different systems and can translate this into technical designs, with probing analysis of the trade-offs between different approaches"
Autodesk, Machine Learning Developer, "Responsibilities Collaborate on projects at the intersection of research and product with a diverse, global team of researchers and developer Support research through the construction of ML pipelines, prototypes, and reusable, testable code Process data and analyze feature extractions Analyze errors and provide solutions to problems Present results to collaborators and leadership Minimum Qualifications BSc or MSc in Computer Science, or equivalent industry experience 3+ years of machine learning experience Experience scaling machine learning training and data pipelines Experience with version control, reproducibility, and deploying machine learning models Experience with cloud services and architectures (e.g. AWS, Azure) Proficiency with modern deep learning libraries and frameworks (PyTorch, Lightning, Ray) Excellent written documentation skills to document code, architectures, and experiments Preferred Qualifications Experience working with distributed systems"
Quantexa, Principal Data Engineer, "What you'll be doing. You will lead, support, and mentor a team of junior data engineers within an agile framework. You set medium term strategy for the technical delivery of delivery projects, seeking to anticipate challenges, risks and issues You operate with a high level of autonomy, seeking guidance and support from others as required Write defensive, fault tolerant and efficient code for production level data processing systems. You'll configure and deploy Quantexa's software using tools such as Spark, Hadoop, Scala, Elasticsearch, with our platform being hosted on both private and public virtual clouds, such as Google cloud, Microsoft Azure and Amazon. You will be a trusted source of knowledge to your clients. You will articulate technical concepts to a non-technical audience helping them to make big decisions. Collaborate with both our solution architects and our R&D engineers to champion solutions and standards for complex big data challenges. You proactively promote knowledge sharing and ensure best practice is followed. This role will require travel to client offices in Ottawa as needed. What you'll bring. You are an accomplished technical lead with a track record of delivering complex or mission critical projects, with at least eight years of industry experience in a data engineering role or engineering equivalent, and at least five years of experience acting in a lead role within a data or software engineering team across a range of projects. Proficiency in Scala, Java, Python, or a programming language associated with data engineering. Our primary language is Scala, but don't worry if that's not currently your strongest language, we believe that strong engineering principles are universal and transferrable. You'll have expertise building and deploying production level data processing batch systems maintained by application support teams. Importantly you'll share an appreciation of what makes a high quality, operationally stable system and how to streamline all areas of development, release, and operations to achieve this. Experience with a variety of modern development tooling (e.g. Git, Gradle, Nexus) and technologies supporting automation and DevOps (e.g. Jenkins, Docker and a little bit of good old Bash scripting). You'll be familiar with developing within a version-controlled process that regularly makes use of these tools and technologies. A strong technical communication ability with demonstrable experience of working in rapidly changing client environments. Knowledge of testing libraries of common programming languages (such as ScalaTest or equivalent). Knows the difference between different test types (unit test, integration test) and can cite specific examples of what they have written themselves."
Everstream Analytics, Data Scientist, "What you'll be doing. You will lead, support, and mentor a team of junior data engineers within an agile framework. You set medium term strategy for the technical delivery of delivery projects, seeking to anticipate challenges, risks and issues You operate with a high level of autonomy, seeking guidance and support from others as required Write defensive, fault tolerant and efficient code for production level data processing systems. You'll configure and deploy Quantexa's software using tools such as Spark, Hadoop, Scala, Elasticsearch, with our platform being hosted on both private and public virtual clouds, such as Google cloud, Microsoft Azure and Amazon. You will be a trusted source of knowledge to your clients. You will articulate technical concepts to a non-technical audience helping them to make big decisions. Collaborate with both our solution architects and our R&D engineers to champion solutions and standards for complex big data challenges. You proactively promote knowledge sharing and ensure best practice is followed. This role will require travel to client offices in Ottawa as needed. What you'll bring. You are an accomplished technical lead with a track record of delivering complex or mission critical projects, with at least eight years of industry experience in a data engineering role or engineering equivalent, and at least five years of experience acting in a lead role within a data or software engineering team across a range of projects. Proficiency in Scala, Java, Python, or a programming language associated with data engineering. Our primary language is Scala, but don't worry if that's not currently your strongest language, we believe that strong engineering principles are universal and transferrable. You'll have expertise building and deploying production level data processing batch systems maintained by application support teams. Importantly you'll share an appreciation of what makes a high quality, operationally stable system and how to streamline all areas of development, release, and operations to achieve this. Experience with a variety of modern development tooling (e.g. Git, Gradle, Nexus) and technologies supporting automation and DevOps (e.g. Jenkins, Docker and a little bit of good old Bash scripting). You'll be familiar with developing within a version-controlled process that regularly makes use of these tools and technologies. A strong technical communication ability with demonstrable experience of working in rapidly changing client environments. Knowledge of testing libraries of common programming languages (such as ScalaTest or equivalent). Knows the difference between different test types (unit test, integration test) and can cite specific examples of what they have written themselves."
DarkVision, Machine Learning Scientist, "What You Will Do Research: Rapid model prototyping, training, and deployment of state-of-the-art deep learning models to solve large-scale industrial problems. You will be investigating image classification, object detection, instance segmentation, semantic segmentation, anomaly detection, and other bespoke tasks for processing industrial ultrasonic array data. Pipeline: Work with our ML Ops team to build cloud-based pipelines for large-scale image pre-processing, data-augmentation, training, post-processing, and inference. Deploy: Work with the Software Development Team to deploy model in production for its end-users. Monitor: Continuously monitor success matrices of the assigned ML project and make continuous improvements for increasing model robustness and efficiency. Data Analysis: Work closely with our Data Analysts to explore, analyze, and organize data; perform data extraction and preprocessing for training and evaluation purposes. Document: Document model architecture, training details, dataset extraction, and cleaning procedures for reproducibility, product management, and internal training. Opportunities to Learn Applying deep learning to real world industrial datasets containing billions of images. Exploration of the state-of-the-art methodologies in computer vision like self-supervised learning and unsupervised learning for the purpose of non-destructive testing. Exposure to full ML lifecycle and orchestration management tools like Prefect, Kubeflow, AWS SageMaker, Weights, and Biases. Converting ideas into intellectual property assets (e.g. patents). Who You Are (Basic Qualifications) Experience developing and training deep learning models from scratch. Master's degree in Computer Science, Computer Engineering, Electrical Engineering or a ML/AI related field; PhD preferred. Proficient in Python and at least one mainstream deep learning framework, such as PyTorch, TensorFlow, JAX, etc. What Will Put You Ahead Experience with medical or industrial ultrasound images for deep learning-related projects. Experience in deep learning projects for industrial non-destructive testing. Experience leading and mentoring ML engineers and ML scientists. Hands-on experience with ML lifecycle management tools like MLFlow, Amazon SageMaker, GPC Vertex AI, etc. Hands-on experience with orchestration tools like Kubeflow, Prefect, Airflow, etc. Working knowledge of Git, Docker, and cloud services like AWS and GPC. Publications in top tier venues likes CVPR, ICCV, etc. Experience with Reinforcement Learning and/or Few-shot Learning Experience with NLP (Transformers, RNNs) Experience in self-supervised and/or unsupervised learning Experience with Generative Model / Diffusion Experience with Causal Inference Great communicator with excellent data presentation, and report writing skills. Be able to work in a fast-paced, self-driven environment."
DarkVision, Machine Learning Scientist, "What You Will Do Research: Rapid model prototyping, training, and deployment of state-of-the-art deep learning models to solve large-scale industrial problems. You will be investigating image classification, object detection, instance segmentation, semantic segmentation, anomaly detection, and other bespoke tasks for processing industrial ultrasonic array data. Pipeline: Work with our ML Ops team to build cloud-based pipelines for large-scale image pre-processing, data-augmentation, training, post-processing, and inference. Deploy: Work with the Software Development Team to deploy model in production for its end-users. Monitor: Continuously monitor success matrices of the assigned ML project and make continuous improvements for increasing model robustness and efficiency. Data Analysis: Work closely with our Data Analysts to explore, analyze, and organize data; perform data extraction and preprocessing for training and evaluation purposes. Document: Document model architecture, training details, dataset extraction, and cleaning procedures for reproducibility, product management, and internal training. Opportunities to Learn Applying deep learning to real world industrial datasets containing billions of images. Exploration of the state-of-the-art methodologies in computer vision like self-supervised learning and unsupervised learning for the purpose of non-destructive testing. Exposure to full ML lifecycle and orchestration management tools like Prefect, Kubeflow, AWS SageMaker, Weights, and Biases. Converting ideas into intellectual property assets (e.g. patents). Who You Are (Basic Qualifications) Experience developing and training deep learning models from scratch. Master's degree in Computer Science, Computer Engineering, Electrical Engineering or a ML/AI related field; PhD preferred. Proficient in Python and at least one mainstream deep learning framework, such as PyTorch, TensorFlow, JAX, etc. What Will Put You Ahead Experience with medical or industrial ultrasound images for deep learning-related projects. Experience in deep learning projects for industrial non-destructive testing. Experience leading and mentoring ML engineers and ML scientists. Hands-on experience with ML lifecycle management tools like MLFlow, Amazon SageMaker, GPC Vertex AI, etc. Hands-on experience with orchestration tools like Kubeflow, Prefect, Airflow, etc. Working knowledge of Git, Docker, and cloud services like AWS and GPC. Publications in top tier venues likes CVPR, ICCV, etc. Experience with Reinforcement Learning and/or Few-shot Learning Experience with NLP (Transformers, RNNs) Experience in self-supervised and/or unsupervised learning Experience with Generative Model / Diffusion Experience with Causal Inference Great communicator with excellent data presentation, and report writing skills. Be able to work in a fast-paced, self-driven environment."
Variational AI, Machine Learning Researcher, "Here is the background we're looking for: Ph.D. in CS, applied mathematics, statistics, physics, or related discipline; Expertise with machine learning techniques, including diffusion models, Transformers, and Bayesian optimization, preferably demonstrated through first-author publications in conferences like NeurIPS, ICLR, and ICML; Two or more years' experience developing robust code on larger projects, including code review, refactoring, unit testing, version control, etc.; Mastery of Python and PyTorch; and Intellectual curiosity and drive to excel."
OnDeck Fisheries AI, Machine Learning Researcher, "Impact and scope of work You will have ownership over mission-critical research, propelling us towards the frontier of ocean tech and our goal of end-to-end visual automation. In this mix of machine learning research and research engineering, you will: Develop state-of-the-art machine learning models that can reason about visual data and retrieve relevant answers from external knowledge sources. Research, implement, and optimize for various tasks, including object detection, species classification, and multi-object tracking for scalable ocean vision systems and environmental monitoring. Devise better data-driven models for information retrieval, hierarchical classification, multi-modal fusion, generation and media understanding (CV & NLP). Collaborate with engineers to translate research into products, as well as communicating research plans, progress, results, and impact to the whole organization and in conference/journal papers. Collaborate and work across teams to develop concepts that advance the entire product pipeline, all the way to production (infrastructure, software, data collection, machine learning, etc.). Conduct research that enables learning the semantics of data across multiple modalities (images, video, text, physical sensors, and other modalities). Minimum Qualifications Ph.D. degree in Computer Science, Machine Learning, Artificial Intelligence, or relevant technical field. Experience as a machine learning researcher, including internships and full-time. Proficiency in programming and implementing machine learning workflows using Python (experience with C, C++, and JavaScript/TypeScript is an asset) Research experience with vision language models/large vision models, multi-modal encoder-decoder design, domain adaptation of open-vocabulary models, information retrieval problems, or other areas related to NLP and CV. Proven track record of achieving significant results as demonstrated by awards, fellowships, patents, as well as first-authored publications at leading journals or conferences such as NeurIPS, ICLR, ICML, JMLR, AAAI, KDD, CVPR, ECCV, ICCV, ACL, NAACL, EACL, or similar. Experience communicating research for public audiences of peers. Experience manipulating and analyzing complex, large scale, potentially high-dimensionality data from varying sources. Experience in utilizing theoretical and empirical research to solve problems. Strong technical communication skills in English, both written and verbal Enthusiasm for research that revolutionizes visual reasoning and ocean tech Preferred Qualifications Previous work experience designing and implementing ML models in production environments Extensive experience in machine learning products, computer vision-as-a-service, serving ML models, and/or optimizing model performance Experience in software engineering including cloud/edge development, Unix/Linux environments, distributed and parallel systems, and data engineering Proficiency with PyTorch, TensorFlow, and multiple machine learning frameworks/tools"
Synechron, ML Data Scientist, "Impact and scope of work You will have ownership over mission-critical research, propelling us towards the frontier of ocean tech and our goal of end-to-end visual automation. In this mix of machine learning research and research engineering, you will: Develop state-of-the-art machine learning models that can reason about visual data and retrieve relevant answers from external knowledge sources. Research, implement, and optimize for various tasks, including object detection, species classification, and multi-object tracking for scalable ocean vision systems and environmental monitoring. Devise better data-driven models for information retrieval, hierarchical classification, multi-modal fusion, generation and media understanding (CV & NLP). Collaborate with engineers to translate research into products, as well as communicating research plans, progress, results, and impact to the whole organization and in conference/journal papers. Collaborate and work across teams to develop concepts that advance the entire product pipeline, all the way to production (infrastructure, software, data collection, machine learning, etc.). Conduct research that enables learning the semantics of data across multiple modalities (images, video, text, physical sensors, and other modalities). Minimum Qualifications Ph.D. degree in Computer Science, Machine Learning, Artificial Intelligence, or relevant technical field. Experience as a machine learning researcher, including internships and full-time. Proficiency in programming and implementing machine learning workflows using Python (experience with C, C++, and JavaScript/TypeScript is an asset) Research experience with vision language models/large vision models, multi-modal encoder-decoder design, domain adaptation of open-vocabulary models, information retrieval problems, or other areas related to NLP and CV. Proven track record of achieving significant results as demonstrated by awards, fellowships, patents, as well as first-authored publications at leading journals or conferences such as NeurIPS, ICLR, ICML, JMLR, AAAI, KDD, CVPR, ECCV, ICCV, ACL, NAACL, EACL, or similar. Experience communicating research for public audiences of peers. Experience manipulating and analyzing complex, large scale, potentially high-dimensionality data from varying sources. Experience in utilizing theoretical and empirical research to solve problems. Strong technical communication skills in English, both written and verbal Enthusiasm for research that revolutionizes visual reasoning and ocean tech Preferred Qualifications Previous work experience designing and implementing ML models in production environments Extensive experience in machine learning products, computer vision-as-a-service, serving ML models, and/or optimizing model performance Experience in software engineering including cloud/edge development, Unix/Linux environments, distributed and parallel systems, and data engineering Proficiency with PyTorch, TensorFlow, and multiple machine learning frameworks/tools"
DRW, Machine Learning Specialist, "Responsibilities: Independently work on end-to-end development of Machine Learning and Natural Language Processing models to derive insights from research publications, legal documents, regulatory requirements etc. Technical analysis and software development Design and implement business solution in agile squads Engage in Machine Learning project, which includes problem definition, Data Engineering, Machine Learning design and documentation for the model risk management and running all the needed ML tests to ensure reliability. Develop, maintain and track detailed delivery plans Requirements: You are: Masters in mathematics, Statistics, Economics, Data Science, Machine Learning, Operational Research, Physics, and other related quantitative fields. At least 4 years of experience with design and implementation of machine learning, predictive analysis, data science, knowledge bases, recommendation systems, information retrieval. Strong understanding of the foundational concepts and applied experience in Machine Learning and model explainability (ideally, a combination of excellent academic research and high-impact commercial projects). In depth understanding of common Machine Learning algorithms (e.g., for classification, regression, and clustering). Experience with LLM models and Open AI. In depth knowledge of advanced statistical theories, methodologies, and inference tools. Proven track record in some of the advanced topics such as Bayesian inference, hierarchical models, deep learning, Gaussian processes, and causal inference. Practical experience in preparing data for Machine Learning integrating with big-data platforms and high-performance computing ecosystems. Strong oral and written communication skills. Strong analytical and problem-solving skills. It would be great if you also had: Familiar with Azure and AWS framework Experience with non-English Natural Language Processing. ML Libraries H2O, Keras, Tensorflow Experience with Deep Learning"
Absorb Software, AI Architect, "What you'll do: Architect and implement AI/ML solutions using cloud infrastructure and relevant tools. Stay updated with the latest AI trends, tools, and technologies, especially in areas like NLP, LLMs, and computer vision. Understand and assess the economic viability and scalability of AI features and solutions. Collaborate with cross-functional teams to present and communicate system architectures for AI/ML projects, including system designs, model performance, and ethics considerations. Ensure models are prepared for scale, including data preparation, training, observability, and performance optimization. Address AI model issues like hallucination and incorporate solutions for AI ethics. Leverage deep knowledge of the vendor ecosystem to integrate third-party tools and solutions. What you'll bring: Education: Bachelor's or Master's degree in Computer Science Infrastructure Expertise: Strong experience with cloud-based infrastructure. AI/ML Expertise: Expertise in NLP, Large Language Models (LLMs), and computer vision. Deep knowledge of model performance and scaling models in production environments. Experience with AI/ML data preparation and model training. Familiarity with AI observability, performance tracking, and troubleshooting hallucinations in models. Experience with AWS Bedrock. Soft Skills: Strong communication and presentation skills with experience presenting project architectures and outcomes. Keen awareness of AI ethics and the challenges associated with AI development. Proactive in researching new AI tools, vendors, and technologies to maintain competitive advantages. Preferred Qualifications: Experience with AI project management and deploying AI features at scale. In-depth knowledge of economic factors affecting the viability of AI projects and their integration into business processes."
Insight Global, Intermediate Data Scientist, "Responsibilities: Spend 50%-70% of your time working with large datasets to extract, manipulate, and clean data for model development. Develop and validate AI/ML models, focusing on fraud detection and AML, using Python, SAS, and SQL. Conduct in-depth data analysis, summarizing findings and generating insights that can be easily communicated to both technical and business audiences. Work closely with cross-functional teams to integrate models and provide continuous monitoring and validation. Measure and assess the performance of models, applying statistical and probability-based techniques. Focus on identifying behavioral patterns in transactional data, such as credit card statements and balances, to build models that detect fraud or money laundering. Ensure that models are built, tested, and validated according to industry best practices. Must-Haves: 3-5 years of experience with Python and SQL. 2+ years of experience developing AI/ML models. Proficiency in end-to-end model development and validation. Strong ability to manipulate and analyze large datasets to extract meaningful insights. Experience with fraud detection and AML models, particularly involving transaction and behavioral data. Strong presentation and communication skills to summarize and present complex analyses to both technical and business stakeholders. Nice to Have: Experience with SAS for model development and validation."
DarkVision, Data Scientist, "What You Will Do Data Pipeline Architecture: Design and build robust data pipelines, ensuring scalability, reliability, and efficiency. Production-Grade Code: Develop high-quality, production-level Python code for data processing, analysis, and modeling. Version Control: Utilize GitHub for version control and collaboration on code repositories. Information Visualization: Create compelling reports and 3D visualizations tailored for client-facing presentations to VPs and C-suite executives. Database Familiarity: Work with a variety of databases and object stores for data storage, manipulation, and retrieval. Cloud Computing: Implement cloud computing solutions with a focus on supporting and optimizing data pipelines. Opportunities to Learn and Work On: State-of-the-art deep learning technologies and pipelines. Work in an interdisciplinary team. Who You Are (Basic Qualifications) Bachelor's degree in computer science, statistics, mathematics, or another STEM field. Industry experience as a data scientist. Experience in image processing, signal processing, and/or geospatial analysis. Professional experience in SQL and Python programming writing production-grade code. Understanding of software engineering principles and experience developing production-grade code for data analysis, ETL, or Machine Learning pipelines. Knowledge of data analysis, data pipelining, and data visualization tools and techniques. Experience with cloud data ecosystems, ideally AWS. What Will Put You Ahead Intermediate to senior level industry experience. Master's degree or PhD. Experience developing data pipelines using orchestration tools such as prefect, airflow, dagster, Kubeflow, etc. Experience analyzing and developing image-based data applications. Experience developing conventional or machine learning Computer Vision algorithms. Experience implementing unit tests and contributing to continuous integration and continuous deployment (CI/CD) pipelines. Excellent communication and collaboration skills."
Rakuten Rewards, Data Scientist, "Key Responsibilities Collaborate with manager and stakeholders to specify business problems with precision Support the translation of business problems into machine learning problems, including ML problem formulation and evaluation metric specification. Perform exploratory data analyses to characterize utility of existing data and to identify needs for new data Develop data pipelines for feature computation, model training, inference and evaluation Design and execute offline experiments and simulations Lead model deployment, business integration and online performance assessment Maintain technical documentation, develop/present demos and presentations Support management in project planning and status reporting Minimum Requirements Very strong SQL skills Experience with big data and machine learning frameworks. Python/sklearn/pytorch strongly preferred. Practical experience with multiple ML problem types (supervised/unsupervised, classification/regression, RL experience is a plus) Strong software development skills Strong problem solving, communication and collaboration skills Firm understanding of statistics and A/B test fundamentals Experience with NLP techniques is a plus Deep learning/NN experience is a plus Qualification Requirements 5+ years industry experience applying machine learning methods to solve real-world problems Bachelor's degree in AI/ML, computational mathematics, computer science, statistics, physics or related field. Master's degree strongly preferred."
Stripe, Data Scientist, "Minimum Requirements 3-8+ years of data science/quantitative modeling experience Proficiency in SQL and a computing language such as Python or R Strong knowledge and hands-on experience in several of the following areas: machine learning, statistics, optimization, product analytics, causal inference, and/or experimentation Experience in working with cross-functional teams to deliver results Ability to communicate results clearly and a focus on driving impact A demonstrated ability to manage and deliver on multiple projects with a high attention to detail Solid business acumen and experience in synthesizing complex analyses into actionable recommendations A builder's mindset with a willingness to question assumptions and conventional wisdom Preferred Qualifications Experience deploying models in production and adjusting model thresholds to improve performance Experience designing, running, and analyzing complex experiments or leveraging causal inference designs Experience with distributed tools such as Spark, Hadoop, etc. A PhD or MS in a quantitative field (e.g., Statistics, Engineering, Mathematics, Economics, Quantitative Finance, Sciences, Operations Research)"
Clio - Cloud-Based Legal Technology,Data Scientist,"What you'll work on: Collaborate with the Clio products teams to refine business problems, develop hypotheses, and provide input that drives growth. Suggest new questions about our business, product, and customers that lead to impactful insights. Work with other team members to develop predictive AI and ML solutions and deploy them in production. Apply rigorous statistical analysis and data mining techniques to evaluate impact of different product features and other business initiatives. Employ statistical analysis, machine learning, GenAI, LLMs, etc. to unlock new product opportunities. Support scientific thinking in product and business teams by enabling discussions with data, disseminating best practices, and leading by example. Effectively communicate complex technical concepts and findings to both technical and non-technical audiences. What you may have: 3+ years applied experience in data science.The ability to translate business requirements into data science solutions. Experience in developing analysis in Python and experience with relevant ML libraries and frameworks (e.g., pandas, PyTorch, scikit-learn) Strong team player mindset, while able to work under your own initiative and prioritize time and tasks effectively. Excellent written and verbal communication skills. Ability to write structured SQL queries for answering questions and manipulating data. Experience in analytics working with product and user behaviordata, e.g., retention or churn analysis. Experience with building ML/AI pipelines and relevant tools (e.g., Kedro, MLFLow) Experience with large datasets and user behavior data. Experience with NLP and LLMs. A graduate degree in a relevant quantitative discipline (computer science, statistics, mathematics, physics, engineering)"
Clio - Cloud-Based Legal Technology,Data Scientist,"What you'll work on: Collaborate with the Clio products teams to refine business problems, develop hypotheses, and provide input that drives growth. Suggest new questions about our business, product, and customers that lead to impactful insights. Work with other team members to develop predictive AI and ML solutions and deploy them in production. Apply rigorous statistical analysis and data mining techniques to evaluate impact of different product features and other business initiatives. Employ statistical analysis, machine learning, GenAI, LLMs, etc. to unlock new product opportunities. Support scientific thinking in product and business teams by enabling discussions with data, disseminating best practices, and leading by example. Effectively communicate complex technical concepts and findings to both technical and non-technical audiences. What you may have: 3+ years applied experience in data science.The ability to translate business requirements into data science solutions. Experience in developing analysis in Python and experience with relevant ML libraries and frameworks (e.g., pandas, PyTorch, scikit-learn) Strong team player mindset, while able to work under your own initiative and prioritize time and tasks effectively. Excellent written and verbal communication skills. Ability to write structured SQL queries for answering questions and manipulating data. Experience in analytics working with product and user behaviordata, e.g., retention or churn analysis. Experience with building ML/AI pipelines and relevant tools (e.g., Kedro, MLFLow) Experience with large datasets and user behavior data. Experience with NLP and LLMs. A graduate degree in a relevant quantitative discipline (computer science, statistics, mathematics, physics, engineering)"
AutoTrader.ca, Data Scientist, "What you'll do: Use predictive modeling to increase and optimize customer experiences across Trader products and services. Work with the Product team to prototype new ideas, models, and ways of leveraging lots of data points to improve business outcomes. Utilize appropriate cloud resources based on project demands and data workload needs to automate and productionalize data science processes. Develop enrichment processes and procedures to improve the cleanliness, usability and understanding of data. Proactively monitor existing Machine Learning models, their performance, output, and accuracy. Management of large-scale data processing system for preparing structured and unstructured data for analytic modeling. Work along with Support Teams to troubleshoot and identify customer-reported issues. What you'll need: Experience within an Azure environment, or other cloud environments Equivalent of 4-8+ years related work experience Scientific approach, curiosity, creativity, and passion for finding the truth in data Excellent team player with interpersonal skills University degree in STEM field Expert level in Python and SQL querying skills Previous experience with training and maintaining ML models Experience with analytical tools and techniques Experience with data cleaning, manipulation, and wrangling Experience using structured and unstructured data, from internal and external sources Initiative and ability to work with little supervision in cross functional teams Ability to collaborate with other teams to achieve organization's goal"
Ideogram, Machine Learning Researcher, "What We're Looking For PhD or Master's degree in Computer Science or equivalent industry experience 2+ years of experience in AI research, including training, fine-tuning, and experimenting with foundation models beyond black-box use Track record of first-author publications at top-tier AI conferences (e.g., NeurIPS, ICML, ICLR, CVPR, ECCV, ICCV, ACL, EMNLP) Strong proficiency in one or more deep learning frameworks (e.g., JAX, PyTorch) Experience communicating complex research to peers Solid knowledge of programming languages and experience in developing, debugging, and optimizing beyond ML systems Nice to have Experience in low-level machine learning optimization, such as writing CUDA kernel code"
Data Scientist (ML), McAfee, "Conceptualize, design, and implement state-of-the-art ML models for dynamic pricing strategies and personalized product recommendations. Develop, implement, and deploy machine learning models that leverage our unique combination of user behavior and subscription data to improve consumer value from our products. Engineer and maintain large-scale consumer behavioral feature stores while ensuring scalability and performance. Develop and maintain data pipelines and infrastructure to support efficient and scalable ML model development and deployment. Collaborate with cross-functional teams (Marketing, Product, Sales) to ensure your solutions align with strategic objectives and deliver real-world impact. Create algorithms for optimizing consumer journeys and increasing conversion and monetization. Design, analyze, and troubleshoot controlled experiments (Causal A/B tests, Multivariate tests) to validate your solutions and measure their effectiveness. About You 7 to 9 years of experience in one or more of the following areas: machine learning (including deep learning), recommendation systems, pattern recognition, data mining or artificial intelligence. Proficient in Python, SQL, intermediate data engineering skill set with tools, libraries, or frameworks such as MapReduce, Hadoop, Hive and Big Data technologies, scikit-learn, Keras, TensorFlow, PyTorch etc. Experience with various ML techniques and frameworks, e.g., data discretization, normalization, sampling, linear regression, decision trees, deep neural networks, etc. Experience in building industry-standard recommender systems and pricing models. Expertise in MLOps, ML Engineering and Solution Design."
StackAdapt, Data Scientist, "What you'll be doing: Innovate ML algorithms to maximize ROI and advertising performance. This ranges from creating entirely new algorithms, to improvements on state-of-the art methods, to development using a deep understanding of classic methods Write production code, sometimes collaborating with Data Engineers, to implement the novel ML algorithms Prototype potential algorithms and pipelines, test them using historical data, and iterate to modify based on insights What you'll bring to the table: Have a Masters degree or PhD in Computer Science, Statistics, Operations Research, or a related field, with dual degrees a plus. Have the ability to take an ambiguously defined task, and break it down into actionable steps Have a comprehensive understanding of statistics, optimization and machine learning Are proficient in coding, data structures, and algorithms Enjoy working in a friendly, collaborative environment with others"
Samsung Electronics, Data Scientist, "About the Job Design and build advanced machine learning and AI models to address business challenges, and generate reports Analyze data in order to understand usage and performance of the Samsung Health service Apply and innovate latest advances in LLM, NLP and recommender systems to build innovative solutions Collaborate with data engineers to develop data and model pipelines Work on complex issues where analyzing customer situation or business data requires an in-depth evaluation of variables Exercises judgement in selecting methods, techniques and evaluation criteria to obtain results. About You Master's degree in a highly quantitative field (Computer Science, Machine Learning, Operational Research, Statistics, Mathematics, etc.) or equivalent professional field 5+ years of working experience in data scientist/analyst role building and deploying ML/AI models or hands on experience developing deep learning and predictive models Expertise in SQL Experience creating visualization & dashboards using tools like Tableau or Apache Superset Expertise in Python using e.g. SciKit-Learn, Pandas, TensorFlow, Keras, Deep Learning Framework etc. Experience with Generative AI technology, LLM and RAG Experience with various AWS services (e.g. EC2, S3, Athena, Lambda, Kinesis Firehose, Redshift) Strong communication and team working skills Bonus Skills Experience with Mobile industry and/or experience with Digital Healthcare AWS / GCP / Azure certification Experience with Airflow, EKS, EMR, Redshift, Kubernetes Experience with CICD tools such as Github Actions or CircleCI"
Ideogram, Machine Learning Engineer, "What We're Looking For 2+ years of experience in developing machine learning models in JAX, PyTorch, or TensorFlow Experience in implementing Machine Learning foundations (e.g., Transformer, VAE, Denoising Diffusion models) from scratch Track record in machine learning innovation and familiarity with Deep Learning and advanced Machine Learning End-to-end understanding of generative media applications and excitement for pushing the state-of-the-art in generative AI Ability to debug machine learning models to iteratively improve model quality and performance Nice to have: Familiarity with kubernetes and docker Optional: Experience in low-level machine learning optimization, e.g., writing CUDA kernel code"
Recursion, Machine Learning Intern Research, "A successful candidate will have most of the following: Currently enrolled in a post-doctoral fellowship, PhD, or Master's degree program. Strong programming skills and understanding of modern software development practices, especially in Python. Experience in building and deploying high-performance implementations of deep learning algorithms. Proven track record in machine learning, including designing new architectures, hands-on experimentation, analysis, visualization, and model deployment. Demonstrated capability to understand and summarize scientific content and implement deep learning models based on descriptions from publications. Strong knowledge of linear algebra, calculus, and statistics. Passion for applying ML research to real-world problems. Nice to have: Authorship of a publication in peer-reviewed conferences (e.g., NeurIPS, ICML, ICLR, or similar). Contribution to high-visibility ML codebases."
Thales, Data Scientist, "Key Areas of Responsibility Data curiosity. Many of the problems we face reduce to data problems. Must be proficient with analysis and processing of very large datasets Determination to solve problems through technical and organizational hurdles. Interest in threat detection/identification in an arms race situation. Attackers are constantly trying to bypass our systems. We need to patch vulnerabilities in our existing detection methods that attackers are exploiting as well as come up with new methods and techniques. Customer focus: this is both a product team as well as a research team. We need to be willing and able to work with the codebase and provide value to our customers. Ability to execute. What we want is demonstrated ability to get things done and work through impediments when they arise. Staying informed about developments in Data Science, statistics and related fields. The ideal candidate can read and implement the ideas of research papers. Minimum Requirements Advanced degree in Data Science, Data Analysis, Statistics, Computer Science, or equivalent experience Extensive experience performing data analysis Demonstrated experience with software experience fundamentals: command line, version control, pull request review, software maintenance, etc. Proficiency in analytical programming in Python, R or Julia (Python is preferred) In-depth knowledge of data processing and analysis libraries such as Pandas or R's tidyverse In-depth, experienced understanding of SQL and data visualization Working knowledge of data structures and algorithms."
PDF Solutions, Mid Level Data Scientist - Machine Learning, "Responsibilities Research, design, implement, and validate ML Pipelines while collaborating with other data scientists. Balance adding new features with the need for stability and performance. Grow development capabilities to align with the pace of business needs. Qualifications Master's degree or higher in Computer Science, Computer Engineering, Electrical Engineering or similar discipline with industrial experience in software development 5+ years of experience with Python coding 5+ years of recent experience working as a Data Scientist in industry Experience with Spark and PySpark Experience with developing production-grade code, preferably in Python Experience with data science and machine learning, including Python libraries such as NumPy, SciPy, Pandas and Scikit-learn Strong professional written and verbal communication skills Ability to pass a Data Science skills-based test Experience with relational or NoSQL databases such as Oracle/Cassandra/Redis or similar Ability to create model-ready data from raw data, at scale Ability to translate business problems into data science pipelines"
Mastercard, Lead Data Scientist, "Key Responsibilities Model Development & Integration: Design, develop, and deploy machine learning models to detect, assess, and mitigate cyber risks across the Mastercard ecosystem. You'll work on end-to-end data pipelines that ingest data, analyze risks, and deliver actionable insights at scale. Collaboration Across Teams: Lead efforts to integrate models into customer-facing products, ensuring they deliver measurable value. Coordinate with cross-functional teams including engineering, product management, and data analysts to ensure that your models not only work, but are scalable, efficient, and aligned with customer needs. Driving Business Impact: Work closely with product and business teams to ensure that models are designed with clear business objectives in mind. You will be accountable for translating data insights into product solutions that help mitigate cyber vulnerabilities and enhance security for customers. Customer-Focused Innovation: Contribute to product ideation by using your deep technical expertise and creativity to solve complex problems and deliver data-driven solutions that meet customer needs in innovative ways. Communication & Leadership: Lead efforts to communicate complex data science concepts and model outputs to both technical and non-technical stakeholders. Serve as a subject matter expert, guiding team members and promoting data-driven decision-making throughout the organization. What You'll Bring We're looking for a highly skilled Lead Data Scientist who is not only an expert in building sophisticated machine learning models but also has a track record of taking those models through to product deployment, ultimately driving customer value. This is a great opportunity for someone who thrives in a fast-paced, collaborative environment and is passionate about delivering real-world impact through data science. Technical Expertise End-to-End Data Science Experience: Proven ability to take models from research and development to full product integration. You know how to design, build, and deploy machine learning models that make a tangible impact on products and business outcomes. Machine Learning Proficiency: Deep expertise in supervised and unsupervised learning, reinforcement learning, and advanced modeling techniques. Strong working knowledge of tools such as Python, SQL, and R; experience with TensorFlow, PyTorch, or similar libraries is a plus. Data Engineering & Pipelines: Strong experience building and optimizing data pipelines that allow seamless integration of models into production environments. Familiar with large data platforms and cloud services such as AWS, GCP, Azure, and big data tools (e.g., Spark, Hadoop). Cybersecurity & Fraud Analytics: Familiarity with cybersecurity, fraud detection, and risk management processes is essential. An understanding of how to model and identify vulnerabilities within payment systems and digital ecosystems is highly valued. Visualization & Reporting: Expertise in creating compelling data stories and presenting insights clearly through dashboards, reports, and visualizations. You know how to communicate insights to both technical and business audiences. Business Acumen & Leadership Value-Driven Mindset: Ability to align data science efforts with clear business goals, and take ownership of delivering value through actionable insights and customer-centric products. Cross-Functional Leadership: Demonstrated ability to work closely with product managers, engineers, and other teams to ensure successful product delivery. Strong collaboration and communication skills are a must. Innovation & Problem Solving: A creative thinker who is always looking for new ways to improve processes, optimize models, and deliver better results. You are a proactive problem solver who thrives in dynamic, fast-paced environments. Customer-Focused: You understand the importance of user-centric design and how to turn data insights into valuable product features. You approach data science with the end-user in mind. Qualifications Education: Advanced degree (MS or PhD) in a quantitative field such as Computer Science, Data Science, Cybersecurity, Mathematics, Statistics, Engineering, or a related discipline. Experience: Experience in data science, machine learning, or related fields, with significant experience in building and deploying models into production environments."
Electronic Arts (EA), Senior Data Scientist, "What You'll Do Work directly with the game teams/partners (internal clients) to understand their offerings/domain and create data science products and solutions to solve for their live service use cases. Assist our team leads across Live Services: Recommendations/Matchmaking/Personalization with generalizing products across titles. Work on team projects within your pillar (live service), proposing methodology and guiding the direction of each project. Ensure sound methodology and experimental design, and guide the direction of each project from start to finish. Effectively communicate data science projects and products to stakeholders, including those with non-data science backgrounds. Design, improve and work with our data pipeline that transfers and processes petabytes of data using S3, Kubernetes, Sagemaker, GCP, Python, Apache Kafka, & Hive. Use data manipulation, processing, and data visualizations to aid in your modeling efforts. Assist game teams and data teams in making important decisions. Mentor and coach junior team members. You Have 4+ years of professional experience manipulating data sets, building statistical models, and productizing models along with experience applying Data Science methodologies to real-world problems. Ph.D./Master's in Statistics, Mathematics, Computer Science, or another quantitative field encouraged. Experience working on a mix of recommendation, personalization, matchmaking, and forecast modeling. Experienced in analyzing extremely complex, multi-dimensional data sets with a variety of tools. Superior verbal, visual, and written communication skills to educate and work with cross-functional teams. Experience using Python, SQL, PyTorch/TensorFlow, Hive/Hadoop, and AWS/GCP."
MathCo, Data Science Manager, "ROLE DESCRIPTION We are looking for passionate individuals to help our clients solve complex challenges, enabling their sustained analytics transformation. As a Data Science leader, you will lead a team of data scientists and consultants to design, execute, and implement cutting-edge AI/ML and analytical solutions and capabilities. You will be responsible for institutionalizing data-driven insights and recommendations to bring customer strategies to life. The ideal candidate should be an expert-level data scientist with a strong track record in Retail or other consumer-facing industries, excel at managing key client engagements and maintaining strong relationships, while effectively leading teams of data scientists and data engineers. This role involves understanding the client's business, generating insights, developing recommendations, and presenting them to clients and their leadership. In addition, you will oversee the communication of strategy and delivery plans, ensuring alignment between the client and internal team while mitigating risks and escalations. You will also familiarize yourself with the strategic direction of our clients, helping them translate their goals into actionable outcomes on current engagements and contributing to the development of future projects. Job Responsibilities Designing strategies and roadmaps to build analytics capabilities for clients Solving complex business problems by leveraging both conventional and emerging data sources and applying advanced analytics techniques Building strong relationships with clients, positioning yourself as a trusted partner who provides thought leadership and strategic guidance Leading, mentoring, and coaching a team of Associates and Senior Associates across our US offices, ensuring the success of both the team and the clients Collaborate with counterparts in India, who lead local delivery teams, to effectively execute projects through our global delivery model Proactively identify business development opportunities within existing client engagements and departments Skills Required 7-13 years of experience in leading data science projects in a fast-paced environment across various functions within retail or other consumer-facing industries, with a proven track record of successful leadership Proven technical expertise in Data Analysis, Modeling, Statistics, and Machine Learning, along with proficiency in Python coding, Python libraries for data science (e.g., Pandas, NumPy, Scikit-learn, TensorFlow, etc.), and familiarity with data pipelines and frameworks (e.g., PySpark, Apache Airflow, etc.). Experience with other analytics tools like R, SAS, SPSS, etc. is a plus. Demonstrated ability to structure and manage large-scale analytics programs, including resource and technical requirements, methodology definition, and program operationalization Strong networking skills and a growth mindset, with a focus on acquiring new projects and expanding existing engagements Experience working with cross-functional and cross-cultural teams High level of comfort with ambiguity and change, with the ability to translate business gaps into analytics problems, define the analytical approach, deliver actionable insights, and operationalize recommendations"
National Bank of Canada, Senior Data Scientist, "Your job: Collaborate with different partners to complete high-impact modeling mandates. Develop, maintain and optimize financial crime detection models. Recommend efficient approaches to the detection of financial crimes. Ensure that you meet the standards surrounding the statistical modeling and model documentation process. Contribute to reporting or analysis activities in the fight against financial crimes. Supporting our partners in their business needs. Prerequisite: Bachelor's degree completed and 6 years of relevant experience or Master's degree completed and 4 years of relevant experience (in statistics, data science, mathematics, computer science or a related field). Experience in modeling and a good command of supervised and unsupervised approaches. Expertise in data visualization. Proficiency in SAS, SAS Miner, SQL and Python and good knowledge of the Power BI tool."
Faire, Senior Data Scientist, "What You'll Do Shipping cost optimization: Build ML models that provide accurate shipping cost estimates. Engineer new features to improve model performance. These models may use live carrier information and be both performant and explainable. Underwriting: Improve Faire's Net Terms portfolio by evaluating the creditworthiness of retailers on the platform. Use predictive modeling to dynamically assign credit limits that minimize default risk while maximizing growth. Leverage ML models to improve the retailer Identity Verification (IDV) experience, reducing default losses for new retailers. Retailer Growth: Build models to automatically generate landing pages and content to target search engine demand. Use natural language processing to understand search engine keyword intent and match to relevant internal content. Build ML models to generate intelligence about retailers to power personalization. Predict retailer lifetime values to optimize retailer acquisition spend. Brand Growth: Prioritize brand leads for sales by predicting their lifetime value. Estimate the incremental value of new brands based on Faire's existing selection and brand characteristics. Optimize how new brands are featured and explored. Listing Quality and Catalog Growth: Use AI techniques to detect and correct image issues, generate product titles and descriptions, extract product attributes, and predict product taxonomy. Perform entity resolution in order to match products across multiple data sources and link product variants. Marketplace Quality: Summarize and tag retailer reviews using LLMs. Detect and remove products that violate Faire's policies, such as counterfeits. Use marketplace levers to direct retailers toward brands with higher service quality. Qualifications An advanced degree (MS or PhD) in a relevant discipline such as statistics, economics, econometrics, mathematics, computer science, operations research, etc. Strong machine learning skills and 3+ years of experience productionizing machine learning models (Sklearn, XGBoost, or Deep Learning) Strong programming skills (Python, Java, Kotlin, C++) Knowledge of statistical techniques such as experimentation and causal inference SQL or other database querying experience preferred An excitement and willingness to learn new tools and techniques"
Manulife, Machine Learning Engineer, "Responsibilities Collaborate with Data Scientists and Data Engineers to design and implement scalable and efficient machine learning pipelines. Evaluate and optimize machine learning models for performance and scalability. Deploy machine learning models into production and monitor their performance. Handle data science infrastructure to streamline model development and deployment. Proposing appropriate tools (languages/libraries/frameworks) for implementing projects. Working closely with infrastructure architects to craft scalable and efficient solutions. Work closely with multi-functional teams to integrate machine learning models into existing systems and processes. Stay up-to-date with the latest advancements in ML & AI. Mentor associates and peers on MLOps standard practices. What We Are Looking For Hands-on experience with large-scale systems in software engineering. Experience in operationalizing code through DevOps pipeline (git, Jenkins pipeline, code scan). Familiarity with big data processing and building data APIs. Experience with automated data quality frameworks is a plus. Working experience in building and deploying machine learning models as REST-based API using Flask, Elasticsearch, etc. Strong programming skills in Python and experience with ML libraries such as TensorFlow, PyTorch, or scikit-learn. Advanced working SQL knowledge and experience working with relational databases and SQL. Experience in infrastructure, including Cloud Computing, Linux OS, Networks, Docker, Kubernetes, RDBMS and NoSQL Databases. Experience working with cloud native architecture (PaaS) using Azure stack preferably and experience with Azure ML, DataBricks (Spark), Azure Data Factory will be an asset. Experience in building ETL pipelines to perform feature engineering on large-scale dataset using Spark. Experience with Large Language Models (LLMs) such as GPT-3 or BERT. An ability to balance a sense of urgency with shipping high quality and pragmatic solutions. Expertise in delivering analytics & machine learning products, with a deep understanding of agile product delivery in an enterprise environment."
Epson Canada, Research Scientist, "Responsibilities: Investigate and utilize complex engineering and mathematical principles to develop core technologies, composing of algorithm structures and finding solutions to complex problems. Design and implement machine learning algorithm for current and future Epson applications. Create, prioritize, communicate, maintain, and execute roadmaps, project plans, and commitments. Oversee completion of machine learning system/software testing/evaluation activities and actively communicate the results to management and stakeholders. Periodically, review and update the system requirements/design documentation together with algorithm reports. Actively participate in the patents filing process to ensure protection of EPSON core technology ideas. Essential Skills & Requirements: PhD or Master of Science Degree in Computer Science, Computer Engineering, or related technical discipline. Good theoretical and practical knowledge of the state-of-the-art AI research and innovation, including deep learning, natural language processing, foundation models, transformer architectures etc.. 2+ years of hands-on experience in developing, fine-tuning and deploying large-scale models, including LLM, VLM, MLLM, for a variety of applications in the fields of robotics and computer vision. Good programming skills with C++, Python, with sufficient familiarity to at least one of the following frameworks: Tensorflow, PyTorch. Publication in reputed conferences and journals with the fields of AI, computer vision and robotics. Well organized with excellent verbal and written communication skills. Team builder/player, with an energetic and proactive attitude. Additional Skills: Experience with software development tools, source control, issue tracking, continuous integration, code coverage"
Lyft, Data Scientist - Algorithms, "Responsibilities: Leverage data and analytic frameworks to identify opportunities for growth and efficiency Partner with product managers, engineers, marketers, designers, and operators to translate data insights into decisions and action Design and analyze online experiments; communicate results and act on launch decisions Develop analytical frameworks to monitor business and product performance Establish metrics that measure the health of our products, as well as rider and driver experience Provide coaching and technical guidance for the team Prioritize and lead deep dives into our data to uncover new product and business opportunities Facilitate and foster data-driven and informed decision making and prioritization Experience: Ph.D. in Statistics, Operations Research, Mathematics, Computer Science, or other quantitative fields or related work experience. Passion for solving unstructured and non-standard, ambiguous mathematical problems by leveraging expertise in one or multiple fields. Strong skills in optimization, statistics, and causal inference. End-to-end experience with data, including querying, aggregation, analysis, and visualization. Proficiency with Python, or another interpreted programming language like R or Matlab Strong communicator. Able to coordinate with several teams of data scientists to deliver on complex initiatives. Strong business sense and understanding of experimentation methodologies. Proficiency in SQL - able to write structured and efficient queries on large data sets. Experience in online experimentation and statistical analysis. Strong oral and written communication skills, and ability to collaborate with and influence cross-functional partners."
Lyft, Data Scientist - Decisions, "Responsibilities: Leverage data to provide insights around driver behavior and sentiment and identify improvement opportunities Apply analytics, inference and experimentation techniques to solve business problems Design and analyze experiments; communicate results to technical and non-technical audiences to act on launch decisions Establish metrics that measure the health of our driver products and driver experience Develop analytical frameworks and dashboards to monitor business and product performance Partner with product managers, engineers, and operators to translate analytical insights into decisions and action, and implement products to drive business goals Design and implement data pipelines to support data analysis and product implementation Experience: Degree in a quantitative field such as statistics, economics, applied math, operations research or engineering (advanced degrees preferred), or relevant work experience 4+ years experience in a data science role or analytics role Proficiency in SQL - able to write structured and efficient queries on large data sets Experience in programming, especially with data science and visualization libraries in Python or R Experience in online experimentation and statistical analysis, and communicating results and recommendations to senior stakeholders Strong oral and written communication skills, and ability to collaborate with and influence cross-functional partners Experience in applying machine learning techniques a plus (e.g. reinforcement learning) to solve customer problems (e.g. personalization, segmentation) Experience working with ETL pipelines a plus"
Thri5, Lead Data Scientist, "Key Responsibilities: AI Vision and Strategy: Leverage deep technical expertise in ML and AI to define and drive the AI strategy for Thri5 across all aspects of the platform. Problem Definition: Partner with Product and Engineering teams to leverage AI models in building high value products from ideation to execution and launch. Model Development: Develop, implement, and optimize high-performing AI and ML products to predict supply chain opportunities, optimize performance, and automate actionable changes. Data Responsibility: Take full ownership of data integration and ensure seamless data flow from various supply chain systems into the AI platform. Entrepreneurial Mindset: Demonstrate an entrepreneurial spirit by independently figuring out the necessary steps to advance projects, and proactively addressing challenges as they arise. Thought Leadership: Provide thought leadership in AI and data science, staying ahead of industry trends and continuously innovating to enhance Thri5's platform Required Skills and Experiences: Educational Background: Master's or Ph.D. in Computer Science, Data Science, Machine Learning, or a related field. Experience: 10+ years of experience in data science and machine learning, with demonstrated success leveraging AI and ML to build and deliver high value products within supply chain and/or retail. Experience with LLMs, newest AI evolution, research and releases Technical Proficiency: Strong programming skills in Python, R, and Java. Proficiency with machine learning frameworks such as TensorFlow, PyTorch, and Scikit-Learn. Experience with big data tools like Hadoop, Spark, and Kafka. Deep Learning Expertise: Extensive experience with deep learning frameworks such as TensorFlow and PyTorch, and a strong understanding of neural networks and other deep learning architectures. Analytical Skills: Expertise in predictive modeling, classification, clustering, and time-series analysis. Leadership: Proven ability to lead and scale a practice, with excellent project management and communication skills."
The Trade Desk, Data Scientist II - Distributed Algorithms, "Who We Are Looking For You have a sustained track record of making significant,self-directed, and end-to-end contributions to complex, multi-layered, impactful machine learning You think beyond just the task at hand to deeply understand the 'why' behind what you are doing and anticipate problems that may be several degrees removed from obvious requirements. You make your team better by proactively advocating for and driving improvements not just to what the team produces, but also the ways in which the team works. You have a strong sense of data intuition. At our scale, many off-the-shelf modeling techniques (open source and enterprise) simply don't work. You are able to work from first principles to develop solutions and adapt them to a unique environment. You are a broadly skilled data scientist with deep expertise working with embedded models in always-onproduction systems and working across a variety of technologies and data sources. You have a product-focused mindset. You have the passion and ability to contribute to the process of discovering what will delight our clients and push forward one of the world's largest and most influential industries toward a vision of openness, transparency, and evidence-based decision-making. You work with confidence and without ego. Our data scientists have deep knowledge and exercise a high degree of leadership in their daily work. You have strongly-held, defensible ideas, and advocate for what you believe is right. You are also adept at identifying andevaluating trade-offs, willing to be proven wrong, and quick to support your fellow teammates. You value, seek out, and foster diversity. We are a global team from many diverse backgrounds, with different experiences and perspectives. To complement this team, you will welcome ideas that are different from your own and be skilled at finding and building from common ground. You are a creative thinker, not bound by "the way things have always been done." What you know is less important than how well you learn and innovate. We don't need data scientists who know all the answers; we need data scientists who can invent the answers no one has thought of yet, to the questions yet to be asked. You have an abundance of intellectual curiosity and are enthusiastic to learn (and teach) new technologies / techniques. You are you comfortable working on an agile, distributed team spanning multiple time zones and continents. You are able to communicate effectively across both technical and non-technical audiences. BS/MS with 4+ years or a PhD with 2+ years of experience working in a DS or ML role that involves bringing products from ideation to production. Experience in deep learning, TensorFlow preferred. Experience running heavy workloads on a distributed computing cluster (especially EMR or Databricks), leveraging technologies like Spark to work with large datasets is preferred Proficient in at least two of: R,Python, Java, or Expert at one of those. Strong SQL skills. Proficient in software engineering concepts like containerization and version control is preferred."
Loblaw Digital, Machine Learning Engineer - Search, "What You'll Do Develop and implement machine learning models and algorithms to solve complex business challenges. Collaborate with senior engineers and cross-functional teams to integrate ML solutions into various product features. Work with large datasets to identify trends, patterns, and insights that influence strategic decisions. Optimize ML models for scalability, performance, and reliability in production environments. Stay abreast of the latest advancements in AI/ML technologies and apply cutting-edge methods to enhance our systems. Engage in continuous learning and knowledge sharing to maintain technological leadership. Assist in code reviews, participate in technical design sessions, and support the development of junior engineers. Address and troubleshoot issues related to ML models and data pipelines. Communicate technical concepts effectively to non-technical stakeholders. Create and maintain detailed documentation for ML models and processes Does This Sound Like You? You have 3-5 years of experience in software development with a strong focus on machine learning. Proficient in ML frameworks and libraries such as TensorFlow, PyTorch, and scikit-learn. Skilled in programming languages like Python, Java, or similar. Experienced in data handling, preprocessing, and pipeline development, including SQL/NoSQL databases. Capable of analyzing large datasets and extracting meaningful insights. Knowledgeable about AI technologies, including Gen AI, LLMs, and semantic search. Understanding of security architecture and data privacy standards. You are an effective problem-solver with a passion for continuous innovation and learning. Excellent communication skills, with the ability to present technical information to diverse audiences. Enthusiastic about working in a team-oriented, collaborative environment. Open to new ideas and continually seeking better ways to achieve goals."
Synopsys, AI Engineer - Staff, "Key Responsibilities: AI/ML Solution Development: Design, develop, and deploy AI and machine learning models and algorithms to solve complex business problems. Technical Leadership: Provide technical leadership and mentorship to junior engineers and data scientists, guiding them in best practices and advanced techniques. Research & Innovation: Stay up to date with the latest advancements in AI and machine learning technologies and apply them to improve existing systems or develop new solutions. Collaboration: Work closely with product managers, software engineers, and other stakeholders to define project requirements, create technical specifications, and ensure successful implementation of AI solutions. Data Analysis & Preprocessing: Perform data analysis, data preprocessing, and feature engineering to prepare datasets for machine learning models. Model Training & Evaluation: Train, validate, and fine-tune machine learning models, ensuring they meet performance and accuracy requirements. Deployment & Monitoring: Deploy AI models into production environments, and monitor their performance, making adjustments as necessary to maintain optimal operation. Documentation: Document AI models, algorithms, and methodologies to ensure reproducibility and knowledge sharing within the team. Compliance & Ethics: Ensure AI solutions adhere to ethical guidelines, data privacy regulations, and industry standards. Qualifications: Education: Bachelor's or Master's degree in Computer Science, Data Science, Electrical Engineering, or a related field. PhD is a plus. Experience: Minimum of 5 years of experience in AI and machine learning, with a proven track record of deploying AI solutions in a production environment. Technical Skills: Strong proficiency in programming languages such as Python, or C++. Extensive experience with machine learning frameworks and libraries (e.g., TensorFlow, PyTorch, scikit-learn). Strong understanding of statistical analysis, data mining, and data visualization techniques. Knowledge of cloud platforms (e.g., AWS, GCP, Azure) and containerization (e.g., Docker, Kubernetes). Familiarity with version control systems (e.g., Git) and software development methodologies (e.g., Agile, Scrum)."
Index Exchange, Senior Data Scientist, "A pragmatic data science technologist: Learn and understand the business vision and market opportunity for the company Work backwards from business problems to generate scientific hypotheses to guide development focus and pragmatism Use analytical, statistical and database skills to understand large, complex datasets with the aim to increase the programmatic efficiency Design, develop, and drive implementation of complex data-centric solutions, including large data set segmentation, transformation, and feature generation Clearly articulate research and development hypothesis, decisions and results Take part in the process of designing, developing & improving algorithms for automated decision making around programmatic exchange optimisation Take active part in designing, and driving implementation production performance monitoring frameworks and KPIs Work together with ML engineers and architects to develop production grade data centric solutions An entrepreneurial spirit with room and expectations to bring innovative ideas to address the problems at hand Here's What You Need Bachelors or Masters in Mathematics, Computer Science, Engineering, or equivalent 3+ years of experience in data science or machine learning environments that requires the combined application of statistical & technical skills Experience in data science practices in operational environments and internet scale, i.e - this is an applied position, rather than a solely research focused one Strong statistical and research skills with a track record of using a variety of statistical methods (especially classification/prediction and time series models) Proven ability to demonstrate and articulate complex results to wide technological audience Demonstrated history of enabling advanced data modeling based guidance for technology development Experience in cross functional team cooperation to drive pragmatic technological solutions Expert in pragmatic application of data science and optimization best practices at large scale (industry scale platform experience preferred) Excellent development skills (Python required, specifically pandas, numpy, SciKit; Knowledge in Scala a plus) Familiarity with digital marketing, advertising or analytics is a plus Work experience in complex system engineering environments is a plus"
Jerry, Data Scientist, "How You Will Make An Impact Define, understand, and test levers to drive profitable and scalable user acquisition across our paid and organic channels Design, run, and analyze A/B experiments across different channels, extract key insights, share learnings, and continue iterating Build key reports, dashboards, and predictive models to monitor the performance of our paid ads and marketing channels, communicate analytical outcomes to our teams, and make recommendations on the next steps Transform and refine raw production data for analytical needs Preferred Experience Bachelor's degree in a quantitatively or intellectually rigorous discipline 3+ years of consulting, data science, or growth analytics experience High level of comfort with SQL and Python (or similar ML programming language"
EY, Manager - AI and Data - Solution Architect, "Skills And Attributes For Success Lead and oversee data architecture projects, from initial discovery and design to implementation and optimization, ensuring alignment with client objectives and best practices Collaborate with clients to understand business requirements, data challenges, and technical constraints, and develop cloud-based data strategies and roadmaps to address client needs Architect, design, and implement scalable and performant data platforms, including data warehouses, data lakes, and data pipelines Develop and document data architecture designs, data models, and integration patterns, ensuring alignment with industry standards, security requirements, and regulatory compliance Develop conceptual, logical, and physical data models, including dimensional modeling (star schema, snowflake schema), Data Vault 2.0 modeling, and graph data modeling, to represent business entities, relationships, and processes Lead and mentor onshore and offshore project teams, including data architects, data engineers, and developers, providing technical guidance, coaching, and support to ensure successful project delivery Conduct performance tuning, optimization, and troubleshooting of data environments, identifying and addressing performance bottlenecks, data loading issues, and query optimization opportunities Collaborate with vendor account teams, technical support, and product management to stay updated on new features, enhancements, and best practices, and provide feedback and recommendations to clients Manage client relationships, expectations, and escalations, ensuring timely delivery, quality assurance, and customer satisfaction Commitment to continuous learning and staying abreast of emerging trends, technologies, and best practices in data management To qualify for the role you must have Bachelor's or Master's degree in Computer Science, Information Systems, or a related field Minimum 7 years experience in data architecture, data engineering, and database design, preferably in a consulting or professional services environment Minimum 3 years' experience in designing and implementing Snowflake architecture, features, and capabilities, including Snowflake's multi-cluster architecture, data sharing, semi-structured data support, and security features Working knowledge and demonstrated experience designing data models with a focus on Data Vault 2.0 methodology as well as working knowledge of different approaches such as dimensional (Star, Snowflake) Demonstrated proficiency in SQL Strong understanding of cloud computing principles, cloud-native architectures, and best practices for deploying and managing data solutions on cloud platforms Hands-on experience with cloud data platforms such as AWS, Azure, or GCP and familiarity with data services Excellent leadership, communication, and interpersonal skills to effectively lead project teams, collaborate with clients, and influence decision-making at executive levels. Technical Skills/Certifications: - SnowPro Core, SnowPro Advanced Architect (considered an asset) / - Data Vault 2.0 (considered an asset) Proven track record of delivering complex data architecture projects on time and within budget, managing client relationships, and driving business outcomes."
First Derivative, Big Data Developer, "Job Description: - Bachelor's Degree or Master's in Computer Science, Engineering, Software Engineering or a relevant field. - 10+ years of software development experience building large scale distributed data processing systems/application or large-scale internet systems. - Must have good communication skills, writing clean code. - Strong experience in Python, Kafka, Spark, SQL, Hive, Couchbase, Impala, Hadoop, Splunk, Autosys, shell scripting etc. - Strong Cloud experience on AWS Lambda, AWS EMR, AWS S3, AWS EC2, EKS. - Experience in DevOps Activity (Snow, Build (TeamCity, Jenkins, Terraform) and U-Deploy. - Good knowledge on Capital Market and Data Analysis. - Experience in dealing with Huge Volume of Data operations. - Design best architecture and select the most appropriate modeling techniques and data visualization for big data analysis. - Performance analysis, troubleshooting and resolution (this includes familiarity and investigation of Cloudera/Hadoop logs) - Experience with Linux-based infrastructures, Linux/Unix administration, and Cloud."
Gusto, Machine Learning Engineer, "Here's What You'll Do Day-to-day Build and deploy machine learning models to identify, assess and mitigate risks Responsible for driving research in the problem space, working with stakeholders to understand model requirements, developing the model from scratch, deploying the model alongside your engineering counterparts, and monitoring and maintaining the model's performance over time Partner with Engineering, Design, and Product counterparts in Payment and Risk to solve complex cross functional problems Develop scalable frameworks and libraries that enhance and contribute to the team's core analysis and modeling capabilities Identify new opportunities to leverage data to improve Gusto's products and help risk management team to understand business requirements and develop tailored solutions Present and communicate results to stakeholders across the company Here's What We're Looking For 8+ years experience conducting statistical analyses on large datasets and deep domain knowledge in machine learning. This could mean either a MS or PhD in a quantitative field with at least 5 years experience in a business environment, or BS or Data Science Bootcamp graduate with at least 8 years of experience working as a data scientist or a machine learning engineer in a business setting. Experience applying a variety of statistical and modeling techniques using Python, R or another statistical modeling language, as indicated by familiarity with many of the following techniques - predictive modeling, anomaly detection, ensemble methods, natural language processing (NLP, optional). Strong programming skills - comfortable with all phases of the data science development process, from initial analysis and model development all the way through to deployment Excellent communication skills - able to effectively deliver findings and recommendations to non-technical stakeholders in a clear and compelling fashion PhD or Masters plus equivalent experience in a quantitative field is a plus"
The Citco Group Limited, Machine Learning Engineer, "Your Role You will design, develop and implement software solutions to support business requirements. Be involved in concept design, brainstorming, research, etc. required to create new solutions for consideration Resolve technical issues through debugging, research and investigation Participate in performance/load tests, making recommendations to improve performance and scalability Provide timely and effective solutions to critical production issues Contribute to continuous integration and continuous deployment automation Document solutions and keep knowledgebase up-to-date to support efficiencies About You You have 4 years of experience in Python 2 years of experience building Machine Learning models using Sklearn, TensorFlow or PyTorch 1+ year experience leveraging LLM's for business use cases including RAG and Agents 1+ year experience using Databricks for data orchestration and ML model development 2 years of experience with relationships databases 2 years experience in AWS or other cloud Proficiency in designing and implementing cloud-based applications in AWS preferred Experience with serverless architecture is a plus Experience working with Linux shell scripting an asset Must have strong communication skills and be able present diagnostic, troubleshooting steps and conclusions to a varied audience Must be able to work with minimal supervision Demonstrated expertise in supporting large-scale systems Strong interpersonal and communication skills and ability to deal effectively in a team environment"
Metaguest, Senior Machine Learning Engineer, "In this role, you will: Design advanced machine learning models for guest personalization, predictive analytics, and natural language processing. Deploy scalable solutions that improve guest engagement and operational efficiency. Develop integrations with Property Management Systems (PMS) to centralize fragmented data and provide seamless hotel operations. Implement solutions like real-time folio queries, digital tipping, unattended POS systems, and predictive maintenance interfaces to enhance guest experiences. Collaborate with cross-functional teams to identify opportunities for AI-driven insights and continuously improve existing models for accuracy and scalability. Manage the full lifecycle of machine learning models, including development, testing, deployment, and performance monitoring. Build MLOps pipelines to ensure robust and efficient deployment processes for enterprise-scale applications. You will thrive in this role if you: Have extensive experience in deep learning frameworks like PyTorch, TensorFlow, and scikit-learn, and are proficient in Python. Are skilled at building and fine-tuning Large Language Models (LLMs) and understand techniques like distillation, policy optimization, and supervised learning. Are familiar with NLP, recommendation systems, and predictive modeling, with the ability to apply these to real-world problems. Have a strong background in cloud computing platforms like AWS or GCP for model deployment and optimization. Have a proactive approach to challenges and excel in collaborative team environments. Have a Master's or Ph.D. in Computer Science, Machine Learning, or a related field (preferred)."
Lyft, Senior Machine Learning Engineer, "Responsibilities: Partner with Engineers, Data Scientists, Product Managers, and Business Partners to apply machine learning for business and user impact Perform data analysis and build proof-of-concept to explore and propose ML solutions to both new and existing problems Develop statistical, machine learning, or optimization models Write production quality code to launch machine learning models at scale Evaluate machine learning systems against business goal Qualifications: B.S., M.S., or Ph.D. in Computer Science or other quantitative fields or related work experience 4+ years of Machine Learning experience Passion for building impactful machine learning models leveraging expertise in one or multiple fields. Proficiency in Python, Golang, or other programming language Excellent communication skills and fluency in English Strong understanding of Machine Learning methodologies, including supervised learning, forecasting, recommendation systems, reinforcement learning, and multi-armed bandits"
Qohash, Senior Machine Learning (AI) Developer, "As a Senior Machine Learning (AI) Developer, your responsibilities will be as follows: Package LLM models, incorporating Generative AI, deploy to production environments, and monitor usage and performance. Collaborate with engineers and product managers to integrate Generative AI features into existing systems. Experiment with different Generative AI techniques and architectures to optimize both delivery and performance. Evaluate and validate the effectiveness of Generative AI models using appropriate metrics. Manage ML infrastructure in the cloud. Test models at scale and ensure CI/CD best practices are followed by the whole team. Diagnose and resolve ML workflow and production issues quickly Design, develop, and maintain machine learning algorithms that work on machines with limited resources as well as in cloud environments Create and maintain scalable feature pipelines. Integrate the algorithms in the current systems, in collaboration with the engineering teams Write production-level code to convert ML models into working pipelines Correct anomalies and problems as they arise. Test the implemented features to make sure all acceptance criteria are met Participate in projects from the initial idea to launch. Contribute to the continuous improvement of development activities (agility, automated tests, deployment, etc.). Collaborate with the security team to integrate security best practices into software development processes. Communicate risks associated with any activity, technology, or processes as you identify them Stay up to date with developments in the machine learning industry What your resume shows Must Haves 5+ years of experience with the following ML model techniques: LLMs, Deep Learning, traditional ML and predictive modeling 5+ years of experience with Python programming 3+ years experience with building, validating, deploying and monitoring production models Strong skills in using generative AI to speed up features delivery Strong understanding of Generative AI techniques and models. Experience with ML frameworks such as Tensorflow, PyTorch. Experience with infrastructure and tooling for MLOps Excellent communication and collaboration skills"
SoundHound AI, Principal Machine Learning Engineer, "In this role, you will: Drive innovation and advancements in the core architecture of Automatic Speech Recognition (ASR). Lead and contribute to cutting-edge research in Machine Learning (ML), with a focus on ASR, natural language understanding, dialogue generation, and context-aware conversational systems. Develop novel algorithms and models to enhance the performance and intelligence of our ASR platform. Train and optimize large-scale deep learning ASR models. Collaborate closely with cross-disciplinary teams, including machine learning engineers, software developers, and product managers, to translate research insights into practical solutions. Mentor and support junior researchers and engineers, fostering a culture of continuous learning and innovation. Stay informed on the latest developments in ASR and large language models (LLMs), driving the ongoing enhancement of our technology stack. We would love to hear from you if: You have a Ph.D. in Computer Science and bring at least five years of relevant, hands-on industry experience, or a Master's in Computer Science with at least eight years of industry experience. You have a proven track record of impactful research and development in Machine Learning and ASR, with a portfolio of publications and projects that showcase your expertise. You are highly skilled in deep learning frameworks (e.g., TensorFlow, PyTorch) and have experience with training large-scale models. You have strong programming abilities in Python, enabling you to implement and optimize ASR models effectively. You are adept at solving complex problems and developing innovative solutions in the realm of conversational AI. You are passionate about staying at the cutting edge of AI research and are driven to push the boundaries of what's possible in conversational AI. You are excellent in teamwork and communication, collaborating seamlessly with cross-functional teams. You are proficient in C/C++ and possess strong overall software engineering expertise."
Coinbase, Senior Machine Learning Engineer, "What you'll be doing (ie. job duties): Investigate and harness cutting-edge machine learning methodologies, including deep learning, large language models (LLMs), and graph neural networks, to address diverse challenges throughout the company. These challenges encompass areas such as fraud detection, feed ranking, recommendation systems, targeting, chatbots, and blockchain mining. Develop and deploy robust, low-maintenance applied machine learning solutions in a production environment. Create onboarding codelabs, tools, and infrastructure to democratize access to machine learning resources across Coinbase, fostering a culture of widespread ML utilization. What we look for in you (ie. job requirements): Experience with at least one ML model: LLMs, GNN, Deep Learning, Logistic Regression, Gradient Boosting trees, etc. Working knowledge in one or more of the following: data mining, information retrieval, advanced statistics or natural language processing, computer vision. PhD degree strongly preferred with 3+ years of relevant experienceOr MS degree in Computer Science, Machine Learning, Data Mining, Statistics, or related technical field with 5+ years of relevant experience Experience with Python. Experience with data analysis and visualization."
Coinbase, Machine Learning Engineer, "What you'll be doing (ie. job duties): Hands-on develop, productionize, and operate Machine Learning models and pipelines at scale Contribute to our ML models for Risk, work at the intersection of Blockchain and AI technologies Work with our senior engineers and collaborate with our product partners to identify and solve new use cases for ML on blockchain What we look for in you (ie. job requirements): 2+ yrs of industry experience as a Machine Learning Engineer Solid software engineering skills Experience with coding on ML platforms (e.g., Tensorflow, PyTorch) Experience with basic ML techniques (e.g., supervised and unsupervised learning) Willingness to learn and adapt to new technologies and challenges Nice to Have: Master's degree or PhD in Machine Learning, Computer Science or related field Interest in DNNs, GANS, GNNs and Time Series modeling is a plus"
Coinbase, Mid-level Software Developer AI/ML, "WHAT YOU'LL DO: Design and build our AI platform used by multiple features across products Implement using API-first, TDD and CICD, our core AI capabilities to allow new AI-powered features across teams and products (NLP, retrieval, recommendation, generation, etc.) Continuously improve our core AI capabilities leveraging state-of-the-art algorithms and methods Balance decisions for cost, quality, performance, personalization and responsibility Operate our AI platform to serve operational & customer functionality Diagnose and mitigate AI systems failure Setup the right metrics and bias evaluation to monitor AI systems quality Apply AI development best practices, including DevOps & MLOps mindset Translate business and operational needs into AI solution Collaborate with product managers, user experience researchers and other software developers to understand customer problems Brainstorm ideas in groups, make hypotheses, validate them methodically and present your findings to your peers for review. Work with the following technologies: Python, LLM (agents, tools, evaluation), AI frameworks/library (LangChain, PyTorch, sklearn, transformers, pandas, etc.), ElasticSearch, AWS, Docker, Kubernetes, RESTful API WHAT YOU'LL NEED: A degree in Computer Science or Engineering with a specialization in artificial intelligence (i.e. machine learning, NLP, recommendation, generative, etc.), and at least 3 years of experience in developing and maintaining software, or an equivalent level of education or work experience, and a track record of substantial contributions to AI projects with high business impact You are proficient in Python using ML/DL frameworks and libraries and at least familiar with one of Node.js or Go Demonstrated experience working with a team on medium-to-large sized projects that had an impact on business, including an understanding of releasing and iterating on AI models used by customers. Any experience with LLM-based project will be well perceived You know how to apply best practices to ensure high-quality code and AI systems (i.e. automated tests; code reviews; metrics and bias evaluation). You have experience in AI systems design: analyze a problem & data from a customer perspective with an AI lens, design data flow and algorithms (ranking, classification, prediction, clustering), drive discussions, and propose technical solutions. Open Communication: clearly conveys thoughts, both written and verbally, listening attentively and asking questions for clarification and understanding Collaboration and Teamwork: works with others to deliver results, meaningfully contributing to the team and prioritizing group needs over individual needs Self Development: is personally committed to, and actively works to continuously improve Problem Solving: uses an organized and logical approach to find solutions to complex problems. Looks beyond the obvious to understand the root cause of problems Resilience, Tolerance for Change/Ambiguity: can effectively cope with change, finding ways to advance work and projects Creativity and Innovation: seeks new and better ways of doing things, generates original and imaginative ideas, products, or solutions"
W3Global, Data Engineer, "Job Overview We are looking for a skilled Data Engineer and ETL Developer to design and manage data pipelines, ETL processes, and cloud-based data solutions. You will work with cutting-edge tools and platforms to ensure efficient data integration and processing Key Responsibilities: Develop and maintain ETL workflows using Ascend.io / Informatica / IICS. Automate and strong python knowledge to build the pipeline with Airflow Composer. Build scalable data pipelines on Google Cloud Platform (GCP). Optimize data storage and querying in BigQuery / DBMS. Write and optimize SQL queries for data extraction and analysis. Use Python to automate and streamline data processes. Collaborate with teams to meet data requirements and troubleshoot issues. Skills & Qualifications: Proficiency with Ascend.io, Informatica, Airflow Composer, and GCP services. Strong SQL and Python programming skills. Experience in data pipeline design and cloud data management, particularly in BigQuery. Solid problem-solving skills and ability to work collaboratively."
Kraft Heinz, Machine Learning Engineer Lead, "What's on the menu? Lead, manage and develop the ML Engineering group in the Toronto office Responsible for the internal and external labor applied to ML Engineering projects. Manage the ML engineering budget and estimation process. Implement machine learning algorithms and customized libraries Assist Data Science team with development of complex tools, models or database builds Collaborate with data engineers and data scientists to develop data and model pipelines Improve existing Machine Learning models Recipe for Success Apply now if this sounds like you! I have an understanding of the major modelling techniques are how to apply them (Linear and Logistics Regression, Bayesian, Time series, Confidence interval, Deep Learning etc.) I have experience in managing technical teams delivering and supporting analytical capabilities. I have experience supporting ML models in a production cloud environment (Azure experience preferred) I have hands-on experience with analytical tools (preferably AzureML, Vertex, SageMaker, Snowflake, DataBricks, Tableau, Alteryx, SQL, Python, and/or R) I have working knowledge on LLM operations (including fine tuning and grounding techniques) and prompt engineering best practices I have strong experience developing CI/CD pipelines I am fluent in version control tools and practices (Github, Azure DevOps)."
Rakuten Rewards, Senior ML Platform Engineer, "Key Responsibilities Develop and maintain scalable machine learning platforms, tools, and frameworks to support various AI/ML use cases, including data processing pipelines, model training workflows, and model serving infrastructure. Work closely with cross-functional teams including data scientists and software engineers to understand requirements and translate them into scalable technical solutions. Create and execute deployment strategies and structures for integrating generative AI models into production environments. Continuously optimize and improve data processing pipelines, model training workflows, and infrastructure to enhance efficiency, performance, and scalability. Build and maintain tools for model versioning, experimentation, evaluation, and deployment to facilitate rapid iteration and development. Collaborate with cross-functional teams to integrate AI/ML capabilities into existing products and services, ensuring seamless integration and alignment with business objectives. Document development processes, and technical specifications of AI/ML Platform, communicating effectively with stakeholders, team members, and collaborators. Minimum Requirements Proficiency in programming languages commonly used in machine learning, such as Python, Java, or Scala. Familiarity with distributed computing frameworks such as Apache Spark, Ray or TensorFlow. Knowledge of cloud computing platforms such as AWS, GCP, or Azure. Experience with designing and implementing data processing pipelines, model training workflows, and model serving infrastructure. Experience with training and fine-tuning language models on large-scale datasets. Proficient in developing customer support chatbots utilizing Large Language Models (LLMs) and Agent Frameworks. Capability to work in a fast-paced and dynamic environment, adapting to changing priorities and requirements. Strong communication skills, with the ability to effectively convey complex technical concepts to diverse audiences. Requirements QUALIFICATION REQUIREMENTS Bachelor's degree in computer science, Computer Engineering, relevant technical field, or equivalent practical experience. 5+ years of work experience in AI/ML Engineering/Data Science 3+ years of work experience in AWS (EC2, S3, SageMaker) Experience in container technologies like Docker, Kubernetes. Experience in productionizing Large Language Models (LLMs) and Agent frameworks."
ServiceNow, Staff Machine Learning Developer, "In This Role, You Will Enable large-scale model training projects that aim to significantly enhance our AI capabilities and ensure they are perfectly adapted to our platform. Take a lead in enhancing our 3D-parallelism training framework, aiming to make it more feature-complete and further increasing its speed and efficiency. This involves identifying compute bottlenecks and implementing critical tensor operations using OpenAI's Triton JIT compiler to maximize GPU utilization and training throughput. Design, implement, and optimize novel language model architectures that prioritize inference speed and scalability, aligning with ServiceNow's operational needs. Contribute to broadening our training framework's support for a diverse range of model architectures, balancing maintainability and stability with the need for innovative features and experimentation. Contribute to the open-sourcing efforts of our frameworks by ensuring they are well-documented, user-friendly, and ready for external adoption. To be successful in this role you have: A Master's degree with at least 5 years of relevant experience, or a Ph.D. in computer science, engineering, or a related field with 3 years of experience. Extensive experience in GPU programming for deep learning, with a proven track record of optimizing large-scale systems for performance and efficiency. Demonstrable skills in designing and implementing innovative deep learning architectures, preferably with experience in large language models. Proficiency in one or more major deep learning frameworks, such as PyTorch or JAX. Significant experience with CUDA and familiarity with OpenAI Triton are highly desirable. Strong collaborative skills with the ability to work in a dynamic environment, with a track record of driving projects to completion. Excellent problem-solving skills and the ability to work independently on complex issues. Effective communication skills, capable of articulating complex technical details and collaborating across teams."
BenchSci, Senior Machine Learning Engineer, "You Will: Continuously improve the performance and scalability of ML models that are at the core of BenchSci's products Leading and providing technical direction for BenchSci's ML team Work with BenchSci's Product Managers and Scientists to correctly capture the nuances of biology Lead or consult the authoring of engineering design proposals following the unified Platform Stream roadmap at BenchSci Leverage a deep understanding of the business context and the team's goals to unlock independent technical decisions in the face of open-ended requirements Proactively identify new opportunities (from both internal and external sources) and advocate for and implement improvements to the current state of projects Respond with urgency and drive urgency in own team to operational issues, owning resolution within one's sphere of responsibility Advocate for code and process improvements across your team, and help to define best practices based on personal industry experience and research Participate in sprint planning, estimation and reviews. Take ownership of deliverables, and work with teammates to ensure high-quality deliverables You Have: 5+ years of software development experience Bachelor's degree in Computer Science or Mathematics Strong experience with NLP Experience with computer vision problems A degree in computer science with a focus in machine learning and at least three years of experience in the industry Strong experience with TensorFlow, PyTorch, and image processing libraries such as OpenCV and scikit-image Experience with data processing frameworks You have a constant desire to grow and develop You have strong cross-team communication and collaboration skills A team player who strives to see teammates succeed together Nice to have: Experience with GCP and its tools including VertexAI and BigQuery Experience implementing proprietary LLMs/Multimodal LLMs like Gemini or GPT series Experience implementing and/or fine-tuning open-sourced LLMs/Multimodal LLMs Research publications in ML/AI-related fields"
Pinterest, Senior Machine Learning Engineer, "What You'll Do Use Python, HuggingFace and PyTorch to train/develop models to extract data from webpages & text Develop features to represent webpages Employ strong software engineering skills to develop frameworks or tools to help productionize our models Research ML/AI techniques to apply to our models Evaluate accuracy and post launch impact Mentor interns and other engineers What We're Looking For 5+ years of industry experience applying machine learning methods -Expertise in NLP applications like NER, information extraction and LLMs -Experience training and optimizing DNN models Nice To Have M.S. or PhD in Machine Learning or related areas Publications at top ML conferences Experience in large scale data processing systems like Presto or Spark Experience building and managing text based datasets and labels"
CrowdStrike, Senior Machine Learning Engineer, "What You'll Do Innovate with the state of the art machine learning technology to accelerate data science. Focus on scalability, and cost effectiveness. Implementing monitoring and providing in-depth analysis to identify potential vulnerabilities or gaps Create backend microservices, pinpointing and addressing critical concerns to implement effective solutions. Employ established CrowdStrike tools and services to build solutions for detecting and countering targeted cyber assaults. Construct and maintain data pipelines,and contribute to the training and implementation of custom models. Collaborate across various teams to brainstorm, define, and devise solutions. Commit to ongoing learning and self-improvement. Stay attuned to our customers' challenges, always seeking ways to enhance support. Emphasize top-tier coding quality by adhering to best practices, rigorous testing, and thorough logging and metrics. Work within a collaborative and agile team environment. Contribute to mentoring fellow engineers across a spectrum of technologies and also absorb knowledge from them. Constantly explore ways to refine product architecture, knowledge models, user experience, performance, and reliability. Own your work with autonomy, end to end: develop, test, deploy and monitor your changes. Thrive in an environment that highly values trust. Tech Stack (not mandatory to know everything; a robust learning capacity is essential): High level coding language such as JVM technologies or Python Docker Kubernetes AWS, GCP, or MaaS Kafka, Cassandra, and Spark ElasticSearch Terraform, Chef, or Ansible What You'll Need Any STEM undergraduate degree Prior work experience with big data and microservices Understanding scalability and distributed systems e.g. sharding, partitioning, and concurrency Be a team player A thorough understanding of engineering best practices from appropriate testing paradigms to effective peer code reviews and resilient architecture. The ability to thrive in a test-driven, collaborative, and iterative programming environment. The skills to meet your commitments on time and produce high-quality software that is unit tested, code reviewed, and checked in regularly for continuous integration. Bonus Points Existing exposure to AWS and distributed systems Experience with Language Models, Data Science, and Data Engineering"
Cresta, Staff Machine Learning Engineer, "Responsibilities Design, develop, and deploy Cresta's AI Agent solutions and proprietary models. Focus on practical AI challenges such as improving reasoning, planning capabilities, and evaluation in real-world scenarios. Collaborate with cross-functional teams including front-end and back-end software engineers to integrate AI Agents into Cresta's customer solutions. Lead initiatives to scale AI systems for production environments, ensuring performance and reliability across use cases. Contribute to solving cutting-edge problems in AI and help define the future roadmap for Cresta's AI Agents. Innovate and research ways to improve security, cost-efficiency, and reliability of AI systems. Qualifications We Value Bachelor's Degree in Computer Science, Mathematics, or a related field; Master's or Ph.D. preferred, or equivalent professional experience 7+ years of hands-on industry experience with AI and machine learning, preferably with 3+ years of experience working with LLMs in large-scale production environments Expert knowledge of machine learning concepts and methods, especially those related to NLP, Generative AI, and working with LLMs Proven leadership in designing and deploying AI solutions at scale, with a deep understanding of model optimization and real-time AI applications Extensive practical knowledge of modern machine learning frameworks and technologies (e.g., PyTorch, Tensorflow, Hugging Face, NumPy), as well as experience with distributed systems and cloud-based AI infrastructure Strong problem-solving and strategic thinking abilities, with a proven ability to lead cross-functional teams and work collaboratively to deliver innovative AI solutions in production"
Coveo, Senior Machine Learning Developer, "Here is a glimpse at your responsibilities: Participate directly in every part of the development life cycle of ML models: conception, implementation, automated testing, deployment, monitoring, etc. Investigate, analyze and improve the performance of our models and systems - including meeting critical SLOs for training models at scale and low-latency inference. Maintain and improve several of our most important client-facing product features. Facilitate the adoption and utilization of ML platform and observability resources and guidelines to improve operational efficiency and service reliability. Engage with your community of peers to challenge the status quo, improve our shared ways of working, and influence overall architecture decisions. Learn, utilize and evolve our data and tech stack which includes Python, AWS, Kubernetes, Pytorch, Terraform, Snowflake, Honeycomb and others Here is what will qualify you for the role: You have 5+ years of Machine Learning industry experience. You have operationalized, instrumented and supported AI models in production at a non-trivial scale before You are fluent in good data and software engineering practices, and you are able to develop the tools and culture which enable your team to deliver reliable production code in an efficient manner. You enjoy collaborating with scientists on a daily basis to understand their pain points and figure out how to improve their tools and increase their efficiency. You also have experience working in cross-functional teams. Here is what will make you stand out: You know when to apply best practices in ML-Ops, ML engineering, and large-scale deployment of ML models. You are familiar with Data Lifecycle in ML and helped structure the data logic in ML projects before. You have been tasked with maintaining and evangelizing internal resources and libraries. You are savvy in specific ML domains such as Natural Language Processing, Information Retrieval, and/or Recommendation Systems, and the technical implications of building pipelines for such use cases. You are recognized for your communication skills and presenting complex technical subjects to audiences with different levels of technical proficiency."
SecurityScorecard, Senior Machine Learning Engineer, "Responsibilities: Technical Leadership: Establish best practices and share expertise through mentorship. Model Development: Design, train, and optimize machine learning models and algorithms. Data Pipeline Creation: Build and maintain scalable data pipelines to preprocess, clean, and transform raw data for analysis and model training. Model Deployment: Implement and manage models in production environments, ensuring scalability, reliability, and performance. Research and Experimentation: Stay updated on the latest machine learning techniques, tools, and frameworks to enhance model accuracy and efficiency. Collaboration: Work closely with data scientists, software engineers, and product teams to understand requirements and integrate ML solutions into products. Performance Monitoring: Continuously monitor, evaluate, and fine-tune models post-deployment to maintain accuracy and robustness. Documentation: Create clear and concise documentation for models, processes, and systems to support team collaboration and knowledge sharing. Required Qualifications: 5+ years of experience or equivalent demonstrable skills in ML Engineering, Data Science or related discipline. Bachelor's or Master's degree in Computer Science, Engineering, Mathematics, Physics, or a related field. Strong programming skills in Python. Experience with machine learning frameworks such as PyTorch, TensorFlow, or Scikit-learn. Proficiency in data manipulation and analysis using tools such as Polars, Pandas, NumPy, or SQL. Solid understanding of algorithms, statistics, and data structures. Experience with cloud platforms (AWS, Azure, GCP) and containerization (Docker, Kubernetes). Knowledge of CI/CD pipelines and version control systems (e.g. Git). Familiarity with Linux/Unix command line tools. Preferred Qualifications: PhD degree in Computer Science, Engineering, Mathematics, Physics or a related field. Hands-on experience with LLMs, RAG, LangChain, or LlamaIndex. Experience with big data technologies such as Hadoop, Spark, or Kafka."
Manulife, Senior Machine Learning Engineer, "Responsibilities Develop feature engineering components to source and acquire structured and unstructured data to support modeling efforts. Strong emphasis on data modeling techniques during feature engineering and system design Collaborate with Data Engineers and IT Domain Experts to understand source data and formulate hypotheses and insights prior to model design , using multi signal data sources. Develop and deploy machine learning models, to solve business problems using variety of algorithms and techniques like LLM , Generative AI & RAG; using Azure and Databricks platforms Work closely with partners to gather requirements and implement solutions that meet their needs Embrace best practices, processes related to MLOps , Coding standard methodologies for the broader data science and engineering teams. Experience in optimizing model accuracy and runtime performance using techniques such as model compression and parallelization Mentor and provide guidance to junior team members, fostering a culture of continuous learning and improvement Stay up to date with industry trends and advancements in Machine Learning, Generative AI, Prompt engineering, Cloud and related technologies What We Are Looking For An advanced degree in Machine learning, Data Science, Computer Science, Engineering, or Statistics Minimum of 5 years of experience solving high-impact business problems using Machine learning & Data science Proven experience as a Data/ML Engineer, with a strong focus on machine learning, generative AI, prompt engineering, and RAG applications. Hands-on experience with Azure tech stack, including Azure cognitive search, Langchain, Vector stores. Experience in Azure OpenAI and open-source models like Llama, is a bonus Strong partner management, ability to translate sophisticated technical topics into business language, presentation skills, and an ability to balance a sense of urgency with shipping high-quality and pragmatic solutions. Strong software development skills with proficiency in Python and advanced working SQL knowledge, preferably using the Azure stack and experience with Azure ML, Databricks, Azure Data Factory. Experience in productionizing code through the DevOps pipeline (git, Jenkins pipeline, code scan). Solid understanding of machine learning algorithms, statistical modeling, and data analysis techniques."
Manulife, Senior AI Engineer Coach, "Champion the transformation of technology staff within Manulife by sharing your technical expertise as part of the Manulife University program. Contribute towards content/technical lab creation and lead the delivery of up-skilling efforts. Embrace an entrepreneurial spirit and comfort working within a constantly evolving environment; you love the challenge of being part of a larger movement to change the technology culture of an enterprise. Strong verbal and written communication with the ability to effectively articulate technical vision, possibilities, and outcomes to influence partners across all levels in the organization. Advocate and influence the design principles which align with our global technology strategy and architecture including AI, cloud and software engineering. Prototype and build new functionality to deliver on key product objectives with strong and extendable architectural design. Be an active contributor in our various internal and external transformation events such as hackathons, demo days, tech talks, and external engineering recruitment campaigns. You Will Bring And Continuously Build Upon The Following Excellent leadership and mentorship abilities to coach and lead others. Proven track record of leading artificial intelligence and machine learning (AI/GenAI/ML) initiatives and fostering a responsible AI culture. Understand model validation and governance strategies for a wide range of AI, GenAI and Machine Learning models through the lifecycle. Able to perform replication tests and develop benchmark models against existing AI/ML solutions. Stay updated with the latest developments in data science (including GenAI research), and the latest regulatory developments concerning model risk (e.g. E-23). Critically challenge the existing AI/ML solutions including documenting model limitations and recommend changes for future model enhancements. Stay apprised of the Advanced Analytics Model Inventory. Code and automate infrastructure using various technologies in Azure. Preferred Skills And Experience Graduate degree (Master's/PhD) in Computer Science, Data Science, Statistics, Engineering, Mathematics or a related quantitative field. Proficiency in programming languages such as Python and experience with machine learning libraries/frameworks, e.g., PyTorch, scikit-learn, Hugging Face, SQL, graph databases (Neo4j/Cypher, Cosmos DB/Gremlin). Experience with Microsoft Azure (AI) services and copilots."
SoundHound AI, Senior Machine Learning Engineer - Large Language Models, "In this role, you will: Lead and contribute to cutting edge development in Generative AI, focusing on natural language understanding, dialogue generation, and context-aware conversational systems Develop novel algorithms and models for enhancing the performance and intelligence of our conversational AI platform Train and fine-tune large-scale deep learning models for natural language processing and dialogue generation Collaborate with cross-functional teams, including machine learning engineers, software developers, and product managers, to integrate research findings into our products and solutions Remain at the forefront of Generative AI advancements, actively contributing to the evolution and enhancement of our technological infrastructure. We would love to hear from you if: You have a Masters or Ph.D. in Computer Science and at least three years of relevant, full-time industry experience You have proven expertise in impactful research and development in Generative AI, with relevant projects demonstrating your expertise You are highly proficient in deep learning frameworks (e.g., TensorFlow, PyTorch) and have experience with large-scale model training You have in-depth knowledge of natural language processing (NLP) techniques, including pre-trained language models, text generation, and dialogue systems You have strong programming skills in Python and can implement and optimize AI models You can tackle complex challenges and develop innovative solutions in the field of conversational AI You have excellent teamwork and communication skills, with the ability to work effectively with cross-functional teams"
ClearVision Technologies Inc., Senior Computer Vision Developer, "Duties & Responsibilities: Development of Machine Vision algorithms that are efficient and robust. Working with other Software and Mechatronics developers to produce a comprehensive quality assurance solution. Testing of algorithms in an industrial environment. Required Experience: 8+ years of Machine Vision programming experience. 6+ years working in industry. 4+ years of either C++ or C# experience. 4+ years of OpenCV experience. 2+ years of Machine Learning. Required Competencies: Ability to create reliable machine vision algorithms that work well under varying lighting and imaging conditions. Ability to use creative ideas to design fast and well-optimized algorithms. Ability to create and train datasets to produce robust models that can be deployed to customer sites. Preferred Competencies: Visual Studio. Git, source code revision control. Experience in vision-based print inspection. Preferred Personal Skills: Good communication skills. Good troubleshooting and problem solving. Customer oriented. Education: Bachelor's degree in engineering- or science-related field. Master's degree preferred."
Affirm, Staff Machine Learning Engineer, "What You'll Do Use Affirm's proprietary and other third party data to develop machine learning models that predict the likelihood of default and make an approval or decline decision to achieve business objectives Partner with platform and product engineering teams to build model training, decisioning, and monitoring systems Research ground breaking solutions and develop prototypes that drive the future of credit decisioning at Affirm Implement and scale data pipelines, new features, and algorithms that are essential to our production models Collaborate with the engineering, credit, and product teams to define requirements for new products What We Look For 8+ years of experience as a machine learning engineer. Relevant PhD can count for up to 2 YOE Experience developing machine learning models at scale from inception to business impact Proficiency in machine learning with experience in areas such as Generalized Linear Models, Gradient Boosting, Deep Learning, and Probabilistic Calibration Strong engineering skills in Python and data manipulation skills like SQL Experience using large scale distributed systems like Spark or Ray Experience using open source projects and software, such as scikit-learn, pandas, NumPy, XGBoost, PyTorch, Kubeflow Experience with Kubernetes, Docker, and Airflow is a plus Excellent written and oral communication skills and the capability to drive cross-functional requirements with product and engineering teams"
Affirm, Staff Machine Learning Engineer, "What You'll Do Use Affirm's proprietary and other third party data to develop machine learning models that predict the likelihood of default and make an approval or decline decision to achieve business objectives Partner with platform and product engineering teams to build model training, decisioning, and monitoring systems Research ground breaking solutions and develop prototypes that drive the future of credit decisioning at Affirm Implement and scale data pipelines, new features, and algorithms that are essential to our production models Collaborate with the engineering, credit, and product teams to define requirements for new products What We Look For 8+ years of experience as a machine learning engineer. Relevant PhD can count for up to 2 YOE Experience developing machine learning models at scale from inception to business impact Proficiency in machine learning with experience in areas such as Generalized Linear Models, Gradient Boosting, Deep Learning, and Probabilistic Calibration Strong engineering skills in Python and data manipulation skills like SQL Experience using large scale distributed systems like Spark or Ray Experience using open source projects and software, such as scikit-learn, pandas, NumPy, XGBoost, PyTorch, Kubeflow Experience with Kubernetes, Docker, and Airflow is a plus Excellent written and oral communication skills and the capability to drive cross-functional requirements with product and engineering teams"
Affirm, Staff Machine Learning Engineer, "What You'll Do Use Affirm's proprietary and other third party data to develop machine learning models that predict the likelihood of default and make an approval or decline decision to achieve business objectives Partner with platform and product engineering teams to build model training, decisioning, and monitoring systems Research ground breaking solutions and develop prototypes that drive the future of credit decisioning at Affirm Implement and scale data pipelines, new features, and algorithms that are essential to our production models Collaborate with the engineering, credit, and product teams to define requirements for new products What We Look For 8+ years of experience as a machine learning engineer. Relevant PhD can count for up to 2 YOE Experience developing machine learning models at scale from inception to business impact Proficiency in machine learning with experience in areas such as Generalized Linear Models, Gradient Boosting, Deep Learning, and Probabilistic Calibration Strong engineering skills in Python and data manipulation skills like SQL Experience using large scale distributed systems like Spark or Ray Experience using open source projects and software, such as scikit-learn, pandas, NumPy, XGBoost, PyTorch, Kubeflow Experience with Kubernetes, Docker, and Airflow is a plus Excellent written and oral communication skills and the capability to drive cross-functional requirements with product and engineering teams"
Mobsquad, Machine Learning Engineer, "ABOUT YOU You have an advanced degree (M.S. or PhD) in Computer Science, Engineering, Mathematics, Physics, or a comparable analytical field from an accredited institution You have over five years of experience working with deep learning frameworks (TensorFlow, Keras) You have over five years of experience with relevant languages (Python, Java) and libraries (scikit-learn, Pandas) You have over five years of experience developing unique algorithms You have strong experience creating and deploying machine learning models You have demonstrated knowledge of relevant libraries and operating systems (OpenCV, Linux) You have knowledge of SQL (MySQL, PostgreSQL) and NoSQL (MongoDB, Cassandra, HBase) databases You have strong attention to detail, translating to strength in data quality verification to enable clean data at all times"
The University of British Columbia, Machine Learning Engineer, "Candidate will be expected to; Develop, apply, and iteratively improve AI guided modeling techniques and scientific workflows to accelerate protein design and antibody optimization campaigns via design-make-test-analyze cycles Explore use of deep learning models to improve antibody design Develop unit testing and regression testing suite for codebase Generate quarterly progress reports on project advances and present at weekly team meetings Collaborate closely with PROGENITER team members working on experimental aspects of biologics design including cryo-EM structural analysis, biochemistry and neutralization mechanisms Stay abreast of the latest advances in the machine learning field and adapt relevant concepts into internal scientific workflows, as needed. Provide strategic guidance and advice on general aspects of implementing computational aspects of protein design approaches for PROGENITER Serve as the primary contact for programming and code implementation needs in the program Minimum Qualifications Undergraduate degree in Engineering or Applied Science. Minimum of three years of related experience, or the equivalent combination of education and experience. Willingness to respect diverse perspectives, including perspectives in conflict with one's own Demonstrates a commitment to enhancing one's own awareness, knowledge, and skills related to equity, diversity, and inclusion Preferred Qualifications MSc or PhD degree in computer science, applied math, statistics, bioinformatics, physics, chemistry or a related discipline. Two or more years' experience working with major deep learning frameworks (PyTorch, TensorFlow, JAX etc.) implementing modern deep learning models such as graph neural networks, Transformers, diffusion models. Experience working with representation learning and generative AI models. In-depth understanding of modern and classical machine learning (ML) methods with practical experience designing, training, and validating such algorithms. Experience building scalable, optimized scientific software, and knowledge of GPU computation and CUDA. Familiarity with 3D protein structures and protein sequences. Demonstrated ability to work at a high level of personal and professional integrity. Excellent written and verbal communication skills, including the ability to communicate with scientific and non-scientific personnel. Excellent attention to detail with strong critical thinking and decision-making abilities. Ability to multitask and thrive in a fast-paced environment with changing priorities. Excellent time management skills with the ability to prioritize and meet deadlines. Must be an independent, self-starter who is also an excellent team player with strong interpersonal skills. Must be adaptable and flexible to work collaboratively and effectively in a multi-disciplinary environment."
WeCloudData, Machine Learning Engineer, "Job Duties Design deep learning algorithms and apply them to practical Natural Language Processing and Time Series analysis use cases Lead and supervise junior data scientists and machine learning engineers on ML projects and provide technical guidance to the development team. Work with the internal data scientists and engineers to engineer and maintain multiple data and ML pipelines. Deploy end-to-end ML models from data collection, feature engineering, to model training, deployment, and monitoring. Work with internal data team on building AI-driven data products to support the growth of multiple platforms. Lead the work on AI consulting projects to assist clients solve tough data & AI challenges. Help consulting clients train, fine-tune, and deploy computer vision and NLP deep learning models in both development and production environments. Work in agile project management teams and act as scrum masters. Preferred Skills Proven practical experience with a leading machine learning platform such as Tensorflow or PyTorch - Pretrained models, and fine-tuning. Strong MLOps experience with the ability to facilitate deployment of ML models for scoring via cloud platforms such as AWS or on edge devices Familiarity with DevOps CI/CD concepts and tools such as Jenkins, Github Action, etc. Strong ability to deploy ML pipelines in various ways including Flask, SageMaker, Tensorflow Serving in a dockerized environment Experienced with model monitoring in production Hands-on knowledge of latest microntrollers and SBC's such as Raspberry Pi and Nvidia's Jetson series. Including deployment and optimization of the ML models, and interfacing with peripherals such as stereo cameras and various other sensors. Linux DevOps and server management. Great documentation skills Must have 1-2 years experience in a similar role. A core understanding of the latest developments in machine learning, and in particular Deep Learning. A strong passion in both ML project implementation and education The ability to package experiments, demos and lessons via a version control platform such as GitHub or GitLab."
Tilda Research, AI/ML Engineer, "Responsibilities Train and/or fine-tune large model as per product requirements Responsible for end to end delivery of AI applications from ideation to production deployment Collaborate with data scientists, product team, clinical team, and management to build our next-generation healthcare AI platform Support and maintain existing data science products, applications and interfaces Write reusable, testable, and efficient code and integrate multiple data sources and databases Minimum Qualifications: Bachelor's or Master's degree in Computer Science, Data Science, Statistics, or a related field 5+ years of experience working with state-of-the-art supervised and unsupervised machine learning algorithms on real-world problems Strong foundational knowledge in a variety of ML approaches and techniques, ranging from neural nets to Bayesian methods Experience in constructing semantic textual similarity pipelines, utilizing embedding techniques and natural language processing Expertise in applying LLMs, prompt design, and fine-tuning techniques Technology Skills / Strengths Python or golang Graph databases (Neo4j)Vector databases (Weaviate, pinecone)"
Tilda Research, Junior Data Scientist and ML Engineer, "What you'll be doing Proactively supports the design, development, and programing methods, processes, and systems to consolidate and analyze unstructured, diverse big data sources to generate actionable insights and solutions for client services and product enhancement. Receives instructions to research, design, implement and deploy scalable data analytics vision and machine learning solutions to challenge business issues. Contributes to the design and enhancement of data collection procedures to include information that is relevant for building analytic systems. Ensures that data used for analysis is processed, cleaned and, integrally verified and build algorithms necessary to find meaningful answers. Contributes to the designing and coding of software programs, algorithms, and automated processes to cleanse, integrates and evaluates large datasets from multiple disparate sources. Takes direction from management/leadership to provide meaningful insights from large data and metadata sources; interprets and communicates insights and findings from analysis and experiments to product, service, and business managers. Supports the design of scalable and highly available applications leveraging the latest tools and technologies. Participates in creatively visualizing and effectively communicating results of data analysis, insights, and ideas in a variety of formats to key decision-makers within the business. Receives instructions to create SQL queries for the analysis of data and visualize the output of the models. Contributes to ensuring that industry standards best practices are applied to development activities. Knowledge And Attributes Developing in data modelling, statistical methods and machine learning techniques. Ability to thrive in a dynamic, fast-paced environment. Quantitative and qualitative analysis skills. Desire to acquire more knowledge to keep up to speed with the ever-evolving field of data science. Curiosity to sift through data to find answers and more insights. Developing understanding of the information technology industry within a matrixed organization and the typical business problems such organizations face. Ability to translate technical findings clearly and fluently to non-technical team business stakeholders to enable informed decision-making. Developing ability to create a storyline around the data to make it easy to interpret and understand. Self-driven and able to work independently yet acts as a team player. Developing ability to apply data science principles through a business lens. Academic Qualifications And Certifications Bachelor's degree or equivalent in Data Science, Business Analytics, Mathematics, Economics, Engineering, Computer Science or a related field. Relevant programming certification preferred. Agile certification preferred. Required Experience Moderate level experience in a data science position in a corporate environment and/or related industry. Moderate level experience in statistical modelling and data modelling, machine learning, data mining, unstructured data analytics, natural language processing. Familiarity with programming languages (R, Python, etc.). Moderate level experience working with and creating data architectures. Moderate level experience with extracting, cleaning, and transforming data and working with data owners to understand the data. Familiarity with visualizing and/or presenting data for stakeholder use and reuse across the business."
Valence, Machine Learning Engineer, "What You'll Do You will develop scalable data pipelines, optimize models for performance and accuracy, and evaluate them to ensure they are production-ready. Develop, design and implement improvements in user experience in conversational interactions leveraging LLMs in new ways to advance product goals Work with the product team to analyze user behavior and prioritize evolving requirements Experiment at a high velocity, conducting statistical analyses, to optimize the end user experience Research and development on new Conversational AI approaches leveraging cutting edge LLM/NLP advancements Documentation of models, prompts, and processes to increase replicability and drive quality improvements. Stay current with the latest leading research advancements in ML, LLMs, and Conversational AI Support other coding and feature development where required What We're Looking For Bachelor's degree in Computer Science, Engineering, Mathematics, related field, or equivalent experience 3+ years of professional experience (or equivalent) in software engineering, AI/ML development (ideally including a Master's or Ph.D. in Computer Science, ML, Data Science, or a related field) Practical experience and theoretical knowledge of language technologies such as: dialogue/conversational systems, NLP, and Information Retrieval Strong foundation in data structures, algorithms, and software engineering principles Proficiency in Python and relevant deep learning frameworks; training (e.g. PyTorch, Tensorflow, JAX, Hugging Face Transformers/Adapters), serving (e.g., Hugging Face TGI//outlines, vLLM) Experience with LLM model development and deployment ideally including experience with model distillation, supervised fine-tuning using RLHF/DPO, and automatic prompt tuning (e.g. DSPy, TextGrad) Experience with cloud deployment of ML systems (e.g., AWS, GCP, Azure) including and open systems (e.g. Docker and Kubernetes) and associated ML services Strong analytical and problem-solving skills Experience structuring and running data-backed experiments Strong written and verbal communication skills"
Waabi, Machine Learning Engineer, "You will... Contribute and build the self-driving engineering foundations to develop the next generation autonomy software across many machine learning projects. Collaborate closely with autonomy and algorithm engineers to scale safe self-driving systems using an AI-first approach. Build standardized frameworks for training and production aligned with industry best practices. Comprehensively profile model runtime and memory to pinpoint performance bottlenecks. Identify and evaluate emerging technologies that can be adopted into Waabi's training and production frameworks, in areas of training and inference optimization (i.e quantization, model compilation and exporting, etc.), distributed training, PyTorch tooling, etc. Work with AWS Cloud infrastructure to ensure training job stability and efficiency (i.e compute environments, S3 buckets, etc.). Qualifications: MS/PhD or Bachelors degree with a minimum of 2 years of industry experience in Computer Science, Robotics and/or similar technical field(s) of study. Solid coding proficiency in a variety of coding languages including Python, C++ or Rust. Ability to rapidly prototype with test driven development methodologies. Experience in deep learning frameworks such as PyTorch. Experience in distributed machine learning training, model deployment and monitoring. Experience building software solutions on cloud infrastructure. Open-minded and collaborative team player with willingness to help others. Passionate about self-driving technologies, solving hard problems, and creating innovative solutions. Bonus/nice to have: Experience in optimizing model training and evaluation, familiar with model profiling tools. Experience in model compilation and exporting, interaction with lower level concepts like TensorRT, cuda kernels."
ProNavigator, Machine Learning Engineer, "Your accountabilities in this role will include: Delivering AI/ML models from initial planning to data sourcing and preparation, building and training models, and deploying models to production in our cloud environment, ideally using and deploying open source models Creating AI-based prototypes to evaluate model and design options, working with internal stakeholders to analyze quality and relevance of results Training and tuning models to optimize outcomes of algorithms Integrating models to production, working with Engineering team members as needed to integrate models into product architecture Working with internal stakeholders to understand product needs of AI and to scope potential solutions Conducting research into relevant AI models and technologies and staying well-connected to trends and developments in this rapidly evolving space 5+ years of software development experience with at least 3 years of experience implementing ML and AI technologies Deep knowledge of data modeling, and current ML and AI technologies with experience working with LLMs in particular to develop language-based solutions Highly analytical with the ability to think outside of the box, consider potential solutions from many different sources and make decisions with rapidly evolving or ambiguous data points Creative and scrappy problem solver who is excited about the prospect of creating greenfield solutions using new and evolving technologies Strong written and verbal communication skills with the ability to present complex ideas Highly collaborative with the ability to work cross-functionally to understand needs and execute against ideas"
Techedin, Machine Learning Engineer, "Responsibilities Collaborate on projects at the intersection of research and product with a diverse, global team of researchers and engineers Develop new or improve existing ML models used in CAD software Process data and analyze feature extractions Design solutions based on error analysis Present results to collaborators and leadership Review relevant AI/ML literature to identify emerging methods, technologies, and best practices Minimum Qualifications BSc or MSc in Computer Science, or equivalent industry experience 3+ years of software development experience Proficiency with modern deep learning techniques (e.g. Network architectures, regularization techniques, learning techniques, loss-functions, optimization strategies, etc.) as well as frameworks (e.g. PyTorch, Lightning, Ray etc.) Experience with deploying machine learning models in production settings Experience with version control, reproducibility, and writing reusable, testable code Experience with data modeling, architecture, and processing using varied data representations including 2D and 3D geometry Experience with cloud services and architectures (e.g. AWS, Azure) Excellent written documentation skills to document code, architectures, and experiments"
Amplifier Health, AI Engineer, "Responsibilities Collaborate with AI Researchers to implement and train LAMs using large-scale datasets Develop and maintain data pipelines for efficient data ingestion, processing, and augmentation Optimize model training processes to improve performance and scalability while reducing computational costs Implement and evaluate different model architectures and hyperparameter settings Deploy trained models into production environments and monitor their performance Contribute to the development of our AI infrastructure and tools Work closely with our software engineering team to integrate AI models into our platform Requirements Master's degree in Computer Science, Engineering, or a related field Strong software engineering skills with proficiency in Python or other relevant languages Experience with deep learning frameworks such as TensorFlow or PyTorch Familiarity with cloud computing platforms (e.g., AWS, Azure, GCP) and MLOps practices Knowledge of data structures, algorithms, and software design patterns Excellent problem-solving and debugging skills Ability to work independently and collaboratively in a fast-paced environment. Experience with audio and speech processing technologies Familiarity with containerization technologies (e.g., Docker, Kubernetes)"
BluWave-ai, AI Engineer, "Responsibilities: Time Series Forecasting: Develop and implement advanced time series forecasting models to predict future trends, demand, and other relevant variables. Apply techniques such as ARIMA, SARIMA, exponential smoothing, or machine learning algorithms tailored to time series data Data Preprocessing and Cleaning: Clean, transform, and preprocess time series data to ensure data integrity and quality. Handle missing data, outliers, and other data anomalies appropriately Feature Engineering: Identify and engineer relevant features to improve the accuracy and performance of forecasting models. Incorporate domain knowledge to enhance feature selection and extraction Model Development and Evaluation: Build, train, and evaluate forecasting models using appropriate evaluation metrics. Select and fine-tune models to achieve optimal performance Performance Monitoring: Continuously monitor and validate the accuracy and performance of forecasting models over time. Identify and address issues related to model drift or degradation Collaboration and Communication: Collaborate with cross-functional teams, including stakeholders from different departments, to understand business requirements and provide actionable insights based on time series analysis and optimization Visualization and Reporting: Create clear and compelling visualizations, reports, and dashboards to effectively communicate forecasting results, optimization recommendations, and key insights to both technical and non-technical stakeholders Research and Innovation: Stay updated with the latest advancements in time series forecasting, optimization techniques, and related domains. Explore and propose innovative approaches to improve forecasting accuracy and optimization outcomes Requirements Education: Bachelor's or Master's degree in Computer Science, Statistics, Mathematics, or a related field (Ph.D. is a plus) 3 years of experience in data science or a related field, with a focus on time series forecasting and optimization Time Series Forecasting: Strong knowledge and hands-on experience in developing time series forecasting models using statistical and machine learning approaches Programming Skills: Proficiency in Python for data manipulation, analysis, and model development Analytical Skills: Ability to apply mathematical concepts and statistical techniques to analyze complex time-dependent data and derive actionable insights Data Visualization: Proficiency in data visualization and dashboarding tools likeGrafana, Plotly, or similar Communication: Excellent verbal and written communication skills for presenting complex concepts and findings Teamwork: Proven ability to collaborate effectively in cross-functional teams Continuous Learning: Strong desire to stay updated with advancements in data science, time series forecasting, and optimization Familiarity with control and optimization of modern power and energy systems Experience with optimization techniques such as linear programming, integer programming, and related tools (e.g. Pyomo, Gurobi, CPLEX) Deployment and Monitoring: Understanding of machine learning model deployment strategies and monitoring techniques MLOps Tools: Familiarity with Kubeflow, MLflow, or similar for managing and deploying machine learning models Cloud Platforms: Experience with Azure, AWS, or Google Cloud Platform and their machine learning services Big Data Technologies: Knowledge of Apache Hadoop, Spark, or Hive and experience with large-scale time series datasets Time Series Databases: Familiarity with InfluxDB, Prometheus, or TimescaleDB for efficient time series data storage Version Control Systems: Experience with Git or similar tools for collaborative development"
Altus Group, Machine Learning Engineer, "Key Responsibilities MLOps Strategy: Develop and implement MLOps strategies, best practices, and standards to enhance the efficiency of end-to-end model lifecycle activities. ML System Design and Development: Design and develop batch and streaming-based AI/ML pipelines including data ingestion, feature engineering, model training and scaled AI model compute. Data Integration: Integrate data from various sources, including structured and unstructured data, to create comprehensive datasets for analysis. Pipeline Development: Design, build, and maintain scalable data pipelines using Databricks, ensuring data is accessible and reliable with quality and version control for analytics and machine learning models; Create continuous integration (CI), continuous delivery (CD), and continuous training (CT) pipelines for ML systems to improve teams' efficiency and ensure consistency and reproducibility of data and model pipelines. Model Development & Management: Develop and implement AI/ML models in collaboration with Data Scientists for better experiment tracking, and to set up the model registry to manage the model lifecycle including staging, serving, production and storing model artifacts. Model Deployment: Deploy models into production environments, ensuring they are scalable, reliable, and reusable. Model Monitoring: Monitor model performance, detect anomalies, and retrain models as necessary. Software Engineering: Write robust and efficient code to integrate ML models into applications and systems. Performance Optimization: Optimize machine learning models and data pipelines for compute, performance, efficiency and scalability, ensuring efficient data processing, model training and serving, with minimal downtime. Technical Leadership: Leading ML engineering and data pipeline projects through planning, execution, and delivery phases in partnership with stakeholders and team members. Stakeholder Communication: Communicating project status, risks, and outcomes with technical and business stakeholders, ensuring alignment on project goals and timelines. Collaboration: Working closely with Data Scientists to understand their modeling and data requirements and provide the necessary infrastructure and support. Documentation: Documenting ML models, data pipelines, processes, and best practices to ensure knowledge sharing and reproducibility across the team. Key Qualifications Collaborative Communicator: You can work independently but enjoy collaborating with a broader team to share ideas, questions, and insights regarding machine learning and data engineering. Focused on Efficiency and Reliability: You focus on building data pipelines and ML systems/infrastructures that are efficient, reliable, and scalable, ensuring smooth data operations and robust model performance. Detail-Oriented Engineer: You are meticulous about model performance optimization, data quality, and setting standards for deployment, monitoring and governance. Skilled in writing production ready code: You have strong programming skills at least in Python, SQL, and Spark, and have prior hands-on experience to refactor the messy codes from Data Scientists to make it become production ready, which is unit tested, reusable, easily maintained and debugged. Having prior experience with Machine Learning Python libraries such as MLflow, TensorFlow, PyTorch, and Scikit-Learn is the plus Experienced in Versioning and Experiment Tracking: You know the best practice of using Github for branching strategy, code versioning and code review. You have hands-on experience using tools such as MLflow for experiment tracking and model registry, including tracking the model development process and saving code snapshots, model parameters, metrics, and other metadata. Having prior experience with Databricks Experiments is a plus. Experienced in AI/ML Model Deployment and Platform Engineering: You are comfortable to work with DevOps team and Data Scientists to understand Altus's infrastructure and software/libraries/model inference requirements for testing and deploying AI/ML models in production so as to ensure seamless integration with existing systems/products, and to guide Data Scientists through the best practices. Having prior experience with Databricks Unity Catalog & Catalogs, Databricks Asset Bundles (DABs), containerization (e.g., Docker) is a plus. Experienced in Statistics Analysis/AI/Machine Learning: To facilitate the discussions with Data Scientists, you are familiar with (or can learn quickly) a range of statistical analysis and machine learning techniques, including hypothesis testing, linear/logistic regression, time series analysis, tree-based models, KNN, neural networks, clustering, and outlier/anomaly detection. Prior experience with NLP, transfer learning, large language models (LLMs), and Generative AI is a plus. 4+ years of experience working in an ML engineering or data engineering position. BS or MS degree in Computer Science, Engineering, Applied Mathematics, or a related technical field."
Yeah! Group, AI Engineer, "Key Responsibilities: Design and implement advanced machine learning algorithms to solve complex problems. Analyze large datasets to extract valuable insights and improve model performance. Train machine learning models and evaluate their performance using various metrics and methodologies. Integrate AI solutions into existing systems to enhance functionality and performance. Stay updated with the latest trends in AI and machine learning, applying this knowledge to drive innovation within the team. Work closely with cross-functional teams, including data scientists, software engineers, and product managers, to deliver high-quality AI solutions. Deploy AI models into production environments, ensuring their ongoing performance and reliability. Create comprehensive documentation for AI models, algorithms, and systems to facilitate knowledge sharing and maintenance. Qualifications: Bachelors or Masters degree in Computer Science, Engineering, or a related field. Proficiency in programming languages such as Python, R, or Java, and strong understanding of machine learning frameworks like TensorFlow, PyTorch, and scikit-learn. Demonstrated experience in developing and deploying AI models in a professional setting. Strong analytical and problem-solving skills with experience in working with large datasets. Excellent verbal and written communication skills to explain complex AI concepts to non-technical stakeholders. Ability to work collaboratively in a team environment and contribute to project success. Preferred Qualifications: Ph.D. in AI, Machine Learning, Data Science, or a related field. Experience in applying AI techniques in industries such as healthcare, finance, or e-commerce. Familiarity with big data platforms like Hadoop, Spark, and AWS. Published research in reputable AI or machine learning conferences or journals."
Akkodis, Machine Learning Engineer, "What You Will Do? Design scalable backend services using FastAPI for ML model deployment and inference Implement state-of-the-art ML algorithms across various platforms and frameworks Coordinate with development teams to determine precise application integration points Ensure security practices are maintained to safeguard sensitive data and compliance Manage the complete software development lifecycle from planning to support Your Skills Possess a Bachelor's degree in relevant fields or equivalent work experience in ML engineering Demonstrated experience in Python, AI/ML solutions, and related technologies Proficiency in FastAPI, extensive usage of SQL/NoSQL databases, and Linux OS Practical knowledge of cloud-based AI/ML services like AWS, GCP, or Azure Effective communication skills necessary to convey complex technical concepts clearly"
Qohash, Data Engineer, "What you will do As a Data Engineer your responsibilities will be as follows: Analyze, design, and implement features. Write well-designed and efficient code, which can be tested & perform units and/or integrated tests. Correct anomalies and problems as they arise. Test the implemented features to make sure all acceptance criteria are met Participate in projects from the initial idea to launch. Advise the product owner on technological choices and decisions concerning developments. Contribute to the continuous improvement of development activities (agility, automated tests, deployment, etc.). Collaborate with the security team to integrate security best practices into software development processes. Communicate risks associated with any activity, technology, or processes as you identify them What your resume shows Must Haves 4+ years of experience in data warehousing and ETL Development 3+ years of experience in either Apache Spark or Databricks 2+ years of experience building, deploying and optimizing data-ingestion solutions Experience in working directly with external and internal stakeholders (good communication skills) A bachelor's or master's degree in computer science or engineering (or equivalent professional experience) Nice to Haves Experience in Streaming (Flink, Spark structured streaming, Kafka streams) Strong preference for candidates with Kafka experience"
Captura, Senior Machine Learning / Computer Vision Research Scientist (Ph.D.), "Job Responsibilities Lead research and development projects in machine learning and computer vision. Design, implement, and evaluate advanced algorithms and models. Collaborate with cross-functional teams to integrate solutions into products. Mentor junior researchers and provide technical guidance. Stay up-to-date with the latest advancements in the field to drive innovation. Contribute to the research and development of new and existing AI-powered products. Quickly prototype a variety of machine learning algorithms to assess their potential and applicability to our projects. Design, run, and analyze experiments, evaluating performance metrics and reporting results to stakeholders. Required Qualifications Ph.D. in Computer Science, Engineering, or related field Strong background in Machine Learning and Computer Vision Experience in developing algorithms and models from scratch Knowledge of deep learning frameworks like TensorFlow or PyTorch Excellent problem-solving and analytical skills Comfortable working independently, as well as part of a fast-paced and collaborative team environment. Preferred Qualifications Proficient in Python and PyTorch. Excellent problem-solving and analytical skills. Strong written and verbal communication abilities. Strong understanding of computer vision techniques and algorithms. Strong understanding of machine learning techniques and algorithms. Experience with more classical ML techniques such as decision trees, naive Bayes, least squares regression, logistic regression, SVM, random forests, etc. Interested in following the state-of-art in deep learning."
Flight Centre, Data Engineer IV, "Key Responsibilities Develop and maintain data pipelines to load and transform data for business analytics and system integrations. Optimize data processing to ensure efficiency and cost optimization Identify and resolve database performance issues. Actively participate in Agile team activities, including standups, planning sessions, and retrospectives. Implement and oversee security best practices to ensure secure database coding. Provide troubleshooting support and identify root causes for technical issues. Apply best practices in architecture to ensure high-quality deliverables. Support the technical documentation team in preparing necessary materials. Educate and mentor staff members through training sessions and individual support. Experience & Qualifications 3+ years of experience with the Azure platform and data engineering. Proven expertise in building and optimizing big data pipelines and transformations. Strong understanding and hands-on experience with Lakehouse architecture. Proficiency in Azure Synapse and Azure Data Factory. Advanced skills in Python, particularly with PySpark Experience with Power BI and/or Fabric SQL Server database development experience Experience with Azure Data Lake and Blob Storage. Experience working in an Agile development environment. Knowledge of PCI, GDPR, ISO, or other security compliance standards. Experience working with travel data and platforms. Familiarity with DevOps practices and CI/CD pipelines"
Dreamfold, Senior Machine Learning Engineer, "As a DreamFold Research Engineer, you will Work closely with our founding team and advisors to build and deploy generative learning models on AWS and GCP, built in Ray and Anyscale. Run ML training and inference workloads parallelized across thousands of GPUs. Build machine learning tooling and library code that enables an efficient, flexible, and scalable platform. Set up validations and benchmarks to ensure a high quality of ML models. Demonstrate a high level of autonomy and critical thinking in order to take full ownership of your work. All to ultimately design new drugs that will save lives. Required skills and experience A degree in computer science, applied math, computational biology, or another quantitative field. 4+ years of experience building and running deep learning models. Demonstrated capability to summarize scientific content, to navigate complex and unknown problems, and to collaborate closely with cross-functional teams. Experience working with industrial scale models on distributed infrastructure. Experience developing and maintaining software libraries, following industry standard expectations. You might also have Knowledge on generative models, reinforcement learning, and natural language processing algorithms. Peer reviewed publication(s) in the field of machine learning and/or experience contributing to open-source libraries."
Intact, Senior AI Developer, "Your Mission Within Our Innovation Lab Participate in the design, development and evolution of microservices, APIs and reusable libraries for the execution of our algorithms in mission critical production environments. Participate in the technological watch and definition of development standards to ensure system efficiency, resilience, and sustainability. Participate in the development and evolution of back-end components for the execution of our algorithms. Optimize the platform for maximum velocity and scalability. Optimize and prioritize development tasks in collaboration with the team. Help our automated testers in setting up component and integration tests frameworks. Ensure the high level of quality of the work delivered. Strengthen the development team productivity. Oversee the development of appropriate tactics to achieve objectives. Support and be proactively involved in problem situations or issues. Play Mentor role for Squad members. You will be supported by quality analysts, business analysts, application architects, and data architects. We use Scrum as our organizational methodology, with the support of our product owners and our scrum masters. What You Bring To The Table Bachelor's degree in computer engineering, computer science, or any combination of equivalent education and experience. Minimum of 5 years of significant experience in development. A very good understanding of machine learning and artificial intelligence. Ability to make complex concepts easy to understand and well documented. Proficiency in the Python programming language and the PySpark Framework. Experience in Cloud technologies, Big Data pipelines and workflows. Proficient in AWS technologies, Lambda functions EC2, Kinesis, SQS, SNS, S3, and CloudWatch. A very good understanding of machine learning concepts on artificial intelligence, MLOps best practices. Experienced with software engineering life cycle and agile methodologies. Experienced with modern DevOps tools such as Gitlab CI/CD, Grafana. Working knowledge of streaming and batch processing systems such as Kafka. Proficiency in Micro services, Java, Spring Boot, Mongo, Oracle, Docker, Kubernetes, GitHub/GitLab, Elastic Search/Kibana/Grafana monitoring"
Intact, Senior AI Software Developer, "Your mission within our Innovation Lab : Participate in the design, development and evolution of microservices, APIs and reusable libraries for the execution of our algorithms in mission critical production environments. Participate in the technological watch and definition of development standards to ensure system efficiency, resilience, and sustainability. Participate in the development and evolution of back-end components for the execution of our algorithms. Optimize the platform for maximum velocity and scalability. Optimize and prioritize development tasks in collaboration with the team. Help our testers in setting up component and integration tests. Ensure the high level of quality of the work delivered. Strengthen the development team productivity. Oversee the development of appropriate tactics to achieve objectives. Support and be proactively involved in problem situations or issues. Play a Technical Leader / Mentor role for Squad members. What You Bring To The Table Bachelor's in computer engineering, computer science or software engineering, with a minimum of 5 years of significant experience in development of enterprise grade software solutions Proficient in Python language & GIT Proficient in AWS technologies such as ECR, ECS, SQS, Lambda and CloudWatch. Proficient in Terraform A very good understanding of machine learning and artificial intelligence. A good knowledge of Generative AI and Retrieval-Augmented Generation (RAG). A good knowledge of MLOps best practices. Experienced with software engineering life cycle and agile methodologies. Experienced in developing complex services based on Docker, Fast API Experienced with modern DevOps tools such as Gitlab CI/CD, Grafana, ELK stack Working knowledge of streaming and batch processing systems such as Kafka."
Inworld AI, Staff/Principal Machine Learning Engineer Speech, "Minimum Qualifications Bachelor's degree in Computer Science, Engineering, or a similar technical field. 6+ years of experience with software development in one or more programming languages, machine learning algorithms and tools (e.g., PyTorch), artificial intelligence, deep learning and/or natural language processing. Excellent problem solving skills and the ability to work independently and as part of a team. Preferred Qualifications Master's degree or PhD in speech synthesis/recognition or adjacent fields. 5+ years of experience with design and architecture; and testing/launching software products. 1+ years of experience in working with sourcing and curating speech datasets. 1+ years of experience in a technical leadership role leading project teams and setting technical direction. 1+ years of experience in building end-to-end speech processing systems and real-time applications. Responsibilities Research and experiment with cutting edge ML techniques for TTS and STT applications. Develop and test production-grade training and inference pipelines for TTS and STT applications. Understand optimization problems in the area of speech, signals, and natural language processing. Collaborate with cross-functional teams"
Mozilla, Staff Machine Learning Engineer, "What You'll Do Apply statistical and machine learning techniques to process and analyze unstructured textual data Develop and finetune machine learning models for tasks such as entity recognition, classification, and text generation Utilize pretrained language models (e.g. GPT, LLAMA) and adapt them for specific use cases Optimize the models for production usage, including considerations for scalability, latency, and resource Monitor and refine deployed models for performance and efficiency, and conduct troubleshooting when necessary Work closely with interdisciplinary teams to deliver high-quality features and solutions Stay current with advancements in NLP research, methodologies, and best practices Be consistently productive and operate with a high degree of autonomy What You'll Bring A bachelor's degree in Statistics, Computer Science, related technical field, or equivalent practical experience A minimum of 6 years of experience in a quantitative role, with ideally much of that as a machine learning engineer or a data scientist Knowledge of and expertise in Natural Language Processing (NLP) Proficiency in a data query language (e.g. SQL), and a programming language (e.g. Python) Demonstrable experience with the full lifecycle of machine learning models - from development to deployment and monitoring Being an excellent team player with a proven ability to work effectively in cross-functional teams, showing a high degree of collaboration, flexibility, and respect for diverse perspectives An ability to be self-directed after work is assigned and help less experienced team members to get unblocked Commitment to our values: Welcoming differences Being relationship-minded Practicing responsible participation Having grit Bonus Points For: An advanced degree (master or PhD) in a quantitative field A deep understanding of deep learning, reinforcement learning, and natural language processing Experience with cloud platforms such as AWS, Google Cloud, or Azure, and familiarity with related machine learning services Experience with big data technologies like Hadoop, Spark, or similar Demonstrated prior experience with large language models, and generative AI"
BuildOps, Senior AI Engineer, "What you will do: Design, develop, and deploy machine learning models using Python and Javascript Collect, clean, and analyze data to identify patterns and insights Explore and implement new AI techniques and algorithms Design and test experiments using Jupiter Notebooks LLM tuning based on manual and automated feedback Write quality scalable code with high test coverage and documentation. Collaborate with cross-functional teams to deliver impactful AI solutions Propose recommendations, build prototypes, and provide feedback. Work in tandem with the QA engineering team to ship quality and high-precision software. Build and maintain automated unit tests: unit, integration and UI Co-own the short and long-term technical vision of your squad with your teammates. Provide technical mentorship and guidance to engineers within your team and others. Help improve the technical quality of our work through regular technical design, code reviews and automated testing. Share your technical knowledge and expertise, promote using AI and ML methodology across the whole organization. Communicate effectively with engineers, product managers, customers, partners, and other leaders. Stay up-to-date on the latest AI technologies and trends What We Look For: 2+ years of experience in developing AI solutions preferably with LLMs 5+ years of experience in Full Stack software development 5+ years of experience in Python 3+ years of experience in Javascript/Typescript, Node.js Proven ability to design, develop, and deploy machine learning models Strong understanding of machine learning principles and algorithms Experience with AI/ML frameworks such as TensorFlow, PyTorch, numpy or scikit-learn Experience with multi-variable non-linear optimization problems preferred You will be expected to shape and guide the AI/ML technology decisions of your squad, while contributing alongside them. As an individual contributor, you are a role model to more junior engineers. Strong communication and technical writing skills. Familiarity with unit testing, debugging, profiling and performance monitoring in AWS environment. Prior experience with Node.js, building features using REST and/or GraphQL APIs using Apollo preferred Prior knowledge or ability to quickly learn developing in a MLOps environment (MLflow or similar) preferred. B.S., M.S. or PhD in computer science, engineering or related fields."
Intuit, Senior AI/ML Software Engineer, "What You'll Do Apply statistical and machine learning techniques to process and analyze unstructured textual data Develop and finetune machine learning models for tasks such as entity recognition, classification, and text generation Utilize pretrained language models (e.g. GPT, LLAMA) and adapt them for specific use cases Optimize the models for production usage, including considerations for scalability, latency, and resource Monitor and refine deployed models for performance and efficiency, and conduct troubleshooting when necessary Work closely with interdisciplinary teams to deliver high-quality features and solutions Stay current with advancements in NLP research, methodologies, and best practices Be consistently productive and operate with a high degree of autonomy What You'll Bring A bachelor's degree in Statistics, Computer Science, related technical field, or equivalent practical experience A minimum of 6 years of experience in a quantitative role, with ideally much of that as a machine learning engineer or a data scientist Knowledge of and expertise in Natural Language Processing (NLP) Proficiency in a data query language (e.g. SQL), and a programming language (e.g. Python) Demonstrable experience with the full lifecycle of machine learning models - from development to deployment and monitoring Being an excellent team player with a proven ability to work effectively in cross-functional teams, showing a high degree of collaboration, flexibility, and respect for diverse perspectives An ability to be self-directed after work is assigned and help less experienced team members to get unblocked An advanced degree (master or PhD) in a quantitative field A deep understanding of deep learning, reinforcement learning, and natural language processing Experience with cloud platforms such as AWS, Google Cloud, or Azure, and familiarity with related machine learning services Experience with big data technologies like Hadoop, Spark, or similar Demonstrated prior experience with large language models, and generative AI"
