{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO's - October\n",
    "\n",
    "1. Refine preprocessing pipeline (use spacy or nltk or some combination of the two)\n",
    "    - There are some quirks with n-grams currently, look into refining the implementation\n",
    "    - Some words like \"use\", \"since\", \"r\", \"x\", are not being filtered out by stopword removal\n",
    "\n",
    "2. Web scraping for job data\n",
    "    - Collect like 50-100 examples per week and create a similar preprocessing pipeline \n",
    "    - Look for ways to programmatically filter sections we want (responsibilities and qualifications).\n",
    "\n",
    "3. Look into topic labeling\n",
    "    - Automatically extracting top n words (and sorting them by relevance)\n",
    "    - Look at how relevance is computed at https://github.com/bmabey/pyLDAvis/blob/master/pyLDAvis/_prepare.py\n",
    "    - BERTopic?\n",
    "    \n",
    "4. Finish Introduction and Data sections before midterm break\n",
    "    - Literature review (Blei paper, Daniel paper, Journal of DSE paper, possibly find topic labeling papers?)\n",
    "    - Decide on final dataset \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO's - Final Submission\n",
    "\n",
    "1. Use HDP output or LDA equivalent for module data. \n",
    "2. Preprocess and filter job data (maybe add more from LinkedIn if less than 150 after filtering).\n",
    "3. Same analysis on job data.\n",
    "4. Results section: Add visualizations and metrics.\n",
    "5. Discussion section: talk about overlaps and differences between both datasets.\n",
    "6. Finalize paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling on MDS Program Lecture Material"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A 'document' is just a collection of words.\n",
    "    - Initially, after loading the data, one document is contained in a string, containing all the text from one module.\n",
    "    - After preprocessing, one document is represented in a \"bag of words\" format, which means it is a *list* of individual tokens (words).\n",
    "- A 'corpus' is a collection of documents.\n",
    "- d = number of documents in the corpus\n",
    "- k = number of topics for the topic model to find\n",
    "- |V| = size of vocabulary, i.e. number of distinct tokens in the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\syeda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\syeda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\syeda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Used to tokenize the text; i.e. create a dictionary mapping words to integers. The dictionary can be used to create a term-document matrix.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "## Preprocessing with nltk\n",
    "import string   # contains a public variable with all ASCII punctuation characters\n",
    "import nltk\n",
    "\n",
    "# list of all stopwords such as 'and', 'the', 'is', etc.\n",
    "nltk.download('stopwords')  \n",
    "\n",
    "# WordNet is a lexical database of English words that groups words into sets of synonyms, while also recording semantic relationships between words such as \"is-a\", \"part-of\", and \"opposite-of\" relationships.\n",
    "nltk.download('wordnet')    \n",
    "\n",
    "# Open Multilingual WordNet (omw) links hand created wordnets and automatically created wordnets for different languages.\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk import ngrams\n",
    "\n",
    "## Preprocessing with gensim and spacy\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models.phrases import ENGLISH_CONNECTOR_WORDS\n",
    "\n",
    "import spacy\n",
    "\n",
    "from textacy import extract\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For topic visualizations \n",
    "import pyLDAvis.gensim_models as gensim_vis\n",
    "import pyLDAvis\n",
    "# For enabling HTML widget in Jupyter notebook\n",
    "from pyLDAvis import enable_notebook\n",
    "\n",
    "enable_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Location of environment for personal reference:  c:\\Users\\syeda\\miniconda3\\envs\\dir-st\\lib\\ (in case large models are downloaded for testing and need to be deleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus combined successfully as a list of strings.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def combine_text_files_to_list(input_directory):\n",
    "\n",
    "    txt_files = [os.path.join(input_directory, file) for file in os.listdir(input_directory) if file.endswith(\".txt\")]\n",
    "    corpus = []\n",
    "\n",
    "    for txt_file in txt_files:\n",
    "        \n",
    "        try:\n",
    "            # Read the entire file as a string and add the string to the corpus\n",
    "            with open(txt_file, 'r', encoding='utf-8') as file:\n",
    "                file_content = file.read()  \n",
    "                corpus.append(file_content)  \n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while reading {txt_file}: {e}\")\n",
    "    \n",
    "    return corpus\n",
    "\n",
    "strings_list = combine_text_files_to_list(\"../Dataset/Parsed_Lectures\")\n",
    "print(\"Corpus combined successfully as a list of strings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each string in `strings_list` is all the text from one PDF of lecture slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n",
      " Learning Objectives‚Ä¢  Explain why it is important to understand and use correct terminology.            ‚Ä¢          Define: computer, software, memory, data, memory size/data size, cloud            ‚Ä¢          Explain \"Big Data\" and describe data growth in the coming years.            ‚Ä¢          Compare and contrast: digital versus analog            ‚Ä¢          Briefly explain how integers, doubles, and strings are encoded.            ‚Ä¢          Explain why ASCII table is required for character en\n"
     ]
    }
   ],
   "source": [
    "print(len(strings_list))\n",
    "print(strings_list[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words:  2429\n",
      "Number of words:  4714\n",
      "Number of words:  2076\n",
      "Number of words:  4303\n",
      "Number of words:  3140\n",
      "Number of words:  1810\n",
      "Number of words:  1829\n",
      "Number of words:  3466\n",
      "Number of words:  2130\n",
      "Number of words:  3368\n",
      "Number of words:  2624\n",
      "Number of words:  3376\n",
      "Number of words:  2477\n",
      "Number of words:  3001\n",
      "Number of words:  2815\n",
      "Number of words:  1843\n",
      "Number of words:  3389\n",
      "Number of words:  2099\n",
      "Number of words:  2520\n",
      "Number of words:  1622\n",
      "Number of words:  1099\n",
      "Number of words:  2014\n",
      "Number of words:  2447\n",
      "Number of words:  2023\n",
      "Number of words:  2621\n",
      "Number of words:  2121\n",
      "Number of words:  3201\n",
      "Number of words:  1874\n",
      "Number of words:  2069\n",
      "Number of words:  5461\n",
      "Number of words:  1241\n",
      "Number of words:  2446\n",
      "Number of words:  1683\n",
      "Number of words:  5307\n",
      "Number of words:  3546\n",
      "Number of words:  4360\n",
      "Number of words:  1584\n",
      "Number of words:  2097\n",
      "Number of words:  3871\n",
      "Number of words:  3414\n",
      "Number of words:  3068\n",
      "Number of words:  3669\n",
      "Number of words:  1922\n",
      "Number of words:  2795\n",
      "Number of words:  2155\n",
      "Number of words:  3167\n",
      "Number of words:  3043\n",
      "Number of words:  2917\n",
      "Number of words:  2217\n",
      "Number of words:  2967\n",
      "Number of words:  1449\n",
      "Number of words:  1091\n",
      "Number of words:  1642\n",
      "Number of words:  3838\n",
      "Number of words:  2068\n",
      "Number of words:  3054\n",
      "Number of words:  4537\n",
      "Number of words:  4264\n",
      "Number of words:  3968\n",
      "Number of words:  3969\n",
      "Number of words:  2712\n",
      "Number of words:  3335\n",
      "Number of words:  4015\n",
      "Number of words:  3712\n",
      "Number of words:  3213\n",
      "Number of words:  3702\n",
      "Number of words:  2824\n",
      "Number of words:  2644\n",
      "Number of words:  1683\n",
      "Number of words:  2295\n",
      "Number of words:  2425\n",
      "Number of words:  1122\n",
      "Number of words:  105\n",
      "Number of words:  4500\n",
      "Number of words:  1522\n",
      "Number of words:  1013\n",
      "Number of words:  10153\n",
      "Number of words:  1841\n",
      "Number of words:  1876\n",
      "Number of words:  1607\n",
      "Number of words:  2715\n",
      "Number of words:  2338\n",
      "Number of words:  2003\n",
      "Number of words:  1650\n",
      "Number of words:  4424\n",
      "Number of words:  1486\n",
      "Number of words:  2064\n",
      "Number of words:  1741\n",
      "Number of words:  590\n",
      "Number of words:  2599\n",
      "Number of words:  2047\n",
      "Number of words:  2229\n",
      "Number of words:  3382\n",
      "Number of words:  1859\n",
      "Number of words:  2117\n",
      "Number of words:  2246\n",
      "Number of words:  2812\n",
      "Number of words:  2824\n",
      "Number of words:  3238\n",
      "Number of words:  1502\n",
      "Number of words:  2322\n",
      "Number of words:  1212\n",
      "Number of words:  1478\n",
      "Number of words:  1712\n",
      "Number of words:  1201\n",
      "Number of words:  3587\n",
      "Number of words:  2377\n",
      "Number of words:  1396\n",
      "Number of words:  2332\n",
      "Number of words:  2825\n",
      "Number of words:  3720\n",
      "Number of words:  4206\n",
      "Number of words:  4528\n",
      "Number of words:  3517\n",
      "Number of words:  4199\n",
      "Number of words:  3585\n",
      "Number of words:  2145\n",
      "Number of words:  3603\n",
      "Number of words:  2671\n",
      "Number of words:  1463\n",
      "Number of words:  2549\n",
      "Number of words:  2857\n",
      "Number of words:  1894\n",
      "Number of words:  5096\n",
      "Number of words:  3076\n",
      "Number of words:  3973\n",
      "Number of words:  5407\n",
      "Number of words:  3855\n",
      "Number of words:  4462\n",
      "Number of words:  5220\n",
      "Number of words:  1843\n",
      "Number of words:  2561\n",
      "Number of words:  1561\n",
      "Number of words:  1825\n",
      "Number of words:  1547\n",
      "Number of words:  1062\n",
      "Number of words:  1041\n",
      "Number of words:  52\n",
      "Number of words:  14824\n",
      "Number of words:  2211\n",
      "Number of words:  2828\n",
      "Number of words:  8064\n",
      "Number of words:  3622\n",
      "Number of words:  2918\n",
      "Number of words:  2937\n",
      "Number of words:  3893\n",
      "Number of words:  3360\n",
      "Number of words:  2869\n",
      "Number of words:  3110\n",
      "Number of words:  1556\n",
      "Number of words:  3069\n",
      "Number of words:  3352\n",
      "Number of words:  2759\n",
      "Number of words:  2261\n",
      "Number of words:  2943\n",
      "Number of words:  2787\n",
      "Number of words:  2532\n",
      "Number of words:  1747\n",
      "Number of words:  2897\n",
      "Number of words:  2707\n",
      "Number of words:  2231\n",
      "Number of words:  2212\n",
      "Total number of words in the corpus: 457124\n",
      "Mean number of words per document: 2821.75\n",
      "Standard deviation: 1580.71\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "doc_length = []\n",
    "for doc in strings_list:\n",
    "    words = doc.split()\n",
    "    sum += len(words)\n",
    "    print(\"Number of words: \", len(words))\n",
    "    doc_length.append(len(words))\n",
    "    \n",
    "print(f\"Total number of words in the corpus: {sum}\")\n",
    "print(f\"Mean number of words per document: {round(np.mean(doc_length),2)}\")\n",
    "print(f\"Standard deviation: {round(np.std(doc_length),2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and preprocessing the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, we explored 2 options, nltk and spaCy. Overall, we found spaCy is a bit easier to use. In both cases, input is a list of strings, and the returned corpus is a list of list of strings, where each nested list of strings is a list of cleaned words from one module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_with_nltk(doc):\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    punctuation = set(string.punctuation)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lower_case_sentences = doc.lower().split()\n",
    "\n",
    "    stop_free = \" \".join([word for word in lower_case_sentences if word not in stop_words])             # only keep words that are not stopwords\n",
    "    # print(stop_free)\n",
    "    punc_free = \"\".join(ch for ch in stop_free if ch not in punctuation and not ch.isnumeric() and not ch == \"‚Ä¢\")         # only keep characters that are not punctuation and not numbers\n",
    "    # print(punc_free)\n",
    "    lemmatized = \" \".join(lemmatizer.lemmatize(word) for word in punc_free.split())             # lemmatize words; convert words to their base or root form using their context in the sentence\n",
    "    # print(lemmatized)\n",
    "\n",
    "    # We do this separately later for nltk\n",
    "    # bigrams = list(ngrams(lemmatized, 2))  \n",
    "    # trigrams = list(ngrams(lemmatized, 3))  \n",
    "    # bigram_strings = [\"_\".join(bigram) for bigram in bigrams]  # Join bigram words with an underscore\n",
    "    # trigram_strings = [\"_\".join(trigram) for trigram in trigrams]\n",
    "\n",
    "    return lemmatized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Old function, kept here because of how stupid it is\n",
    "def clean_with_spacy(doc):\n",
    "    \n",
    "    spacy_doc = nlp(doc.lower())  \n",
    "    ngrams = [\n",
    "        ngram.text.replace(\" \", \"_\")    # ngrams are separated by spaces, so we replace them with underscores\n",
    "        for ngram in extract.ngrams(spacy_doc, n = 2, min_freq = 4, filter_punct = True, filter_nums = True, exclude_pos=[\"PROPN\", \"ORG\", \"DATE\", \"X\"]) \n",
    "        if not ngram.text.__contains__(\"=\") \n",
    "            and not ngram.text.__contains__(\"@\") \n",
    "            and not ngram.text.__contains__(\"$\")\n",
    "    ]\n",
    "    \n",
    "    # Remove stopwords, punctuation, and numeric tokens\n",
    "    tokens = [\n",
    "        token.lemma_ \n",
    "        for token in spacy_doc \n",
    "        if not token.is_stop and not token.is_punct and not token.is_digit and token.is_alpha       # Keep only words that are not stop words\n",
    "            and token.text not in [\"_\", \"+\", \"=\", \"\\n\",\"-\",\"*\",\"<\",\">\"]                             # Remove special characters       \n",
    "            and not len(token.text) == 1                                                            # Remove single character words\n",
    "    ]    \n",
    "\n",
    "    tokens = [token.replace(\"datum\", \"data\") for token in tokens]  # Replace 'datum' (lemma of data) with 'data' for clarity                                                                         \n",
    "    \n",
    "    return tokens + ngrams\n",
    "\"\"\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])  \n",
    "\n",
    "# Add custom stop words \n",
    "nlp.Defaults.stop_words |= {\"ubc\", \"mds\", \"lecture\", \"lab\", \"assignments\", \"example\", \"british\",\"columbia\", \"introduction\" ,\"page\", \"file\", \"question\", \"ex\", \"import\", \"jeffrey\", \"andrews\", \"irene\", \"vrbik\", \"shan\", \"du\", \"ifeoma\", \"adaji\", \"gema\", \"rodrigues\", \"fatemeh\", \"fard\", \"emelie\", \"gustafsson\", \"heinz\", \"bauschke\", \"travis\", \"douglas\", \"jones\", \"dave\", \"xiaoping\", \"shi\", \"khalad\", \"hasan\", \"ladan\", \"tazik\", \"ramon\", \"lawrence\", \"chu\", \"miller\", \"casey\", \"ritish\", \"smith\", \"lee\", \"university\", \"Œπc\", \"jan\", \"feb\", \"mar\", \"tn\", \"pu\", \"xn\", \"ee\", \"sa\", \"fa\", \"toys\", \"bat\", \"clothing\", \"apples\", \"jacknife\", \"jacket\", \"following\", \"treatment\", \"let\", \"return\", \"returns\", \"true\", \"nh\", \"Œªy\", \"ùëòth\", \"ll\", \"lll\", \"calibri\", \"york\", \"florida\", \"illinois\", \"texas\", \"francisco\", \"quartersales\", \"quarterpivot\", \"food\", \"wind\", \"steak\", \"xlsx\", \"phd\", \"na\", \"kkt\", \"dur\", \"earlier\", \"city\", \"street\", \"false\"}\n",
    "\n",
    "def clean_without_ngrams(doc):\n",
    "\n",
    "    spacy_doc = nlp(doc.lower())\n",
    "\n",
    "    # Remove stopwords, punctuation, and numeric tokens\n",
    "    tokens = [\n",
    "        token.text \n",
    "        for token in spacy_doc \n",
    "        if not token.is_stop and not token.is_punct and not token.is_digit and token.is_alpha       # Keep only words that are not stop words\n",
    "            and token.text not in [\"_\", \"+\", \"=\", \"\\n\",\"-\",\"*\",\"<\",\">\"]                             # Remove special characters       \n",
    "            and not len(token.text) == 1                                                            # Remove single character words\n",
    "            # and token.pos_ in [\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]                                        # Keep only nouns, adjectives, verbs, and adverbs\n",
    "    ]    \n",
    "                                                                           \n",
    "    return tokens\n",
    "\n",
    "def lemmatize(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    tokens = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        sent_tokens = []\n",
    "        for token in doc: \n",
    "            if \"_\" in token.text:\n",
    "                sent_tokens.append(token.text)\n",
    "            else:\n",
    "                if token.pos_ in allowed_postags:\n",
    "                    sent_tokens.append(token.lemma_)\n",
    "                    \n",
    "        sent_tokens = [token.replace(\"datum\", \"data\") for token in sent_tokens]\n",
    "        tokens.append(sent_tokens)\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning with spaCy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the cleaned corpus: 151447\n"
     ]
    }
   ],
   "source": [
    "bag_of_words_list = [clean_without_ngrams(doc) for doc in strings_list]\n",
    "\n",
    "bigram = Phrases(bag_of_words_list, min_count=10, threshold=20) \n",
    "bigram_mod = Phraser(bigram)    # For speed\n",
    "\n",
    "# Add bigrams\n",
    "bag_of_words_list = [bigram_mod[doc] for doc in bag_of_words_list]\n",
    "\n",
    "# Lemmatize the words, exluding bigrams\n",
    "bag_of_words_list = lemmatize(bag_of_words_list)\n",
    "\n",
    "sum = 0\n",
    "for doc in bag_of_words_list:\n",
    "    sum += len(doc)\n",
    "\n",
    "print(f\"Total number of words in the cleaned corpus: {sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['learn', 'explain', 'important', 'understand', 'use', 'correct', 'terminology', 'define', 'computer', 'software', 'necessary', 'transform', 'data', 'format', 'excel', 'analysis', 'ubco_master', 'data', 'science', 'data']\n"
     ]
    }
   ],
   "source": [
    "print(bag_of_words_list[0][:10] + bag_of_words_list[0][-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning with nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_cleaned_corpus = [clean_with_nltk(doc).split() for doc in corpus]\n",
    "print(nltk_cleaned_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the cleaned corpus: 181461\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for doc in nltk_cleaned_corpus:\n",
    "    sum += len(doc)\n",
    "\n",
    "print(f\"Total number of words in the cleaned corpus: {sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Total number of words in the corpus with ngrams: 164907\n"
     ]
    }
   ],
   "source": [
    "bigram = Phrases(nltk_cleaned_corpus, min_count=10, connector_words=ENGLISH_CONNECTOR_WORDS)  \n",
    "# trigram = Phrases(bigram[clean_corpus], threshold=10, connector_words=ENGLISH_CONNECTOR_WORDS)\n",
    "\n",
    "bigram_mod = Phraser(bigram)\n",
    "# trigram_mod = Phraser(trigram)\n",
    "\n",
    "# add bigrams and trigrams to the clean corpus\n",
    "corpus_with_bigrams = [bigram_mod[doc] for doc in nltk_cleaned_corpus]\n",
    "\n",
    "sum = 0\n",
    "for doc in corpus_with_bigrams:\n",
    "    sum += len(doc)\n",
    "\n",
    "print(f\"Total number of words in the nltk corpus with ngrams: {sum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing into Document-Term matrix and id2word dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 3), (2, 1), (3, 1), (4, 4), (5, 6), (6, 1), (7, 1), (8, 2), (9, 1), (10, 2), (11, 13), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 4), (20, 2), (21, 1), (22, 5), (23, 6), (24, 22), (25, 1), (26, 4), (27, 1), (28, 12), (29, 2), (30, 3), (31, 1), (32, 1), (33, 1), (34, 2), (35, 1), (36, 15), (37, 2), (38, 3), (39, 4), (40, 2), (41, 2), (42, 1), (43, 1), (44, 1), (45, 1), (46, 1), (47, 1), (48, 1), (49, 2), (50, 1), (51, 1), (52, 1), (53, 19), (54, 1), (55, 1), (56, 1), (57, 2), (58, 1), (59, 4), (60, 2), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 2), (68, 1), (69, 65), (70, 1), (71, 5), (72, 6), (73, 1), (74, 1), (75, 2), (76, 3), (77, 1), (78, 1), (79, 1), (80, 10), (81, 4), (82, 3), (83, 3), (84, 1), (85, 2), (86, 2), (87, 1), (88, 1), (89, 1), (90, 2), (91, 1), (92, 4), (93, 1), (94, 2), (95, 1), (96, 1), (97, 20), (98, 11), (99, 2), (100, 1), (101, 1), (102, 2), (103, 2), (104, 2), (105, 1), (106, 1), (107, 1), (108, 1), (109, 6), (110, 1), (111, 2), (112, 1), (113, 1), (114, 2), (115, 2), (116, 1), (117, 11), (118, 2), (119, 1), (120, 2), (121, 1), (122, 1), (123, 4), (124, 1), (125, 9), (126, 1), (127, 1), (128, 1), (129, 1), (130, 1), (131, 1), (132, 4), (133, 1), (134, 1), (135, 3), (136, 1), (137, 2), (138, 1), (139, 1), (140, 1), (141, 1), (142, 1), (143, 1), (144, 2), (145, 1), (146, 1), (147, 1), (148, 2), (149, 1), (150, 1), (151, 1), (152, 1), (153, 8), (154, 1), (155, 1), (156, 6), (157, 1), (158, 1), (159, 2), (160, 5), (161, 1), (162, 2), (163, 1), (164, 1), (165, 3), (166, 1), (167, 2), (168, 2), (169, 2), (170, 1), (171, 2), (172, 1), (173, 2), (174, 2), (175, 1), (176, 1), (177, 1), (178, 1), (179, 1), (180, 1), (181, 1), (182, 1), (183, 2), (184, 1), (185, 1), (186, 2), (187, 1), (188, 1), (189, 2), (190, 2), (191, 4), (192, 14), (193, 1), (194, 2), (195, 1), (196, 2), (197, 1), (198, 3), (199, 1), (200, 2), (201, 2), (202, 1), (203, 2), (204, 1), (205, 1), (206, 1), (207, 2), (208, 3), (209, 1), (210, 13), (211, 1), (212, 2), (213, 1), (214, 1), (215, 1), (216, 1), (217, 1), (218, 1), (219, 1), (220, 1), (221, 1), (222, 1), (223, 1), (224, 1), (225, 1), (226, 2), (227, 2), (228, 1), (229, 1), (230, 1), (231, 2), (232, 1), (233, 1), (234, 2), (235, 1), (236, 1), (237, 1), (238, 1), (239, 2), (240, 9), (241, 1), (242, 1), (243, 3), (244, 6), (245, 1), (246, 1), (247, 2), (248, 1), (249, 1), (250, 1), (251, 1), (252, 1), (253, 1), (254, 1), (255, 1), (256, 1), (257, 2), (258, 2), (259, 1), (260, 1), (261, 2), (262, 1), (263, 26), (264, 4), (265, 1), (266, 4), (267, 2), (268, 1), (269, 1), (270, 1), (271, 1), (272, 1), (273, 2), (274, 1), (275, 2), (276, 1), (277, 3), (278, 1), (279, 3), (280, 1), (281, 8), (282, 1), (283, 5), (284, 3), (285, 1), (286, 1), (287, 2), (288, 1), (289, 1), (290, 1), (291, 14), (292, 1), (293, 1), (294, 2), (295, 1), (296, 9), (297, 1), (298, 1), (299, 1), (300, 1), (301, 1), (302, 1), (303, 2), (304, 4), (305, 16), (306, 10), (307, 1), (308, 1), (309, 1), (310, 1), (311, 1), (312, 3), (313, 2), (314, 2), (315, 1), (316, 5), (317, 1), (318, 2), (319, 1), (320, 1), (321, 1), (322, 1), (323, 2), (324, 1), (325, 3), (326, 1), (327, 2), (328, 1), (329, 12), (330, 3), (331, 1), (332, 9), (333, 3), (334, 1), (335, 2), (336, 3), (337, 1), (338, 1), (339, 1), (340, 1), (341, 6), (342, 4), (343, 2), (344, 1), (345, 1), (346, 1), (347, 1), (348, 12), (349, 2), (350, 1), (351, 3), (352, 8), (353, 1), (354, 1), (355, 1), (356, 3), (357, 2), (358, 2), (359, 3), (360, 4), (361, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary mapping token ID integers to words\n",
    "dictionary = Dictionary(bag_of_words_list)    \n",
    "\n",
    "# Create a d x |V| term-document matrix, where each row represents a document and each column represents a unique token in the corpus. \n",
    "# Value at row i and column j is the how many times token j appears in document i.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in bag_of_words_list]  \n",
    "\n",
    "print(doc_term_matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.save_as_text(\"lectures_dictionary.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9922\n"
     ]
    }
   ],
   "source": [
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First run of LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 23\n",
    "PATH_TO_MODEL = f\"162_Lectures_Test_LDA_spacy_{NUM_TOPICS}_topics\"\n",
    "lda_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.016*\"function\" + 0.015*\"data\" + 0.012*\"value\" + 0.010*\"use\" + 0.007*\"test\" + 0.006*\"number\" + 0.006*\"list\" + 0.006*\"create\" + 0.005*\"column\" + 0.005*\"set\"'),\n",
       " (1,\n",
       "  '0.026*\"data\" + 0.012*\"model\" + 0.011*\"value\" + 0.008*\"function\" + 0.007*\"probability\" + 0.006*\"variable\" + 0.005*\"time\" + 0.005*\"estimate\" + 0.005*\"sample\" + 0.005*\"number\"'),\n",
       " (2,\n",
       "  '0.011*\"data\" + 0.009*\"function\" + 0.009*\"value\" + 0.008*\"set\" + 0.008*\"model\" + 0.006*\"sample\" + 0.006*\"group\" + 0.005*\"method\" + 0.005*\"probability\" + 0.005*\"number\"'),\n",
       " (3,\n",
       "  '0.020*\"data\" + 0.011*\"value\" + 0.008*\"model\" + 0.006*\"function\" + 0.006*\"sample\" + 0.005*\"class\" + 0.005*\"use\" + 0.004*\"estimate\" + 0.003*\"number\" + 0.003*\"variable\"'),\n",
       " (4,\n",
       "  '0.010*\"data\" + 0.007*\"package\" + 0.007*\"function\" + 0.007*\"use\" + 0.006*\"test\" + 0.006*\"error\" + 0.005*\"value\" + 0.005*\"table\" + 0.005*\"number\" + 0.004*\"select\"'),\n",
       " (5,\n",
       "  '0.013*\"model\" + 0.010*\"data\" + 0.007*\"value\" + 0.006*\"sample\" + 0.005*\"set\" + 0.005*\"number\" + 0.005*\"variable\" + 0.004*\"estimate\" + 0.004*\"tree\" + 0.004*\"use\"'),\n",
       " (6,\n",
       "  '0.018*\"data\" + 0.009*\"value\" + 0.008*\"model\" + 0.008*\"function\" + 0.006*\"table\" + 0.006*\"query\" + 0.005*\"class\" + 0.005*\"database\" + 0.005*\"number\" + 0.005*\"use\"'),\n",
       " (7,\n",
       "  '0.016*\"data\" + 0.010*\"model\" + 0.010*\"sample\" + 0.008*\"value\" + 0.008*\"use\" + 0.006*\"set\" + 0.005*\"function\" + 0.005*\"distribution\" + 0.005*\"mean\" + 0.004*\"create\"'),\n",
       " (8,\n",
       "  '0.017*\"data\" + 0.008*\"value\" + 0.007*\"model\" + 0.006*\"set\" + 0.006*\"use\" + 0.005*\"create\" + 0.004*\"number\" + 0.004*\"function\" + 0.004*\"add\" + 0.004*\"row\"'),\n",
       " (9,\n",
       "  '0.013*\"data\" + 0.011*\"model\" + 0.007*\"value\" + 0.006*\"sample\" + 0.006*\"use\" + 0.005*\"function\" + 0.005*\"number\" + 0.004*\"estimate\" + 0.004*\"set\" + 0.004*\"problem\"'),\n",
       " (10,\n",
       "  '0.036*\"data\" + 0.007*\"group\" + 0.007*\"use\" + 0.006*\"value\" + 0.005*\"set\" + 0.005*\"cluster\" + 0.004*\"number\" + 0.004*\"create\" + 0.004*\"model\" + 0.004*\"function\"'),\n",
       " (11,\n",
       "  '0.025*\"data\" + 0.009*\"model\" + 0.009*\"function\" + 0.007*\"value\" + 0.005*\"time\" + 0.005*\"set\" + 0.005*\"use\" + 0.005*\"number\" + 0.005*\"output\" + 0.005*\"variable\"'),\n",
       " (12,\n",
       "  '0.030*\"data\" + 0.012*\"value\" + 0.008*\"use\" + 0.007*\"model\" + 0.007*\"sample\" + 0.006*\"set\" + 0.004*\"estimate\" + 0.004*\"database\" + 0.004*\"number\" + 0.004*\"table\"'),\n",
       " (13,\n",
       "  '0.014*\"data\" + 0.013*\"value\" + 0.011*\"model\" + 0.006*\"function\" + 0.006*\"prior\" + 0.005*\"use\" + 0.004*\"set\" + 0.004*\"type\" + 0.004*\"column\" + 0.004*\"class\"'),\n",
       " (14,\n",
       "  '0.013*\"data\" + 0.008*\"function\" + 0.008*\"class\" + 0.007*\"value\" + 0.006*\"use\" + 0.004*\"number\" + 0.004*\"create\" + 0.004*\"model\" + 0.004*\"test\" + 0.004*\"estimate\"'),\n",
       " (15,\n",
       "  '0.012*\"data\" + 0.008*\"model\" + 0.007*\"set\" + 0.006*\"function\" + 0.005*\"value\" + 0.005*\"use\" + 0.005*\"create\" + 0.004*\"number\" + 0.004*\"output\" + 0.003*\"prior\"'),\n",
       " (16,\n",
       "  '0.020*\"data\" + 0.013*\"model\" + 0.011*\"probability\" + 0.008*\"value\" + 0.008*\"function\" + 0.006*\"number\" + 0.006*\"distribution\" + 0.006*\"time\" + 0.005*\"use\" + 0.005*\"test\"'),\n",
       " (17,\n",
       "  '0.013*\"data\" + 0.011*\"value\" + 0.007*\"function\" + 0.007*\"model\" + 0.006*\"use\" + 0.005*\"number\" + 0.005*\"distribution\" + 0.004*\"list\" + 0.004*\"add\" + 0.004*\"posterior\"'),\n",
       " (18,\n",
       "  '0.010*\"data\" + 0.007*\"function\" + 0.006*\"value\" + 0.005*\"use\" + 0.005*\"time\" + 0.005*\"model\" + 0.005*\"problem\" + 0.004*\"create\" + 0.004*\"number\" + 0.004*\"variable\"'),\n",
       " (19,\n",
       "  '0.026*\"data\" + 0.013*\"model\" + 0.008*\"value\" + 0.006*\"number\" + 0.006*\"use\" + 0.005*\"database\" + 0.005*\"set\" + 0.005*\"function\" + 0.005*\"prior\" + 0.004*\"probability\"'),\n",
       " (20,\n",
       "  '0.011*\"data\" + 0.008*\"value\" + 0.007*\"model\" + 0.007*\"set\" + 0.006*\"problem\" + 0.005*\"use\" + 0.005*\"class\" + 0.005*\"function\" + 0.004*\"branch\" + 0.004*\"change\"'),\n",
       " (21,\n",
       "  '0.027*\"data\" + 0.012*\"value\" + 0.010*\"model\" + 0.009*\"function\" + 0.007*\"distribution\" + 0.006*\"number\" + 0.006*\"set\" + 0.005*\"use\" + 0.005*\"time\" + 0.004*\"sample\"'),\n",
       " (22,\n",
       "  '0.016*\"data\" + 0.011*\"value\" + 0.007*\"table\" + 0.006*\"model\" + 0.006*\"relationship\" + 0.006*\"attribute\" + 0.006*\"sample\" + 0.005*\"use\" + 0.005*\"select\" + 0.005*\"graph\"')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "# from pprint import pprint\n",
    "\n",
    "lda_model = LdaModel(doc_term_matrix, num_topics=NUM_TOPICS, id2word = dictionary, random_state=448)\n",
    "lda_model.show_topics(num_topics = -1, num_words = 10)\n",
    "# pprint(lda_model.print_topics(num_topics=NUM_TOPICS, num_words=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row corresponds to a topic, and each coefficient next to a word represents the probability of that word being sampled from that topic. The order of the rows is arbitrary. Note that each row actually contains |V| elements, where coefficients sum to 1, here we only show the top 10 words sorted by their coefficients. \n",
    "\n",
    "These topics, however, are not very good. There is no debatably no automatic quantitative metric that can be used for measuring how \"good\" the topics found by a topic model are. Here is an example of a metric called \"coherence\" using the \"u_mass\" based on co-occurence of word pairs, and \"c_v\" which has been found to have the highest correlation with human rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([(0.02506777, 'data'),\n",
       "   (0.008995723, 'model'),\n",
       "   (0.008952424, 'function'),\n",
       "   (0.0067993742, 'value'),\n",
       "   (0.0054631867, 'time'),\n",
       "   (0.0052471864, 'set'),\n",
       "   (0.005177578, 'use'),\n",
       "   (0.004867948, 'number'),\n",
       "   (0.0047514704, 'output'),\n",
       "   (0.004589467, 'variable')],\n",
       "  -0.3354164662938507),\n",
       " ([(0.02665689, 'data'),\n",
       "   (0.012298621, 'value'),\n",
       "   (0.010319182, 'model'),\n",
       "   (0.008858897, 'function'),\n",
       "   (0.0065668197, 'distribution'),\n",
       "   (0.0062051914, 'number'),\n",
       "   (0.0058252197, 'set'),\n",
       "   (0.0049118996, 'use'),\n",
       "   (0.0046668258, 'time'),\n",
       "   (0.004485156, 'sample')],\n",
       "  -0.388593523397973),\n",
       " ([(0.013296178, 'data'),\n",
       "   (0.010989369, 'model'),\n",
       "   (0.006540581, 'value'),\n",
       "   (0.006004543, 'sample'),\n",
       "   (0.0056757624, 'use'),\n",
       "   (0.0049256054, 'function'),\n",
       "   (0.0048304754, 'number'),\n",
       "   (0.004410559, 'estimate'),\n",
       "   (0.0043707774, 'set'),\n",
       "   (0.003883086, 'problem')],\n",
       "  -0.4292133121228956),\n",
       " ([(0.009532973, 'data'),\n",
       "   (0.0066517214, 'function'),\n",
       "   (0.0055661397, 'value'),\n",
       "   (0.005339825, 'use'),\n",
       "   (0.005168748, 'time'),\n",
       "   (0.0049756146, 'model'),\n",
       "   (0.004785499, 'problem'),\n",
       "   (0.004113878, 'create'),\n",
       "   (0.0039600725, 'number'),\n",
       "   (0.003621229, 'variable')],\n",
       "  -0.4385872952345914),\n",
       " ([(0.016451668, 'data'),\n",
       "   (0.010212803, 'model'),\n",
       "   (0.0097103715, 'sample'),\n",
       "   (0.008255791, 'value'),\n",
       "   (0.007756338, 'use'),\n",
       "   (0.006096903, 'set'),\n",
       "   (0.0053947624, 'function'),\n",
       "   (0.0053378725, 'distribution'),\n",
       "   (0.004916036, 'mean'),\n",
       "   (0.0044688527, 'create')],\n",
       "  -0.4434354054556073),\n",
       " ([(0.019560268, 'data'),\n",
       "   (0.012750958, 'model'),\n",
       "   (0.011297839, 'probability'),\n",
       "   (0.008392239, 'value'),\n",
       "   (0.0077155908, 'function'),\n",
       "   (0.0062671117, 'number'),\n",
       "   (0.0059213405, 'distribution'),\n",
       "   (0.0057663037, 'time'),\n",
       "   (0.0051262863, 'use'),\n",
       "   (0.0046724067, 'test')],\n",
       "  -0.4463267727752554),\n",
       " ([(0.02040847, 'data'),\n",
       "   (0.010716565, 'value'),\n",
       "   (0.007842615, 'model'),\n",
       "   (0.006064431, 'function'),\n",
       "   (0.0057678577, 'sample'),\n",
       "   (0.0052734753, 'class'),\n",
       "   (0.0051265624, 'use'),\n",
       "   (0.003565827, 'estimate'),\n",
       "   (0.0034483229, 'number'),\n",
       "   (0.0034467794, 'variable')],\n",
       "  -0.47102124013604785),\n",
       " ([(0.010419113, 'data'),\n",
       "   (0.0072477986, 'package'),\n",
       "   (0.0065480713, 'function'),\n",
       "   (0.006527727, 'use'),\n",
       "   (0.005699974, 'test'),\n",
       "   (0.0055702026, 'error'),\n",
       "   (0.0054287594, 'value'),\n",
       "   (0.0048113298, 'table'),\n",
       "   (0.004776517, 'number'),\n",
       "   (0.004143278, 'select')],\n",
       "  -0.48345600574928205),\n",
       " ([(0.025864612, 'data'),\n",
       "   (0.011843062, 'model'),\n",
       "   (0.0106738275, 'value'),\n",
       "   (0.007865251, 'function'),\n",
       "   (0.0065141125, 'probability'),\n",
       "   (0.006420521, 'variable'),\n",
       "   (0.0053492817, 'time'),\n",
       "   (0.0053260406, 'estimate'),\n",
       "   (0.0048825312, 'sample'),\n",
       "   (0.0047229226, 'number')],\n",
       "  -0.5036759632922488),\n",
       " ([(0.016839977, 'data'),\n",
       "   (0.0077412743, 'value'),\n",
       "   (0.0072014495, 'model'),\n",
       "   (0.0064957095, 'set'),\n",
       "   (0.006061399, 'use'),\n",
       "   (0.004880679, 'create'),\n",
       "   (0.0041864156, 'number'),\n",
       "   (0.004098187, 'function'),\n",
       "   (0.0039488273, 'add'),\n",
       "   (0.0035706467, 'row')],\n",
       "  -0.5096318144405894),\n",
       " ([(0.015761621, 'function'),\n",
       "   (0.015493795, 'data'),\n",
       "   (0.011764593, 'value'),\n",
       "   (0.01047186, 'use'),\n",
       "   (0.0072902194, 'test'),\n",
       "   (0.006443856, 'number'),\n",
       "   (0.0059911935, 'list'),\n",
       "   (0.0055276216, 'create'),\n",
       "   (0.0053116977, 'column'),\n",
       "   (0.005229815, 'set')],\n",
       "  -0.5115297611548484),\n",
       " ([(0.036219373, 'data'),\n",
       "   (0.0066882395, 'group'),\n",
       "   (0.006658754, 'use'),\n",
       "   (0.0061922884, 'value'),\n",
       "   (0.005338416, 'set'),\n",
       "   (0.005217395, 'cluster'),\n",
       "   (0.004330132, 'number'),\n",
       "   (0.004309252, 'create'),\n",
       "   (0.004288576, 'model'),\n",
       "   (0.004093941, 'function')],\n",
       "  -0.5166989985732063),\n",
       " ([(0.013294185, 'data'),\n",
       "   (0.008194442, 'function'),\n",
       "   (0.007616207, 'class'),\n",
       "   (0.0065425793, 'value'),\n",
       "   (0.0056338888, 'use'),\n",
       "   (0.0042706677, 'number'),\n",
       "   (0.0038555348, 'create'),\n",
       "   (0.003751674, 'model'),\n",
       "   (0.0037510537, 'test'),\n",
       "   (0.003727207, 'estimate')],\n",
       "  -0.5496802927647442),\n",
       " ([(0.011411502, 'data'),\n",
       "   (0.009374749, 'function'),\n",
       "   (0.009125434, 'value'),\n",
       "   (0.007781865, 'set'),\n",
       "   (0.0077427193, 'model'),\n",
       "   (0.0060588405, 'sample'),\n",
       "   (0.0056194635, 'group'),\n",
       "   (0.00500853, 'method'),\n",
       "   (0.004948337, 'probability'),\n",
       "   (0.0045828572, 'number')],\n",
       "  -0.5633428840112922),\n",
       " ([(0.012613863, 'model'),\n",
       "   (0.010171271, 'data'),\n",
       "   (0.0069977627, 'value'),\n",
       "   (0.005533627, 'sample'),\n",
       "   (0.0051382673, 'set'),\n",
       "   (0.0046965214, 'number'),\n",
       "   (0.004665847, 'variable'),\n",
       "   (0.004144867, 'estimate'),\n",
       "   (0.0039959257, 'tree'),\n",
       "   (0.0039414363, 'use')],\n",
       "  -0.6281130712458578),\n",
       " ([(0.013872323, 'data'),\n",
       "   (0.012681716, 'value'),\n",
       "   (0.010967826, 'model'),\n",
       "   (0.0056358287, 'function'),\n",
       "   (0.0055211326, 'prior'),\n",
       "   (0.004989675, 'use'),\n",
       "   (0.0041172374, 'set'),\n",
       "   (0.004066296, 'type'),\n",
       "   (0.0038799806, 'column'),\n",
       "   (0.003874387, 'class')],\n",
       "  -0.6603691083396973),\n",
       " ([(0.018351724, 'data'),\n",
       "   (0.009460563, 'value'),\n",
       "   (0.007710765, 'model'),\n",
       "   (0.0075964606, 'function'),\n",
       "   (0.0064048003, 'table'),\n",
       "   (0.0059424737, 'query'),\n",
       "   (0.00493809, 'class'),\n",
       "   (0.004837831, 'database'),\n",
       "   (0.004769965, 'number'),\n",
       "   (0.0047301482, 'use')],\n",
       "  -0.6798914868983268),\n",
       " ([(0.011511382, 'data'),\n",
       "   (0.008291137, 'model'),\n",
       "   (0.0071411193, 'set'),\n",
       "   (0.0063396976, 'function'),\n",
       "   (0.0051747547, 'value'),\n",
       "   (0.0050938535, 'use'),\n",
       "   (0.004675482, 'create'),\n",
       "   (0.0035773583, 'number'),\n",
       "   (0.0035128638, 'output'),\n",
       "   (0.0034625225, 'prior')],\n",
       "  -0.6889877580372847),\n",
       " ([(0.029936023, 'data'),\n",
       "   (0.0123913735, 'value'),\n",
       "   (0.0075409724, 'use'),\n",
       "   (0.007348158, 'model'),\n",
       "   (0.0067688175, 'sample'),\n",
       "   (0.0056070397, 'set'),\n",
       "   (0.004186322, 'estimate'),\n",
       "   (0.0041429736, 'database'),\n",
       "   (0.0041150325, 'number'),\n",
       "   (0.0041048243, 'table')],\n",
       "  -0.753310341356302),\n",
       " ([(0.011001491, 'data'),\n",
       "   (0.007960772, 'value'),\n",
       "   (0.007207956, 'model'),\n",
       "   (0.006747245, 'set'),\n",
       "   (0.005734003, 'problem'),\n",
       "   (0.0050517153, 'use'),\n",
       "   (0.0048004626, 'class'),\n",
       "   (0.00458224, 'function'),\n",
       "   (0.0044085914, 'branch'),\n",
       "   (0.0043576453, 'change')],\n",
       "  -0.8985720294001993),\n",
       " ([(0.025529137, 'data'),\n",
       "   (0.012895562, 'model'),\n",
       "   (0.008148973, 'value'),\n",
       "   (0.0062389206, 'number'),\n",
       "   (0.0058513544, 'use'),\n",
       "   (0.005461074, 'database'),\n",
       "   (0.004788723, 'set'),\n",
       "   (0.0046319603, 'function'),\n",
       "   (0.0045650257, 'prior'),\n",
       "   (0.003665863, 'probability')],\n",
       "  -0.9018164875354446),\n",
       " ([(0.015801787, 'data'),\n",
       "   (0.011081924, 'value'),\n",
       "   (0.0069116084, 'table'),\n",
       "   (0.006337477, 'model'),\n",
       "   (0.0059488183, 'relationship'),\n",
       "   (0.0058110803, 'attribute'),\n",
       "   (0.00570708, 'sample'),\n",
       "   (0.0054378267, 'use'),\n",
       "   (0.0053009233, 'select'),\n",
       "   (0.004781435, 'graph')],\n",
       "  -0.9175813239760269),\n",
       " ([(0.01319951, 'data'),\n",
       "   (0.010939635, 'value'),\n",
       "   (0.0068027265, 'function'),\n",
       "   (0.006701132, 'model'),\n",
       "   (0.005963871, 'use'),\n",
       "   (0.0049387994, 'number'),\n",
       "   (0.0045337686, 'distribution'),\n",
       "   (0.0043721977, 'list'),\n",
       "   (0.003932851, 'add'),\n",
       "   (0.003848471, 'posterior')],\n",
       "  -1.0067247432836066)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.top_topics(doc_term_matrix, dictionary=dictionary, coherence='u_mass', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, these values (the negative float values after each list of 10 words) are hard to interpret, so we won't be using these metrics moving forward and just going with a subjective perspective. Also, as we'll see later, better topics seem to have *worse* coherence values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "lda_model.save(datapath(PATH_TO_MODEL))\n",
    "\n",
    "# Datapath: c:\\Users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages\\gensim\\test\\test_data\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing with pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save computation, previous results can be loaded from the disk after running only the cell where PATH_TO_MODEL is defined\n",
    "lda_model_to_display = LdaModel.load(datapath(PATH_TO_MODEL)) if lda_model is None else lda_model \n",
    "\n",
    "# Options for 'mds' (dimensionality reduction): mds = 'pcoa' (Principle Coordinate Analysis), 'tsne', 'mmds'\n",
    "LDAvis_prepared = gensim_vis.prepare(lda_model_to_display, doc_term_matrix, dictionary, mds='mmds')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "\n",
    "# To save the visualization to an HTML file\n",
    "pyLDAvis.save_html(LDAvis_prepared, 'Lectures_Test_run_LDA_'+ str(NUM_TOPICS) + '.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared = gensim_vis.prepare(lda_model_to_display, doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, 'Lectures_Test_run_LDA_'+ str(NUM_TOPICS) + '_pcoa.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to interpret the visualizations: \n",
    "\n",
    "- The size of the circle corresponds to the importance of the topic in the corpus (i.e. topics with words that occur more frequently in the corpus are larger)\n",
    "- The distance between circles is how \"far apart\" they are.\n",
    "- Overlaps between circles represents some similarity in the per-word probabilities for those topics.\n",
    "- Note that the radii of the circles and distances between circles are not on the same scales, they are presented in this way for simplicity of visualization.\n",
    "\n",
    "Rough overview of how the visualization is generated:\n",
    "\n",
    "- First, the k x |V| per-word topic (phi) matrix from the LdaModel object is used to calculate pairwise distances between topics, using Jensen-Shannon Divergence (a method of measuring the similarity between two probability distributions, remembering that each row of this matrix is a probability distribution over all words in the vocabulary). \n",
    "- Then, this distance matrix is projected into a 2D plane using the various options for dimensionality reduction techniques accepted as parameters. \n",
    "\n",
    "For more info, see: https://pyldavis.readthedocs.io/en/latest/_modules/pyLDAvis/_prepare.html#js_PCoA\n",
    "\n",
    "Here we show two dimensionality reduction techniques on the same LDA model: \n",
    "- MMDS (Metric Multidimensional Scaling) and PCoA (Principle Coordinate Analysis; different from Principle Component Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commonly-occuring words like 'data', 'model', 'value', 'function' clog up the outputs for most of the topics. Not just that, most topics seem to be too general to be given narrower labels like \"machine learning\" or \"databases\". Lastly, it seems that topics after topic 13 don't have that many words within them, so maybe around 10-12 topics should also be explored. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing an HDP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDP stands for Heirarchical Dirichlet Process. For an HDP topic model, we don't need to provide `k`, the number of topics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.014*sample + 0.010*data + 0.007*probability + 0.006*population + 0.004*unit + 0.004*error + 0.004*use + 0.004*model + 0.003*distribution + 0.003*number + 0.003*value + 0.003*test + 0.003*estimate + 0.003*information + 0.003*function + 0.002*stratum + 0.002*mean + 0.002*student + 0.002*design + 0.002*system'),\n",
       " (1,\n",
       "  '0.007*model + 0.007*value + 0.006*data + 0.005*estimate + 0.005*block + 0.004*mean + 0.004*distribution + 0.003*use + 0.003*variable + 0.003*plot + 0.003*time + 0.003*response + 0.003*parameter + 0.003*sample + 0.003*prediction + 0.003*selection + 0.003*problem + 0.002*proposal + 0.002*error + 0.002*design'),\n",
       " (2,\n",
       "  '0.015*data + 0.007*rdd + 0.005*model + 0.005*spark + 0.004*time + 0.004*function + 0.004*number + 0.004*value + 0.004*set + 0.004*variable + 0.003*point + 0.003*notation + 0.003*partition + 0.003*use + 0.003*predictive_modelling + 0.003*program + 0.003*wage_genes + 0.003*process + 0.002*distribute + 0.002*output'),\n",
       " (3,\n",
       "  '0.009*function + 0.007*value + 0.006*data + 0.005*index + 0.005*column + 0.004*output + 0.004*group + 0.004*array + 0.004*select + 0.004*table + 0.004*dataframe + 0.003*list + 0.003*query + 0.003*sort + 0.003*hash + 0.003*use + 0.003*case + 0.003*string + 0.002*style + 0.002*key'),\n",
       " (4,\n",
       "  '0.013*data + 0.007*function + 0.005*dataframe + 0.005*value + 0.005*module + 0.004*open + 0.004*use + 0.004*package + 0.003*sequence + 0.003*model + 0.003*moving_linearity + 0.003*software + 0.003*word + 0.003*object + 0.003*column + 0.003*output + 0.002*create + 0.002*set + 0.002*code + 0.002*vector'),\n",
       " (5,\n",
       "  '0.008*data + 0.007*prior + 0.006*value + 0.005*function + 0.005*string + 0.005*list + 0.004*distribution + 0.004*use + 0.004*index + 0.004*beta_binomial + 0.004*dataframe + 0.003*likelihood + 0.003*class + 0.003*object + 0.003*conjugate_prior + 0.003*beta_prior + 0.003*posterior + 0.003*year + 0.003*type + 0.003*informative_prior'),\n",
       " (6,\n",
       "  '0.008*layer + 0.006*network + 0.006*data + 0.005*model + 0.004*input + 0.004*block + 0.004*element + 0.004*value + 0.004*output + 0.003*weight + 0.003*time_step + 0.003*document + 0.003*number + 0.002*key + 0.002*parameter + 0.002*hidden_state + 0.002*training + 0.002*gradient + 0.002*emp_emp + 0.002*time'),\n",
       " (7,\n",
       "  '0.012*class + 0.007*data + 0.007*cell + 0.005*object + 0.004*attribute + 0.003*value + 0.003*def + 0.003*format + 0.003*method + 0.003*formula + 0.003*age + 0.002*select + 0.002*function + 0.002*directory + 0.002*column + 0.002*use + 0.002*data_rf + 0.002*number + 0.002*create + 0.002*inheritance'),\n",
       " (8,\n",
       "  '0.012*data + 0.005*privacy + 0.004*value + 0.004*record + 0.004*time + 0.004*structure + 0.004*information + 0.003*compartment + 0.003*queue + 0.003*stack + 0.003*probability + 0.003*algorithm + 0.003*state + 0.003*table + 0.003*individual + 0.003*list + 0.002*item + 0.002*use + 0.002*search + 0.002*model'),\n",
       " (9,\n",
       "  '0.009*data + 0.006*hyperplane + 0.005*classifier + 0.005*tree + 0.005*observation + 0.004*value + 0.004*use + 0.004*margin + 0.004*distance + 0.003*maximal_margin + 0.003*number + 0.003*vector + 0.003*support_vector + 0.003*object + 0.003*function + 0.003*class + 0.003*case + 0.003*node + 0.002*frame + 0.002*leave'),\n",
       " (10,\n",
       "  '0.006*data + 0.006*web + 0.003*use + 0.003*new + 0.003*create + 0.003*convex_analysis + 0.002*request + 0.002*tool + 0.002*need + 0.002*element + 0.002*object + 0.002*state + 0.002*server + 0.002*line + 0.002*type + 0.002*application + 0.002*table + 0.002*run + 0.002*country + 0.002*analysis'),\n",
       " (11,\n",
       "  '0.008*relationship + 0.007*model + 0.007*function + 0.005*value + 0.005*data + 0.005*attribute + 0.005*sample + 0.004*entity + 0.004*estimate + 0.003*measurement + 0.003*type + 0.003*project + 0.003*relation + 0.003*mean + 0.002*number + 0.002*give + 0.002*normal + 0.002*simulate + 0.002*distribution + 0.002*probability'),\n",
       " (12,\n",
       "  '0.014*data + 0.005*database + 0.005*macro + 0.004*relation + 0.004*table + 0.004*value + 0.003*cell + 0.003*column + 0.003*select + 0.003*revenue + 0.003*chart + 0.003*create + 0.003*analysis + 0.003*excel + 0.003*filter + 0.003*key + 0.002*object + 0.002*set + 0.002*add + 0.002*row'),\n",
       " (13,\n",
       "  '0.019*data + 0.009*regression + 0.008*value + 0.007*simple_linear + 0.006*model + 0.004*line + 0.004*function + 0.004*predictor + 0.004*multiple_linear + 0.004*plot + 0.004*residual + 0.003*fit + 0.003*test + 0.003*mean + 0.003*moving_linearity + 0.003*package + 0.003*estimate + 0.003*point + 0.003*distance + 0.003*sample'),\n",
       " (14,\n",
       "  '0.006*factor + 0.005*interaction + 0.005*effect + 0.004*command + 0.003*data + 0.003*window + 0.003*service + 0.003*level + 0.003*directory + 0.003*experiment + 0.003*command_line + 0.002*factorial + 0.002*response + 0.002*design + 0.002*use + 0.002*average + 0.002*system + 0.002*search + 0.002*user + 0.002*contrast')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import HdpModel\n",
    "# from pprint import pprint\n",
    "\n",
    "hdp_model = HdpModel(doc_term_matrix, id2word = dictionary)\n",
    "# hdp_model.optimal_ordering()\n",
    "hdp_model.show_topics(num_topics=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.014*sample + 0.010*data + 0.007*probability + 0.006*population + 0.004*unit + 0.004*error + 0.004*use + 0.004*model + 0.003*distribution + 0.003*number + 0.003*value + 0.003*test + 0.003*estimate + 0.003*information + 0.003*function + 0.002*stratum + 0.002*mean + 0.002*student + 0.002*design + 0.002*system'),\n",
       " (1,\n",
       "  '0.007*model + 0.007*value + 0.006*data + 0.005*estimate + 0.005*block + 0.004*mean + 0.004*distribution + 0.003*use + 0.003*variable + 0.003*plot + 0.003*time + 0.003*response + 0.003*parameter + 0.003*sample + 0.003*prediction + 0.003*selection + 0.003*problem + 0.002*proposal + 0.002*error + 0.002*design'),\n",
       " (2,\n",
       "  '0.015*data + 0.007*rdd + 0.005*model + 0.005*spark + 0.004*time + 0.004*function + 0.004*number + 0.004*value + 0.004*set + 0.004*variable + 0.003*point + 0.003*notation + 0.003*partition + 0.003*use + 0.003*predictive_modelling + 0.003*program + 0.003*wage_genes + 0.003*process + 0.002*distribute + 0.002*output'),\n",
       " (3,\n",
       "  '0.009*function + 0.007*value + 0.006*data + 0.005*index + 0.005*column + 0.004*output + 0.004*group + 0.004*array + 0.004*select + 0.004*table + 0.004*dataframe + 0.003*list + 0.003*query + 0.003*sort + 0.003*hash + 0.003*use + 0.003*case + 0.003*string + 0.002*style + 0.002*key'),\n",
       " (4,\n",
       "  '0.013*data + 0.007*function + 0.005*dataframe + 0.005*value + 0.005*module + 0.004*open + 0.004*use + 0.004*package + 0.003*sequence + 0.003*model + 0.003*moving_linearity + 0.003*software + 0.003*word + 0.003*object + 0.003*column + 0.003*output + 0.002*create + 0.002*set + 0.002*code + 0.002*vector'),\n",
       " (5,\n",
       "  '0.008*data + 0.007*prior + 0.006*value + 0.005*function + 0.005*string + 0.005*list + 0.004*distribution + 0.004*use + 0.004*index + 0.004*beta_binomial + 0.004*dataframe + 0.003*likelihood + 0.003*class + 0.003*object + 0.003*conjugate_prior + 0.003*beta_prior + 0.003*posterior + 0.003*year + 0.003*type + 0.003*informative_prior'),\n",
       " (6,\n",
       "  '0.008*layer + 0.006*network + 0.006*data + 0.005*model + 0.004*input + 0.004*block + 0.004*element + 0.004*value + 0.004*output + 0.003*weight + 0.003*time_step + 0.003*document + 0.003*number + 0.002*key + 0.002*parameter + 0.002*hidden_state + 0.002*training + 0.002*gradient + 0.002*emp_emp + 0.002*time'),\n",
       " (7,\n",
       "  '0.012*class + 0.007*data + 0.007*cell + 0.005*object + 0.004*attribute + 0.003*value + 0.003*def + 0.003*format + 0.003*method + 0.003*formula + 0.003*age + 0.002*select + 0.002*function + 0.002*directory + 0.002*column + 0.002*use + 0.002*data_rf + 0.002*number + 0.002*create + 0.002*inheritance'),\n",
       " (8,\n",
       "  '0.012*data + 0.005*privacy + 0.004*value + 0.004*record + 0.004*time + 0.004*structure + 0.004*information + 0.003*compartment + 0.003*queue + 0.003*stack + 0.003*probability + 0.003*algorithm + 0.003*state + 0.003*table + 0.003*individual + 0.003*list + 0.002*item + 0.002*use + 0.002*search + 0.002*model'),\n",
       " (9,\n",
       "  '0.009*data + 0.006*hyperplane + 0.005*classifier + 0.005*tree + 0.005*observation + 0.004*value + 0.004*use + 0.004*margin + 0.004*distance + 0.003*maximal_margin + 0.003*number + 0.003*vector + 0.003*support_vector + 0.003*object + 0.003*function + 0.003*class + 0.003*case + 0.003*node + 0.002*frame + 0.002*leave'),\n",
       " (10,\n",
       "  '0.006*data + 0.006*web + 0.003*use + 0.003*new + 0.003*create + 0.003*convex_analysis + 0.002*request + 0.002*tool + 0.002*need + 0.002*element + 0.002*object + 0.002*state + 0.002*server + 0.002*line + 0.002*type + 0.002*application + 0.002*table + 0.002*run + 0.002*country + 0.002*analysis'),\n",
       " (11,\n",
       "  '0.008*relationship + 0.007*model + 0.007*function + 0.005*value + 0.005*data + 0.005*attribute + 0.005*sample + 0.004*entity + 0.004*estimate + 0.003*measurement + 0.003*type + 0.003*project + 0.003*relation + 0.003*mean + 0.002*number + 0.002*give + 0.002*normal + 0.002*simulate + 0.002*distribution + 0.002*probability'),\n",
       " (12,\n",
       "  '0.014*data + 0.005*database + 0.005*macro + 0.004*relation + 0.004*table + 0.004*value + 0.003*cell + 0.003*column + 0.003*select + 0.003*revenue + 0.003*chart + 0.003*create + 0.003*analysis + 0.003*excel + 0.003*filter + 0.003*key + 0.002*object + 0.002*set + 0.002*add + 0.002*row'),\n",
       " (13,\n",
       "  '0.019*data + 0.009*regression + 0.008*value + 0.007*simple_linear + 0.006*model + 0.004*line + 0.004*function + 0.004*predictor + 0.004*multiple_linear + 0.004*plot + 0.004*residual + 0.003*fit + 0.003*test + 0.003*mean + 0.003*moving_linearity + 0.003*package + 0.003*estimate + 0.003*point + 0.003*distance + 0.003*sample'),\n",
       " (14,\n",
       "  '0.006*factor + 0.005*interaction + 0.005*effect + 0.004*command + 0.003*data + 0.003*window + 0.003*service + 0.003*level + 0.003*directory + 0.003*experiment + 0.003*command_line + 0.002*factorial + 0.002*response + 0.002*design + 0.002*use + 0.002*average + 0.002*system + 0.002*search + 0.002*user + 0.002*contrast')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdp_model.optimal_ordering()\n",
    "hdp_model.show_topics(num_topics=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics are better! Broadly speaking, we can see topics for probability and modeling, supervised learning techniques, databases, python programming, excel, etc. Towards the end of the list we do see some junk topics, and as you'll see, there were actually many more junk topics found after topic 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150,) (150, 9922)\n"
     ]
    }
   ],
   "source": [
    "alpha, beta = hdp_model.hdp_to_lda()\n",
    "print(alpha.shape, beta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we see that HDP actually finds 150 topics, most of which are junk topics. Obtaining a somewhat equivalent LDA model using the same alpha and beta from the HDP model, we see the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(138,\n",
       "  '0.000*\"misclassi\" + 0.000*\"guenin\" + 0.000*\"consist\" + 0.000*\"clo\" + 0.000*\"concise\" + 0.000*\"email\" + 0.000*\"involve\" + 0.000*\"correspondingly\" + 0.000*\"chlostrol\" + 0.000*\"omitting\" + 0.000*\"mean_squared\" + 0.000*\"negligible\" + 0.000*\"loosely\" + 0.000*\"nonrespondent\" + 0.000*\"windstate\"'),\n",
       " (74,\n",
       "  '0.000*\"init\" + 0.000*\"bid\" + 0.000*\"submodule\" + 0.000*\"intend\" + 0.000*\"actcost\" + 0.000*\"float\" + 0.000*\"strangitie\" + 0.000*\"seii\" + 0.000*\"obstacle\" + 0.000*\"motivate\" + 0.000*\"scrapy\" + 0.000*\"hmc\" + 0.000*\"preliminary\" + 0.000*\"dyno\" + 0.000*\"packagesg\"'),\n",
       " (82,\n",
       "  '0.000*\"hurry\" + 0.000*\"operating\" + 0.000*\"result\" + 0.000*\"instantiate\" + 0.000*\"sized\" + 0.000*\"judgement\" + 0.000*\"isdecimal\" + 0.000*\"housing\" + 0.000*\"destine\" + 0.000*\"equijoin\" + 0.000*\"ÀÜiŒ±\" + 0.000*\"destination\" + 0.000*\"eavesdrop\" + 0.000*\"nosniff\" + 0.000*\"dimensionsy\"'),\n",
       " (115,\n",
       "  '0.000*\"illiteracy\" + 0.000*\"uncorrelate\" + 0.000*\"generl\" + 0.000*\"proper\" + 0.000*\"poisson_processes\" + 0.000*\"stump\" + 0.000*\"tear\" + 0.000*\"ycol\" + 0.000*\"hypertexte\" + 0.000*\"syntactic\" + 0.000*\"repetition\" + 0.000*\"dividing\" + 0.000*\"winebag\" + 0.000*\"random\" + 0.000*\"react\"'),\n",
       " (128,\n",
       "  '0.000*\"bere\" + 0.000*\"advice\" + 0.000*\"mutual\" + 0.000*\"driveway\" + 0.000*\"packagespackage\" + 0.000*\"flu\" + 0.000*\"overly\" + 0.000*\"ilabl\" + 0.000*\"measurementssample\" + 0.000*\"purchase\" + 0.000*\"residential\" + 0.000*\"component\" + 0.000*\"enrollsin\" + 0.000*\"andplatform\" + 0.000*\"quently\"'),\n",
       " (4,\n",
       "  '0.000*\"comparment\" + 0.000*\"decompostion\" + 0.000*\"cumulative\" + 0.000*\"deal\" + 0.000*\"ùõΩùë•ùëñ\" + 0.000*\"coercion\" + 0.000*\"accounting\" + 0.000*\"strikingly\" + 0.000*\"experiment\" + 0.000*\"objective_function\" + 0.000*\"truthful\" + 0.000*\"stochastic_gradient\" + 0.000*\"equal\" + 0.000*\"interconnect\" + 0.000*\"pickle\"'),\n",
       " (3,\n",
       "  '0.000*\"eroom\" + 0.000*\"similarity\" + 0.000*\"righttouch\" + 0.000*\"pressure\" + 0.000*\"standardize\" + 0.000*\"quartersalesfi\" + 0.000*\"import\" + 0.000*\"homework\" + 0.000*\"ralker\" + 0.000*\"variance\" + 0.000*\"normal_normal\" + 0.000*\"critically\" + 0.000*\"known\" + 0.000*\"directly\" + 0.000*\"overfit\"'),\n",
       " (2,\n",
       "  '0.000*\"impossible\" + 0.000*\"uniquenesse\" + 0.000*\"nil\" + 0.000*\"consulting\" + 0.000*\"standalone\" + 0.000*\"retain\" + 0.000*\"modifiy\" + 0.000*\"usenet\" + 0.000*\"integer\" + 0.000*\"textbook\" + 0.000*\"editor\" + 0.000*\"googlemap\" + 0.000*\"lense\" + 0.000*\"healthcare\" + 0.000*\"opacity\"'),\n",
       " (1,\n",
       "  '0.000*\"adolescent\" + 0.000*\"noyb\" + 0.000*\"previously\" + 0.000*\"working\" + 0.000*\"collectively\" + 0.000*\"calculate\" + 0.000*\"outli\" + 0.000*\"dining\" + 0.000*\"lb\" + 0.000*\"orangepredicte\" + 0.000*\"autoload\" + 0.000*\"military\" + 0.000*\"evolving\" + 0.000*\"grape\" + 0.000*\"labz\"'),\n",
       " (0,\n",
       "  '0.000*\"imhbre\" + 0.000*\"better_business\" + 0.000*\"ofmisleade\" + 0.000*\"perpendicular\" + 0.000*\"trade\" + 0.000*\"engage\" + 0.000*\"informational\" + 0.000*\"discuss\" + 0.000*\"eme\" + 0.000*\"simulate\" + 0.000*\"linkedlntop\" + 0.000*\"hash\" + 0.000*\"epicentre\" + 0.000*\"programmatically\" + 0.000*\"synchronously\"')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suggested_lda_model = hdp_model.suggested_lda_model()\n",
    "suggested_lda_model.show_topics(num_topics=10, num_words=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics don't make sense. Notice how all word probabilities are < 0.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared = gensim_vis.prepare(suggested_lda_model, doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, 'Lectures_Test_run_HDP2LDA.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe the suggested_lda_model() doesn't work as intended, but the visualization is not very appealing. \n",
    "\n",
    "However, we saw earlier from the top 15 topics generated by HDP, that obtaining meaningful topics is possible\n",
    "\n",
    "We will now try tweaking some of the parameters and inputs for the LDA model, until the results are satisfactory. LDA is more desirable as we can visualize the results with pyLDAvis, get document-topic probabilities with built-in functions, and generally be able to explain the results a bit better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing different parameters for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.022*\"data\" + 0.008*\"function\" + 0.007*\"value\" + 0.007*\"use\" + 0.007*\"sample\" + 0.007*\"test\" + 0.006*\"set\" + 0.005*\"model\" + 0.004*\"estimate\" + 0.004*\"class\" + 0.004*\"method\" + 0.004*\"code\" + 0.004*\"database\" + 0.003*\"number\" + 0.003*\"error\" + 0.003*\"package\" + 0.003*\"create\" + 0.003*\"case\" + 0.003*\"mean\" + 0.003*\"time\"'),\n",
       " (1,\n",
       "  '0.012*\"data\" + 0.010*\"function\" + 0.010*\"value\" + 0.008*\"model\" + 0.007*\"use\" + 0.006*\"number\" + 0.005*\"time\" + 0.004*\"give\" + 0.004*\"sample\" + 0.004*\"set\" + 0.004*\"select\" + 0.003*\"create\" + 0.003*\"regression\" + 0.003*\"distribution\" + 0.003*\"result\" + 0.003*\"need\" + 0.003*\"table\" + 0.003*\"prior\" + 0.003*\"list\" + 0.003*\"add\"'),\n",
       " (2,\n",
       "  '0.029*\"data\" + 0.009*\"value\" + 0.007*\"model\" + 0.006*\"function\" + 0.005*\"number\" + 0.005*\"set\" + 0.005*\"column\" + 0.004*\"use\" + 0.004*\"group\" + 0.004*\"dataframe\" + 0.004*\"probability\" + 0.004*\"create\" + 0.004*\"output\" + 0.003*\"time\" + 0.003*\"variable\" + 0.003*\"mean\" + 0.003*\"new\" + 0.003*\"key\" + 0.003*\"problem\" + 0.003*\"type\"'),\n",
       " (3,\n",
       "  '0.013*\"data\" + 0.009*\"model\" + 0.009*\"value\" + 0.007*\"use\" + 0.005*\"function\" + 0.005*\"list\" + 0.005*\"number\" + 0.004*\"variable\" + 0.004*\"create\" + 0.004*\"set\" + 0.004*\"type\" + 0.004*\"table\" + 0.004*\"string\" + 0.004*\"distribution\" + 0.003*\"time\" + 0.003*\"method\" + 0.003*\"estimate\" + 0.003*\"sample\" + 0.003*\"relationship\" + 0.003*\"error\"'),\n",
       " (4,\n",
       "  '0.021*\"data\" + 0.012*\"model\" + 0.011*\"value\" + 0.006*\"class\" + 0.005*\"distribution\" + 0.005*\"function\" + 0.005*\"use\" + 0.005*\"probability\" + 0.005*\"time\" + 0.004*\"variable\" + 0.004*\"mean\" + 0.004*\"number\" + 0.004*\"sample\" + 0.004*\"set\" + 0.004*\"error\" + 0.004*\"point\" + 0.003*\"regression\" + 0.003*\"estimate\" + 0.003*\"give\" + 0.003*\"method\"')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changing number of topics \n",
    "new_lda_model = LdaModel(doc_term_matrix, num_topics=5, id2word = dictionary)\n",
    "new_lda_model.show_topics(num_words=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is again a lot of overlap, with the top 3 words being similar in all topics, as well as \"data\", \"function\", \"value\", occuring frequently in multiple topics. We likely need more topics, as we can see that topic 0 has some database concepts along with \"variable\" and \"time\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.023*\"data\" + 0.007*\"value\" + 0.005*\"use\" + 0.005*\"model\" + 0.005*\"number\" + 0.005*\"function\" + 0.004*\"set\" + 0.003*\"test\" + 0.003*\"find\" + 0.003*\"probability\"'),\n",
       " (1,\n",
       "  '0.023*\"data\" + 0.007*\"function\" + 0.007*\"value\" + 0.006*\"model\" + 0.005*\"set\" + 0.005*\"sample\" + 0.005*\"use\" + 0.004*\"create\" + 0.004*\"result\" + 0.004*\"class\"'),\n",
       " (2,\n",
       "  '0.014*\"data\" + 0.009*\"value\" + 0.008*\"use\" + 0.006*\"create\" + 0.006*\"function\" + 0.005*\"set\" + 0.004*\"output\" + 0.004*\"dataframe\" + 0.004*\"list\" + 0.003*\"table\"'),\n",
       " (3,\n",
       "  '0.015*\"model\" + 0.015*\"data\" + 0.009*\"value\" + 0.006*\"use\" + 0.005*\"set\" + 0.005*\"time\" + 0.005*\"distribution\" + 0.004*\"number\" + 0.004*\"probability\" + 0.004*\"function\"'),\n",
       " (4,\n",
       "  '0.028*\"data\" + 0.014*\"function\" + 0.012*\"model\" + 0.011*\"value\" + 0.006*\"use\" + 0.005*\"regression\" + 0.005*\"test\" + 0.005*\"set\" + 0.005*\"variable\" + 0.005*\"error\"'),\n",
       " (5,\n",
       "  '0.016*\"data\" + 0.010*\"value\" + 0.007*\"model\" + 0.006*\"create\" + 0.006*\"class\" + 0.006*\"use\" + 0.006*\"function\" + 0.005*\"output\" + 0.005*\"set\" + 0.004*\"table\"'),\n",
       " (6,\n",
       "  '0.016*\"data\" + 0.010*\"model\" + 0.007*\"value\" + 0.006*\"number\" + 0.005*\"function\" + 0.005*\"output\" + 0.005*\"use\" + 0.005*\"sample\" + 0.004*\"error\" + 0.004*\"create\"'),\n",
       " (7,\n",
       "  '0.017*\"data\" + 0.008*\"model\" + 0.007*\"time\" + 0.007*\"value\" + 0.006*\"set\" + 0.006*\"function\" + 0.006*\"use\" + 0.005*\"prior\" + 0.005*\"probability\" + 0.004*\"number\"'),\n",
       " (8,\n",
       "  '0.018*\"data\" + 0.012*\"sample\" + 0.009*\"value\" + 0.007*\"model\" + 0.005*\"function\" + 0.005*\"estimate\" + 0.005*\"distribution\" + 0.005*\"class\" + 0.005*\"use\" + 0.005*\"probability\"'),\n",
       " (9,\n",
       "  '0.016*\"data\" + 0.010*\"value\" + 0.006*\"model\" + 0.005*\"function\" + 0.005*\"table\" + 0.005*\"use\" + 0.004*\"class\" + 0.004*\"group\" + 0.004*\"number\" + 0.004*\"select\"'),\n",
       " (10,\n",
       "  '0.014*\"data\" + 0.009*\"sample\" + 0.007*\"model\" + 0.006*\"value\" + 0.006*\"class\" + 0.005*\"use\" + 0.004*\"estimate\" + 0.004*\"test\" + 0.004*\"function\" + 0.004*\"population\"'),\n",
       " (11,\n",
       "  '0.031*\"data\" + 0.009*\"value\" + 0.009*\"use\" + 0.007*\"model\" + 0.006*\"number\" + 0.005*\"variable\" + 0.005*\"function\" + 0.004*\"set\" + 0.004*\"sample\" + 0.004*\"database\"'),\n",
       " (12,\n",
       "  '0.015*\"data\" + 0.015*\"value\" + 0.009*\"function\" + 0.008*\"number\" + 0.007*\"model\" + 0.005*\"distribution\" + 0.005*\"use\" + 0.005*\"probability\" + 0.005*\"sample\" + 0.004*\"set\"')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changing number of topics \n",
    "new_lda_model = LdaModel(doc_term_matrix, num_topics=13, id2word = dictionary)\n",
    "new_lda_model.show_topics(num_topics = 13, num_words = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has somehow gotten worse at capturing some expected topics like databases from earlier. It seems that the underlying problem might be in another parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.019*\"function\" + 0.018*\"value\" + 0.015*\"data\" + 0.012*\"use\" + 0.010*\"test\" + 0.010*\"list\" + 0.008*\"code\" + 0.008*\"create\" + 0.008*\"column\" + 0.008*\"number\"'),\n",
       " (1,\n",
       "  '0.024*\"data\" + 0.018*\"model\" + 0.012*\"value\" + 0.010*\"function\" + 0.009*\"probability\" + 0.009*\"estimate\" + 0.008*\"distribution\" + 0.008*\"variable\" + 0.007*\"regression\" + 0.006*\"mean\"'),\n",
       " (2,\n",
       "  '0.021*\"set\" + 0.014*\"function\" + 0.013*\"pc\" + 0.012*\"point\" + 0.012*\"convex\" + 0.011*\"case\" + 0.010*\"method\" + 0.010*\"minimizer\" + 0.010*\"give\" + 0.009*\"let\"'),\n",
       " (3,\n",
       "  '0.029*\"data\" + 0.009*\"problem\" + 0.008*\"class\" + 0.008*\"big\" + 0.006*\"linear_discriminant\" + 0.006*\"machine\" + 0.005*\"course\" + 0.005*\"analysis\" + 0.005*\"work\" + 0.005*\"model\"'),\n",
       " (4,\n",
       "  '0.024*\"package\" + 0.012*\"problem\" + 0.011*\"module\" + 0.010*\"function\" + 0.009*\"condition\" + 0.009*\"solution\" + 0.009*\"error\" + 0.009*\"pypi\" + 0.007*\"solve\" + 0.006*\"let\"'),\n",
       " (5,\n",
       "  '0.020*\"layer\" + 0.016*\"relationship\" + 0.015*\"model\" + 0.014*\"network\" + 0.013*\"convolution\" + 0.012*\"image\" + 0.010*\"attribute\" + 0.009*\"entity\" + 0.009*\"block\" + 0.009*\"number\"'),\n",
       " (6,\n",
       "  '0.018*\"table\" + 0.012*\"query\" + 0.011*\"database\" + 0.010*\"select\" + 0.009*\"create\" + 0.009*\"graph\" + 0.009*\"data\" + 0.008*\"column\" + 0.007*\"branch\" + 0.007*\"value\"'),\n",
       " (7,\n",
       "  '0.035*\"sample\" + 0.021*\"prior\" + 0.018*\"model\" + 0.017*\"posterior\" + 0.016*\"distribution\" + 0.013*\"population\" + 0.012*\"data\" + 0.009*\"estimate\" + 0.009*\"probability\" + 0.008*\"variance\"'),\n",
       " (8,\n",
       "  '0.021*\"classifier\" + 0.019*\"support_vector\" + 0.018*\"observation\" + 0.018*\"hyperplane\" + 0.013*\"class\" + 0.012*\"margin\" + 0.011*\"data\" + 0.011*\"semi_supervised\" + 0.010*\"maximal_margin\" + 0.009*\"markdown\"'),\n",
       " (9,\n",
       "  '0.014*\"problem\" + 0.011*\"variable\" + 0.009*\"computing_environments\" + 0.008*\"step\" + 0.008*\"disk\" + 0.008*\"model\" + 0.007*\"data\" + 0.007*\"solution\" + 0.007*\"constraint\" + 0.007*\"find\"'),\n",
       " (10,\n",
       "  '0.032*\"data\" + 0.007*\"user\" + 0.007*\"use\" + 0.006*\"key\" + 0.006*\"system\" + 0.005*\"database\" + 0.005*\"information\" + 0.005*\"group\" + 0.005*\"web\" + 0.004*\"access\"'),\n",
       " (11,\n",
       "  '0.024*\"data\" + 0.013*\"class\" + 0.011*\"model\" + 0.009*\"input\" + 0.008*\"output\" + 0.008*\"time\" + 0.008*\"function\" + 0.007*\"value\" + 0.006*\"exception\" + 0.006*\"object\"'),\n",
       " (12,\n",
       "  '0.034*\"data\" + 0.016*\"value\" + 0.014*\"tree\" + 0.010*\"index\" + 0.010*\"column\" + 0.009*\"set\" + 0.007*\"dataframe\" + 0.006*\"model\" + 0.006*\"regression\" + 0.006*\"create\"')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying multiple passes through the corpus\n",
    "new_lda_model = LdaModel(doc_term_matrix, num_topics = 13, id2word = dictionary, passes = 10, random_state=448)\n",
    "new_lda_model.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics are not great, but we can see some topics for databases, probability, supervised learning, optimization, etc. These aren't as well-defined as the HDP topics we saw earlier, but this is a step in the right direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see if this improves with number of passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.020*\"function\" + 0.018*\"value\" + 0.015*\"data\" + 0.012*\"use\" + 0.010*\"list\" + 0.010*\"test\" + 0.008*\"code\" + 0.008*\"create\" + 0.008*\"string\" + 0.008*\"number\"'),\n",
       " (1,\n",
       "  '0.025*\"data\" + 0.019*\"model\" + 0.012*\"value\" + 0.010*\"function\" + 0.009*\"estimate\" + 0.009*\"probability\" + 0.008*\"variable\" + 0.008*\"distribution\" + 0.007*\"regression\" + 0.007*\"mean\"'),\n",
       " (2,\n",
       "  '0.022*\"set\" + 0.015*\"function\" + 0.014*\"pc\" + 0.013*\"point\" + 0.012*\"convex\" + 0.011*\"minimizer\" + 0.011*\"case\" + 0.011*\"method\" + 0.010*\"let\" + 0.010*\"give\"'),\n",
       " (3,\n",
       "  '0.031*\"data\" + 0.009*\"big\" + 0.009*\"problem\" + 0.008*\"class\" + 0.007*\"linear_discriminant\" + 0.006*\"machine\" + 0.006*\"course\" + 0.006*\"analysis\" + 0.005*\"work\" + 0.005*\"privacy\"'),\n",
       " (4,\n",
       "  '0.026*\"package\" + 0.015*\"module\" + 0.011*\"problem\" + 0.010*\"condition\" + 0.010*\"error\" + 0.009*\"pypi\" + 0.009*\"function\" + 0.008*\"solution\" + 0.007*\"solve\" + 0.006*\"install\"'),\n",
       " (5,\n",
       "  '0.021*\"layer\" + 0.016*\"relationship\" + 0.015*\"network\" + 0.014*\"model\" + 0.013*\"convolution\" + 0.013*\"image\" + 0.010*\"attribute\" + 0.009*\"entity\" + 0.009*\"block\" + 0.008*\"number\"'),\n",
       " (6,\n",
       "  '0.018*\"table\" + 0.013*\"database\" + 0.012*\"query\" + 0.011*\"create\" + 0.010*\"select\" + 0.009*\"data\" + 0.009*\"column\" + 0.008*\"rdd\" + 0.008*\"graph\" + 0.008*\"value\"'),\n",
       " (7,\n",
       "  '0.034*\"sample\" + 0.023*\"prior\" + 0.019*\"model\" + 0.018*\"posterior\" + 0.017*\"distribution\" + 0.013*\"population\" + 0.012*\"data\" + 0.011*\"probability\" + 0.009*\"estimate\" + 0.008*\"likelihood\"'),\n",
       " (8,\n",
       "  '0.024*\"classifier\" + 0.022*\"class\" + 0.020*\"observation\" + 0.020*\"support_vector\" + 0.018*\"hyperplane\" + 0.012*\"margin\" + 0.011*\"semi_supervised\" + 0.010*\"maximal_margin\" + 0.010*\"classification\" + 0.009*\"markdown\"'),\n",
       " (9,\n",
       "  '0.015*\"problem\" + 0.011*\"variable\" + 0.010*\"computing_environments\" + 0.009*\"solution\" + 0.009*\"step\" + 0.008*\"find\" + 0.008*\"sort\" + 0.008*\"disk\" + 0.008*\"constraint\" + 0.007*\"linear_regression\"'),\n",
       " (10,\n",
       "  '0.031*\"data\" + 0.007*\"user\" + 0.007*\"use\" + 0.006*\"key\" + 0.006*\"system\" + 0.005*\"information\" + 0.005*\"web\" + 0.004*\"access\" + 0.004*\"group\" + 0.004*\"database\"'),\n",
       " (11,\n",
       "  '0.023*\"data\" + 0.014*\"class\" + 0.010*\"model\" + 0.010*\"input\" + 0.009*\"output\" + 0.009*\"time\" + 0.007*\"function\" + 0.007*\"value\" + 0.007*\"exception\" + 0.006*\"object\"'),\n",
       " (12,\n",
       "  '0.038*\"data\" + 0.016*\"tree\" + 0.016*\"value\" + 0.012*\"index\" + 0.010*\"column\" + 0.010*\"set\" + 0.008*\"dataframe\" + 0.006*\"regression\" + 0.006*\"number\" + 0.005*\"model\"')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying multiple passes through the corpus\n",
    "new_lda_model = LdaModel(doc_term_matrix, num_topics = 13, id2word = dictionary, passes = 25, random_state=448)\n",
    "new_lda_model.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These topics are definitely easier to distinguish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.020*\"function\" + 0.018*\"value\" + 0.015*\"data\" + 0.012*\"use\" + 0.010*\"list\" + 0.010*\"test\" + 0.009*\"code\" + 0.008*\"create\" + 0.008*\"string\" + 0.008*\"number\"'),\n",
       " (1,\n",
       "  '0.025*\"data\" + 0.020*\"model\" + 0.012*\"value\" + 0.010*\"function\" + 0.010*\"estimate\" + 0.009*\"probability\" + 0.008*\"variable\" + 0.008*\"regression\" + 0.008*\"distribution\" + 0.007*\"mean\"'),\n",
       " (2,\n",
       "  '0.022*\"set\" + 0.015*\"function\" + 0.014*\"pc\" + 0.014*\"point\" + 0.012*\"convex\" + 0.012*\"minimizer\" + 0.011*\"let\" + 0.011*\"method\" + 0.011*\"case\" + 0.010*\"give\"'),\n",
       " (3,\n",
       "  '0.032*\"data\" + 0.009*\"big\" + 0.008*\"problem\" + 0.007*\"course\" + 0.007*\"analysis\" + 0.007*\"linear_discriminant\" + 0.007*\"machine\" + 0.007*\"class\" + 0.005*\"work\" + 0.005*\"time\"'),\n",
       " (4,\n",
       "  '0.029*\"package\" + 0.016*\"module\" + 0.011*\"problem\" + 0.010*\"error\" + 0.010*\"condition\" + 0.010*\"pypi\" + 0.007*\"function\" + 0.007*\"install\" + 0.007*\"code\" + 0.007*\"solution\"'),\n",
       " (5,\n",
       "  '0.022*\"layer\" + 0.017*\"network\" + 0.016*\"relationship\" + 0.014*\"model\" + 0.014*\"convolution\" + 0.013*\"image\" + 0.010*\"attribute\" + 0.009*\"entity\" + 0.009*\"block\" + 0.008*\"number\"'),\n",
       " (6,\n",
       "  '0.019*\"table\" + 0.014*\"database\" + 0.013*\"query\" + 0.011*\"create\" + 0.011*\"select\" + 0.010*\"data\" + 0.009*\"value\" + 0.009*\"column\" + 0.009*\"rdd\" + 0.008*\"graph\"'),\n",
       " (7,\n",
       "  '0.033*\"sample\" + 0.023*\"prior\" + 0.019*\"model\" + 0.017*\"posterior\" + 0.017*\"distribution\" + 0.013*\"population\" + 0.012*\"probability\" + 0.011*\"data\" + 0.009*\"likelihood\" + 0.008*\"estimate\"'),\n",
       " (8,\n",
       "  '0.028*\"class\" + 0.024*\"classifier\" + 0.021*\"observation\" + 0.020*\"support_vector\" + 0.017*\"hyperplane\" + 0.012*\"classification\" + 0.012*\"margin\" + 0.010*\"semi_supervised\" + 0.010*\"maximal_margin\" + 0.009*\"markdown\"'),\n",
       " (9,\n",
       "  '0.016*\"problem\" + 0.012*\"variable\" + 0.010*\"computing_environments\" + 0.009*\"solution\" + 0.009*\"sort\" + 0.009*\"find\" + 0.008*\"step\" + 0.008*\"constraint\" + 0.008*\"disk\" + 0.007*\"linear_regression\"'),\n",
       " (10,\n",
       "  '0.030*\"data\" + 0.007*\"use\" + 0.007*\"user\" + 0.006*\"key\" + 0.006*\"system\" + 0.006*\"web\" + 0.006*\"information\" + 0.004*\"access\" + 0.004*\"privacy\" + 0.004*\"provide\"'),\n",
       " (11,\n",
       "  '0.023*\"data\" + 0.014*\"class\" + 0.010*\"input\" + 0.010*\"model\" + 0.009*\"output\" + 0.009*\"time\" + 0.007*\"function\" + 0.007*\"exception\" + 0.007*\"value\" + 0.006*\"object\"'),\n",
       " (12,\n",
       "  '0.042*\"data\" + 0.016*\"tree\" + 0.016*\"value\" + 0.012*\"index\" + 0.011*\"column\" + 0.010*\"set\" + 0.009*\"dataframe\" + 0.006*\"group\" + 0.006*\"regression\" + 0.006*\"number\"')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lda_model = LdaModel(doc_term_matrix, num_topics = 13, id2word = dictionary, passes = 50, random_state=448)\n",
    "new_lda_model.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the best so far! While some topics might not be all that clear, Topic 2 is probability/statistics, Topic 3 is linear optimization, Topic 4 seems to be Python programming, Topic 5 is neual networks, Topic 6 is database related, Topic 7 is Bayesian stats, Topic 8 is supervised learning techniques.  \n",
    "\n",
    "Let's see if more passes improves the quality of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.021*\"function\" + 0.018*\"value\" + 0.014*\"data\" + 0.012*\"use\" + 0.011*\"list\" + 0.010*\"test\" + 0.009*\"code\" + 0.009*\"create\" + 0.008*\"string\" + 0.008*\"number\"'),\n",
       " (1,\n",
       "  '0.026*\"data\" + 0.021*\"model\" + 0.013*\"value\" + 0.010*\"estimate\" + 0.010*\"function\" + 0.009*\"variable\" + 0.009*\"probability\" + 0.008*\"regression\" + 0.007*\"distribution\" + 0.007*\"mean\"'),\n",
       " (2,\n",
       "  '0.022*\"set\" + 0.016*\"function\" + 0.014*\"point\" + 0.014*\"pc\" + 0.012*\"minimizer\" + 0.012*\"let\" + 0.012*\"convex\" + 0.011*\"method\" + 0.010*\"case\" + 0.010*\"give\"'),\n",
       " (3,\n",
       "  '0.032*\"data\" + 0.010*\"big\" + 0.008*\"problem\" + 0.008*\"course\" + 0.008*\"analysis\" + 0.007*\"linear_discriminant\" + 0.007*\"machine\" + 0.005*\"class\" + 0.005*\"work\" + 0.005*\"material\"'),\n",
       " (4,\n",
       "  '0.030*\"package\" + 0.018*\"module\" + 0.011*\"error\" + 0.010*\"condition\" + 0.010*\"pypi\" + 0.010*\"problem\" + 0.007*\"install\" + 0.007*\"code\" + 0.007*\"python_package\" + 0.007*\"function\"'),\n",
       " (5,\n",
       "  '0.023*\"layer\" + 0.017*\"network\" + 0.016*\"relationship\" + 0.014*\"convolution\" + 0.013*\"image\" + 0.013*\"model\" + 0.010*\"attribute\" + 0.010*\"entity\" + 0.009*\"block\" + 0.008*\"number\"'),\n",
       " (6,\n",
       "  '0.019*\"table\" + 0.015*\"database\" + 0.013*\"query\" + 0.011*\"create\" + 0.011*\"select\" + 0.010*\"data\" + 0.009*\"value\" + 0.009*\"column\" + 0.009*\"rdd\" + 0.008*\"graph\"'),\n",
       " (7,\n",
       "  '0.033*\"sample\" + 0.023*\"prior\" + 0.018*\"model\" + 0.017*\"posterior\" + 0.017*\"distribution\" + 0.013*\"probability\" + 0.013*\"population\" + 0.010*\"data\" + 0.009*\"likelihood\" + 0.008*\"estimate\"'),\n",
       " (8,\n",
       "  '0.032*\"class\" + 0.023*\"classifier\" + 0.021*\"observation\" + 0.019*\"support_vector\" + 0.016*\"hyperplane\" + 0.016*\"classification\" + 0.011*\"margin\" + 0.010*\"semi_supervised\" + 0.010*\"maximal_margin\" + 0.009*\"markdown\"'),\n",
       " (9,\n",
       "  '0.016*\"problem\" + 0.012*\"variable\" + 0.010*\"computing_environments\" + 0.010*\"solution\" + 0.009*\"find\" + 0.009*\"sort\" + 0.009*\"constraint\" + 0.008*\"step\" + 0.008*\"disk\" + 0.007*\"linear_regression\"'),\n",
       " (10,\n",
       "  '0.029*\"data\" + 0.007*\"use\" + 0.006*\"user\" + 0.006*\"web\" + 0.006*\"key\" + 0.006*\"information\" + 0.006*\"system\" + 0.004*\"privacy\" + 0.004*\"access\" + 0.004*\"security\"'),\n",
       " (11,\n",
       "  '0.022*\"data\" + 0.015*\"class\" + 0.011*\"input\" + 0.010*\"model\" + 0.010*\"output\" + 0.009*\"time\" + 0.007*\"exception\" + 0.007*\"function\" + 0.007*\"value\" + 0.006*\"object\"'),\n",
       " (12,\n",
       "  '0.043*\"data\" + 0.016*\"value\" + 0.016*\"tree\" + 0.013*\"index\" + 0.012*\"column\" + 0.010*\"set\" + 0.009*\"dataframe\" + 0.008*\"group\" + 0.007*\"cell\" + 0.006*\"cluster\"')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lda_model = LdaModel(doc_term_matrix, num_topics = 13, id2word = dictionary, passes = 100, random_state=448)\n",
    "new_lda_model.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics didn't change much, so we can say that the estimates for topic distributions converge after sufficient number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared = gensim_vis.prepare(new_lda_model, doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, 'Lectures_100pass_LDA_13.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the visualization, we can see clear zones with probability, excel, databases, and Python programming! It is especially interesting to see how there is a supercluster for probability distributions, which encompasses/is close to a topic about Markov-Chain related theory! Also, it should be noted that many topics are mostly junk and contain seemingly unrelated words while accounting for few tokens.\n",
    "\n",
    "We should eventually hone in on an appropriate value for k, after deciding on all the other parameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before that, we can also try changing another parameter in LdaModel called 'alpha'. The default value of alpha is 'symmetric' which assumes a symmetric Dirichlet prior over the topic distributions. \n",
    "\n",
    "Essentially, this means that all our topic models till now have assumed alpha to be a vector of length k where all values are equal (default value is 1/k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      " 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308 0.07692308\n",
      " 0.07692308]\n"
     ]
    }
   ],
   "source": [
    "print(new_lda_model.alpha)\n",
    "\n",
    "# Since k = 13, all values of alpha will be 1/13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.025*\"image\" + 0.019*\"convolution\" + 0.019*\"markov_chain\" + 0.019*\"state\" + 0.013*\"transition\" + 0.012*\"compartment\" + 0.011*\"probability\" + 0.011*\"cross_validation\" + 0.010*\"model\" + 0.010*\"layer\"'),\n",
       " (1,\n",
       "  '0.036*\"data\" + 0.016*\"value\" + 0.015*\"model\" + 0.014*\"regression\" + 0.010*\"relationship\" + 0.009*\"sample\" + 0.007*\"variable\" + 0.007*\"select\" + 0.007*\"error\" + 0.007*\"residual\"'),\n",
       " (2,\n",
       "  '0.020*\"set\" + 0.014*\"function\" + 0.011*\"point\" + 0.011*\"minimizer\" + 0.011*\"let\" + 0.011*\"pc\" + 0.011*\"convex\" + 0.011*\"method\" + 0.011*\"problem\" + 0.010*\"case\"'),\n",
       " (3,\n",
       "  '0.027*\"class\" + 0.015*\"factor\" + 0.015*\"test\" + 0.013*\"effect\" + 0.011*\"block\" + 0.011*\"experiment\" + 0.010*\"design\" + 0.009*\"classifier\" + 0.009*\"hyperplane\" + 0.008*\"support_vector\"'),\n",
       " (4,\n",
       "  '0.012*\"time\" + 0.012*\"data\" + 0.010*\"rdd\" + 0.008*\"function\" + 0.008*\"cell\" + 0.008*\"problem\" + 0.008*\"point\" + 0.007*\"find\" + 0.007*\"value\" + 0.006*\"number\"'),\n",
       " (5,\n",
       "  '0.016*\"data\" + 0.011*\"probability\" + 0.009*\"privacy\" + 0.009*\"posterior_exchangeability\" + 0.009*\"information\" + 0.009*\"likelihood_prior\" + 0.007*\"layer\" + 0.007*\"record\" + 0.006*\"block\" + 0.006*\"brief_history\"'),\n",
       " (6,\n",
       "  '0.014*\"data\" + 0.009*\"change\" + 0.008*\"create\" + 0.008*\"command\" + 0.008*\"branch\" + 0.008*\"value\" + 0.008*\"user\" + 0.007*\"use\" + 0.007*\"plot\" + 0.006*\"dash\"'),\n",
       " (7,\n",
       "  '0.025*\"data\" + 0.017*\"model\" + 0.011*\"tree\" + 0.008*\"set\" + 0.008*\"classification\" + 0.008*\"estimate\" + 0.007*\"training\" + 0.007*\"value\" + 0.007*\"class\" + 0.007*\"input\"'),\n",
       " (8,\n",
       "  '0.028*\"model\" + 0.021*\"distribution\" + 0.016*\"data\" + 0.016*\"function\" + 0.015*\"prior\" + 0.015*\"value\" + 0.012*\"probability\" + 0.011*\"posterior\" + 0.011*\"sample\" + 0.009*\"likelihood\"'),\n",
       " (9,\n",
       "  '0.076*\"sample\" + 0.045*\"population\" + 0.020*\"probability\" + 0.018*\"estimate\" + 0.018*\"unit\" + 0.017*\"stratum\" + 0.015*\"strata\" + 0.015*\"srswor\" + 0.012*\"variance\" + 0.011*\"estimator\"'),\n",
       " (10,\n",
       "  '0.030*\"data\" + 0.017*\"value\" + 0.017*\"function\" + 0.011*\"use\" + 0.011*\"object\" + 0.010*\"list\" + 0.009*\"dataframe\" + 0.008*\"string\" + 0.008*\"create\" + 0.008*\"index\"'),\n",
       " (11,\n",
       "  '0.021*\"table\" + 0.016*\"database\" + 0.013*\"query\" + 0.012*\"data\" + 0.010*\"select\" + 0.008*\"create\" + 0.007*\"web\" + 0.007*\"column\" + 0.007*\"employee\" + 0.006*\"group\"'),\n",
       " (12,\n",
       "  '0.033*\"data\" + 0.008*\"use\" + 0.006*\"system\" + 0.005*\"information\" + 0.005*\"software\" + 0.004*\"normal_regression\" + 0.004*\"package\" + 0.004*\"code\" + 0.004*\"key\" + 0.004*\"user\"')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing alpha = 'auto', meaning that the alpha prior is no longer assumed to be symmetric and is learned from the data. \n",
    "\n",
    "auto_lda_model = LdaModel(doc_term_matrix, num_topics = 13, id2word = dictionary, alpha = 'auto', passes = 50)\n",
    "auto_lda_model.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01037392 0.02794973 0.01626494 0.01367742 0.02262196 0.01463735\n",
      " 0.02639266 0.03807209 0.03883517 0.01059209 0.04052699 0.01785196\n",
      " 0.02190316]\n"
     ]
    }
   ],
   "source": [
    "print(auto_lda_model.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared = gensim_vis.prepare(auto_lda_model, doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, 'Lectures_Alpha_LDA_13.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is even better! Neural networks, traditional machine learning algorithms, predictive modeling on time series, etc... Plus a new smaller cluster for command line and source control. Seems like 10 topics is a good amount. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same thing with the parameter \"eta\" which is the name used by gensim for the \"beta\" Dirichlet priors from the paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.034*\"database\" + 0.023*\"data\" + 0.021*\"user\" + 0.013*\"privilege\" + 0.010*\"view\" + 0.010*\"macro\" + 0.010*\"object\" + 0.008*\"create\" + 0.008*\"access\" + 0.008*\"grant\" + 0.007*\"use\" + 0.007*\"select\" + 0.007*\"update\"'),\n",
       " (1,\n",
       "  '0.022*\"model\" + 0.020*\"distribution\" + 0.015*\"prior\" + 0.013*\"data\" + 0.012*\"probability\" + 0.012*\"value\" + 0.011*\"posterior\" + 0.011*\"sample\" + 0.009*\"function\" + 0.009*\"normal\" + 0.008*\"mean\" + 0.008*\"number\" + 0.008*\"simulate\"'),\n",
       " (2,\n",
       "  '0.016*\"table\" + 0.009*\"value\" + 0.008*\"select\" + 0.008*\"data\" + 0.008*\"block\" + 0.008*\"group\" + 0.008*\"factor\" + 0.007*\"employee\" + 0.007*\"attribute\" + 0.007*\"query\" + 0.007*\"relationship\" + 0.007*\"design\" + 0.007*\"relation\"'),\n",
       " (3,\n",
       "  '0.042*\"data\" + 0.010*\"privacy\" + 0.007*\"information\" + 0.006*\"table\" + 0.006*\"use\" + 0.005*\"value\" + 0.005*\"record\" + 0.005*\"column\" + 0.005*\"security\" + 0.005*\"right\" + 0.004*\"analysis\" + 0.004*\"key\" + 0.004*\"time\"'),\n",
       " (4,\n",
       "  '0.012*\"branch\" + 0.011*\"create\" + 0.011*\"change\" + 0.010*\"add\" + 0.010*\"use\" + 0.010*\"plot\" + 0.010*\"dash\" + 0.009*\"value\" + 0.009*\"repository\" + 0.009*\"app\" + 0.007*\"data\" + 0.007*\"code\" + 0.007*\"main\"'),\n",
       " (5,\n",
       "  '0.025*\"function\" + 0.019*\"class\" + 0.014*\"test\" + 0.014*\"value\" + 0.013*\"object\" + 0.013*\"list\" + 0.013*\"code\" + 0.012*\"package\" + 0.011*\"string\" + 0.011*\"use\" + 0.011*\"def\" + 0.011*\"error\" + 0.009*\"exception\"'),\n",
       " (6,\n",
       "  '0.026*\"sample\" + 0.012*\"population\" + 0.010*\"data\" + 0.008*\"unit\" + 0.005*\"estimate\" + 0.005*\"course\" + 0.005*\"error\" + 0.005*\"use\" + 0.005*\"student\" + 0.005*\"weight\" + 0.004*\"stratum\" + 0.004*\"probability\" + 0.004*\"command\"'),\n",
       " (7,\n",
       "  '0.022*\"data\" + 0.013*\"time\" + 0.011*\"search\" + 0.010*\"value\" + 0.009*\"tree\" + 0.009*\"input\" + 0.009*\"cell\" + 0.009*\"structure\" + 0.008*\"output\" + 0.007*\"function\" + 0.007*\"array\" + 0.006*\"sort\" + 0.006*\"operation\"'),\n",
       " (8,\n",
       "  '0.031*\"data\" + 0.027*\"model\" + 0.013*\"estimate\" + 0.013*\"regression\" + 0.012*\"value\" + 0.011*\"function\" + 0.010*\"observation\" + 0.009*\"variable\" + 0.008*\"fit\" + 0.008*\"predictor\" + 0.008*\"class\" + 0.007*\"layer\" + 0.007*\"error\"'),\n",
       " (9,\n",
       "  '0.046*\"data\" + 0.015*\"value\" + 0.009*\"rdd\" + 0.009*\"use\" + 0.007*\"type\" + 0.007*\"element\" + 0.005*\"column\" + 0.005*\"store\" + 0.005*\"create\" + 0.005*\"spark\" + 0.005*\"document\" + 0.005*\"open\" + 0.005*\"work\"'),\n",
       " (10,\n",
       "  '0.013*\"data\" + 0.008*\"service\" + 0.007*\"network\" + 0.007*\"web\" + 0.007*\"address\" + 0.007*\"key\" + 0.006*\"server\" + 0.006*\"distribute\" + 0.006*\"message\" + 0.005*\"internet\" + 0.005*\"parallel\" + 0.005*\"api\" + 0.005*\"program\"'),\n",
       " (11,\n",
       "  '0.026*\"data\" + 0.022*\"dataframe\" + 0.016*\"index\" + 0.015*\"column\" + 0.010*\"value\" + 0.010*\"probability\" + 0.008*\"posterior_exchangeability\" + 0.008*\"likelihood_prior\" + 0.007*\"output\" + 0.007*\"use\" + 0.006*\"create\" + 0.006*\"key\" + 0.006*\"graph\"'),\n",
       " (12,\n",
       "  '0.016*\"set\" + 0.015*\"data\" + 0.011*\"problem\" + 0.010*\"method\" + 0.010*\"solution\" + 0.009*\"point\" + 0.008*\"let\" + 0.008*\"function\" + 0.008*\"group\" + 0.007*\"cluster\" + 0.007*\"case\" + 0.007*\"value\" + 0.007*\"give\"')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_lda_model = LdaModel(doc_term_matrix, num_topics = 13, id2word = dictionary, alpha = 'auto', eta = 'auto', passes = 50)\n",
    "auto_lda_model.show_topics(num_topics = -1, num_words = 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at the visualization directly to judge these topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02473091 0.01621987 0.00616194 0.01416105 0.01736566 0.01356093\n",
      " 0.01139303 0.01502603 0.01140727 0.01734054]\n",
      "[0.10823856 0.08902945 0.08991552 ... 0.08943335 0.09130245 0.09040242]\n"
     ]
    }
   ],
   "source": [
    "print(auto_lda_model.alpha)\n",
    "print(auto_lda_model.eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared = gensim_vis.prepare(auto_lda_model, doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, 'Lectures_AlphaEta_LDA_13.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the visualization, we can see that these topics are fairly good. There are very few clearly junk topics, and \"nearby\" topics seem to be related how we would expect. However, looking closer, we can see that some unrelated modules are combined into topics, like visualization and version control in Topic 9. We also don't see a clear topic for deep learning concepts, which are taught in one or two modules. We need to try playing around with the topic number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([(0.03080338, 'data'),\n",
       "   (0.027073132, 'model'),\n",
       "   (0.012912613, 'estimate'),\n",
       "   (0.012740664, 'regression'),\n",
       "   (0.011573426, 'value'),\n",
       "   (0.010506095, 'function'),\n",
       "   (0.010155926, 'observation'),\n",
       "   (0.008580755, 'variable'),\n",
       "   (0.008272204, 'fit'),\n",
       "   (0.008062827, 'predictor')],\n",
       "  -0.6835266504471039),\n",
       " ([(0.025470572, 'function'),\n",
       "   (0.01914074, 'class'),\n",
       "   (0.013960703, 'test'),\n",
       "   (0.013805784, 'value'),\n",
       "   (0.013312178, 'object'),\n",
       "   (0.013249324, 'list'),\n",
       "   (0.013077197, 'code'),\n",
       "   (0.012108417, 'package'),\n",
       "   (0.011469826, 'string'),\n",
       "   (0.0109232515, 'use')],\n",
       "  -0.7212922984581832),\n",
       " ([(0.022434076, 'model'),\n",
       "   (0.020137763, 'distribution'),\n",
       "   (0.015208991, 'prior'),\n",
       "   (0.0125170695, 'data'),\n",
       "   (0.012285216, 'probability'),\n",
       "   (0.011503181, 'value'),\n",
       "   (0.0114624845, 'posterior'),\n",
       "   (0.010662859, 'sample'),\n",
       "   (0.009220459, 'function'),\n",
       "   (0.008731245, 'normal')],\n",
       "  -0.7574259545818814),\n",
       " ([(0.021919647, 'data'),\n",
       "   (0.012912154, 'time'),\n",
       "   (0.01055452, 'search'),\n",
       "   (0.009704697, 'value'),\n",
       "   (0.00922769, 'tree'),\n",
       "   (0.009192748, 'input'),\n",
       "   (0.009164053, 'cell'),\n",
       "   (0.008768699, 'structure'),\n",
       "   (0.0076160757, 'output'),\n",
       "   (0.007375447, 'function')],\n",
       "  -0.8067851880577974),\n",
       " ([(0.02580582, 'sample'),\n",
       "   (0.01220844, 'population'),\n",
       "   (0.009500216, 'data'),\n",
       "   (0.0077087176, 'unit'),\n",
       "   (0.0052291485, 'estimate'),\n",
       "   (0.00520544, 'course'),\n",
       "   (0.0051992764, 'error'),\n",
       "   (0.005089828, 'use'),\n",
       "   (0.00497039, 'student'),\n",
       "   (0.004897908, 'weight')],\n",
       "  -0.8319963976390772),\n",
       " ([(0.01642674, 'set'),\n",
       "   (0.014537574, 'data'),\n",
       "   (0.011174548, 'problem'),\n",
       "   (0.009682597, 'method'),\n",
       "   (0.009501326, 'solution'),\n",
       "   (0.008668378, 'point'),\n",
       "   (0.00831528, 'let'),\n",
       "   (0.008110931, 'function'),\n",
       "   (0.007951395, 'group'),\n",
       "   (0.007327766, 'cluster')],\n",
       "  -0.859340404605195),\n",
       " ([(0.04238149, 'data'),\n",
       "   (0.009558689, 'privacy'),\n",
       "   (0.006516757, 'information'),\n",
       "   (0.005772653, 'table'),\n",
       "   (0.005684635, 'use'),\n",
       "   (0.005374114, 'value'),\n",
       "   (0.005196181, 'record'),\n",
       "   (0.0047696126, 'column'),\n",
       "   (0.004652588, 'security'),\n",
       "   (0.004507399, 'right')],\n",
       "  -0.9147231493541598),\n",
       " ([(0.013347916, 'data'),\n",
       "   (0.008480055, 'service'),\n",
       "   (0.0071430537, 'network'),\n",
       "   (0.006828393, 'web'),\n",
       "   (0.006810607, 'address'),\n",
       "   (0.006783168, 'key'),\n",
       "   (0.006347346, 'server'),\n",
       "   (0.0058054025, 'distribute'),\n",
       "   (0.005568787, 'message'),\n",
       "   (0.0054969173, 'internet')],\n",
       "  -0.9816146856873377),\n",
       " ([(0.04555898, 'data'),\n",
       "   (0.015413321, 'value'),\n",
       "   (0.008852386, 'rdd'),\n",
       "   (0.008740821, 'use'),\n",
       "   (0.006644359, 'type'),\n",
       "   (0.0065039406, 'element'),\n",
       "   (0.005494978, 'column'),\n",
       "   (0.0054477463, 'store'),\n",
       "   (0.0053227814, 'create'),\n",
       "   (0.0049319156, 'spark')],\n",
       "  -1.020177916296232),\n",
       " ([(0.015771043, 'table'),\n",
       "   (0.008912236, 'value'),\n",
       "   (0.008480979, 'select'),\n",
       "   (0.008082274, 'data'),\n",
       "   (0.007995704, 'block'),\n",
       "   (0.007783869, 'group'),\n",
       "   (0.0076674125, 'factor'),\n",
       "   (0.0073667346, 'employee'),\n",
       "   (0.0073599615, 'attribute'),\n",
       "   (0.007177611, 'query')],\n",
       "  -1.3048984593681547),\n",
       " ([(0.012134232, 'branch'),\n",
       "   (0.011473637, 'create'),\n",
       "   (0.010678598, 'change'),\n",
       "   (0.009927331, 'add'),\n",
       "   (0.009718347, 'use'),\n",
       "   (0.009611457, 'plot'),\n",
       "   (0.009605548, 'dash'),\n",
       "   (0.009077008, 'value'),\n",
       "   (0.008838619, 'repository'),\n",
       "   (0.008796558, 'app')],\n",
       "  -1.7844077457654015),\n",
       " ([(0.03412908, 'database'),\n",
       "   (0.02283866, 'data'),\n",
       "   (0.021161713, 'user'),\n",
       "   (0.012953632, 'privilege'),\n",
       "   (0.010221067, 'view'),\n",
       "   (0.010078174, 'macro'),\n",
       "   (0.00974526, 'object'),\n",
       "   (0.008323094, 'create'),\n",
       "   (0.008178361, 'access'),\n",
       "   (0.0077181375, 'grant')],\n",
       "  -2.754395994638238),\n",
       " ([(0.025801877, 'data'),\n",
       "   (0.021569429, 'dataframe'),\n",
       "   (0.015844477, 'index'),\n",
       "   (0.015141495, 'column'),\n",
       "   (0.010415289, 'value'),\n",
       "   (0.01012251, 'probability'),\n",
       "   (0.0076350495, 'posterior_exchangeability'),\n",
       "   (0.0076316902, 'likelihood_prior'),\n",
       "   (0.0074520684, 'output'),\n",
       "   (0.006710225, 'use')],\n",
       "  -4.490680898381744)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_lda_model.top_topics(doc_term_matrix, dictionary=dictionary, coherence='u_mass', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These coherence scores are worse than the ones we saw in the first model, which is a good enough reason not to use such metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing k-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since the priors are no longer symmetric, the estimation can not be parallelized so it will take noticably longer. We first checked 7,10,13,17,25. \n",
    "\n",
    "7 and 10 were not enough, as there was too much overlap between unrelated topics. 25 was too much; had several junk topics and some topics with 0% of tokens. We then decided to add 15\n",
    "\n",
    "We then compared 15 and 17, and both seemed to have a few confusing topics, with 17 causing only having two topics (Topics #13 and #16) with a few words be unrelated. We decided to have one final comparison with 17, 18, and 19 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.021*\"data\" + 0.020*\"model\" + 0.016*\"sample\" + 0.012*\"distribution\" + 0.012*\"value\" + 0.009*\"probability\" + 0.008*\"prior\" + 0.008*\"estimate\" + 0.008*\"function\" + 0.008*\"mean\"'),\n",
       " (1,\n",
       "  '0.028*\"data\" + 0.007*\"database\" + 0.006*\"system\" + 0.005*\"class\" + 0.005*\"user\" + 0.005*\"use\" + 0.005*\"key\" + 0.004*\"web\" + 0.004*\"group\" + 0.004*\"cluster\"'),\n",
       " (2,\n",
       "  '0.019*\"data\" + 0.019*\"model\" + 0.011*\"predictor\" + 0.011*\"set\" + 0.011*\"class\" + 0.009*\"relationship\" + 0.009*\"branch\" + 0.008*\"estimate\" + 0.008*\"classification\" + 0.007*\"observation\"'),\n",
       " (3,\n",
       "  '0.016*\"data\" + 0.016*\"layer\" + 0.015*\"model\" + 0.011*\"network\" + 0.010*\"input\" + 0.009*\"training\" + 0.008*\"output\" + 0.007*\"image\" + 0.007*\"function\" + 0.006*\"value\"'),\n",
       " (4,\n",
       "  '0.042*\"data\" + 0.014*\"value\" + 0.012*\"dataframe\" + 0.009*\"column\" + 0.008*\"use\" + 0.007*\"index\" + 0.007*\"output\" + 0.007*\"time\" + 0.006*\"operation\" + 0.006*\"rdd\"'),\n",
       " (5,\n",
       "  '0.014*\"tree\" + 0.012*\"set\" + 0.009*\"problem\" + 0.008*\"factor\" + 0.008*\"function\" + 0.008*\"solution\" + 0.007*\"method\" + 0.007*\"point\" + 0.007*\"case\" + 0.007*\"value\"'),\n",
       " (6,\n",
       "  '0.014*\"value\" + 0.014*\"function\" + 0.011*\"use\" + 0.009*\"create\" + 0.009*\"data\" + 0.009*\"table\" + 0.009*\"list\" + 0.007*\"number\" + 0.007*\"code\" + 0.007*\"column\"')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_lda_model = LdaModel(doc_term_matrix, num_topics = NUM_TOPICS, id2word = dictionary, alpha = 'auto', eta = 'auto', passes = 50)\n",
    "\n",
    "LDAvis_prepared = gensim_vis.prepare(final_lda_model, doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, f'Lectures_Final_{NUM_TOPICS}.html')\n",
    "\n",
    "final_lda_model.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.023*\"data\" + 0.008*\"class\" + 0.007*\"attribute\" + 0.007*\"use\" + 0.006*\"information\" + 0.005*\"privacy\" + 0.005*\"relationship\" + 0.005*\"project\" + 0.004*\"model\" + 0.004*\"type\"'),\n",
       " (1,\n",
       "  '0.037*\"sample\" + 0.017*\"population\" + 0.010*\"estimate\" + 0.010*\"value\" + 0.009*\"set\" + 0.008*\"use\" + 0.008*\"unit\" + 0.007*\"probability\" + 0.007*\"mean\" + 0.007*\"data\"'),\n",
       " (2,\n",
       "  '0.017*\"problem\" + 0.014*\"solution\" + 0.012*\"function\" + 0.010*\"point\" + 0.010*\"set\" + 0.010*\"minimizer\" + 0.010*\"convex\" + 0.010*\"let\" + 0.009*\"constraint\" + 0.008*\"find\"'),\n",
       " (3,\n",
       "  '0.026*\"data\" + 0.017*\"dataframe\" + 0.012*\"value\" + 0.012*\"column\" + 0.010*\"output\" + 0.008*\"use\" + 0.008*\"create\" + 0.007*\"function\" + 0.007*\"rdd\" + 0.007*\"layer\"'),\n",
       " (4,\n",
       "  '0.022*\"data\" + 0.012*\"table\" + 0.010*\"database\" + 0.008*\"select\" + 0.008*\"value\" + 0.007*\"create\" + 0.007*\"use\" + 0.007*\"user\" + 0.007*\"column\" + 0.006*\"query\"'),\n",
       " (5,\n",
       "  '0.010*\"branch\" + 0.009*\"factor\" + 0.008*\"effect\" + 0.008*\"design\" + 0.008*\"experiment\" + 0.008*\"change\" + 0.008*\"test\" + 0.007*\"block\" + 0.007*\"value\" + 0.007*\"number\"'),\n",
       " (6,\n",
       "  '0.029*\"model\" + 0.019*\"data\" + 0.019*\"distribution\" + 0.016*\"function\" + 0.016*\"prior\" + 0.014*\"probability\" + 0.012*\"posterior\" + 0.011*\"value\" + 0.010*\"likelihood\" + 0.008*\"parameter\"'),\n",
       " (7,\n",
       "  '0.034*\"data\" + 0.024*\"model\" + 0.013*\"regression\" + 0.012*\"value\" + 0.010*\"variable\" + 0.009*\"estimate\" + 0.007*\"error\" + 0.007*\"set\" + 0.007*\"fit\" + 0.007*\"training\"'),\n",
       " (8,\n",
       "  '0.022*\"data\" + 0.015*\"tree\" + 0.010*\"class\" + 0.009*\"classification\" + 0.006*\"set\" + 0.006*\"distance\" + 0.006*\"analysis\" + 0.005*\"model\" + 0.005*\"regression\" + 0.005*\"observation\"'),\n",
       " (9,\n",
       "  '0.022*\"data\" + 0.018*\"function\" + 0.016*\"value\" + 0.011*\"list\" + 0.011*\"test\" + 0.009*\"object\" + 0.009*\"error\" + 0.009*\"string\" + 0.008*\"code\" + 0.008*\"number\"')]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_lda_model = LdaModel(doc_term_matrix, num_topics = NUM_TOPICS, id2word = dictionary, alpha = 'auto', eta = 'auto', passes = 50)\n",
    "\n",
    "LDAvis_prepared = gensim_vis.prepare(final_lda_model, doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, f'Lectures_Final_{NUM_TOPICS}.html')\n",
    "\n",
    "final_lda_model.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.019*\"data\" + 0.013*\"value\" + 0.010*\"plot\" + 0.009*\"column\" + 0.009*\"cell\" + 0.008*\"use\" + 0.007*\"select\" + 0.007*\"dash\" + 0.007*\"function\" + 0.006*\"point\"'),\n",
       " (1,\n",
       "  '0.032*\"table\" + 0.030*\"database\" + 0.019*\"query\" + 0.018*\"select\" + 0.014*\"user\" + 0.012*\"value\" + 0.012*\"employee\" + 0.010*\"list\" + 0.010*\"column\" + 0.010*\"string\"'),\n",
       " (2,\n",
       "  '0.061*\"data\" + 0.016*\"value\" + 0.012*\"column\" + 0.011*\"dataframe\" + 0.010*\"type\" + 0.010*\"group\" + 0.008*\"set\" + 0.008*\"cluster\" + 0.008*\"attribute\" + 0.008*\"relationship\"'),\n",
       " (3,\n",
       "  '0.018*\"function\" + 0.013*\"value\" + 0.012*\"use\" + 0.011*\"list\" + 0.011*\"data\" + 0.009*\"number\" + 0.008*\"object\" + 0.008*\"graph\" + 0.007*\"code\" + 0.006*\"search\"'),\n",
       " (4,\n",
       "  '0.066*\"class\" + 0.016*\"module\" + 0.014*\"method\" + 0.014*\"def\" + 0.012*\"object\" + 0.011*\"attribute\" + 0.011*\"error\" + 0.011*\"package\" + 0.010*\"age\" + 0.010*\"data\"'),\n",
       " (5,\n",
       "  '0.022*\"test\" + 0.015*\"branch\" + 0.014*\"command\" + 0.012*\"code\" + 0.012*\"change\" + 0.011*\"create\" + 0.010*\"repository\" + 0.009*\"directory\" + 0.008*\"command_line\" + 0.007*\"file\"'),\n",
       " (6,\n",
       "  '0.032*\"prior\" + 0.019*\"tree\" + 0.015*\"likelihood\" + 0.015*\"set\" + 0.014*\"posterior\" + 0.009*\"beta_binomial\" + 0.009*\"value\" + 0.008*\"give\" + 0.008*\"problem\" + 0.008*\"case\"'),\n",
       " (7,\n",
       "  '0.028*\"data\" + 0.027*\"model\" + 0.013*\"value\" + 0.012*\"probability\" + 0.012*\"distribution\" + 0.011*\"estimate\" + 0.010*\"function\" + 0.009*\"regression\" + 0.008*\"sample\" + 0.008*\"variable\"'),\n",
       " (8,\n",
       "  '0.015*\"data\" + 0.014*\"rdd\" + 0.013*\"exception\" + 0.011*\"spark\" + 0.008*\"function\" + 0.008*\"program\" + 0.008*\"operation\" + 0.008*\"value\" + 0.007*\"error\" + 0.007*\"use\"'),\n",
       " (9,\n",
       "  '0.016*\"function\" + 0.016*\"posterior\" + 0.013*\"model\" + 0.010*\"let\" + 0.010*\"prior\" + 0.010*\"normal_regression\" + 0.009*\"chain\" + 0.009*\"problem\" + 0.008*\"solution\" + 0.008*\"point\"'),\n",
       " (10,\n",
       "  '0.080*\"sample\" + 0.040*\"population\" + 0.018*\"unit\" + 0.015*\"stratum\" + 0.013*\"variance\" + 0.013*\"estimate\" + 0.013*\"strata\" + 0.013*\"srswor\" + 0.011*\"student\" + 0.010*\"estimator\"'),\n",
       " (11,\n",
       "  '0.030*\"package\" + 0.016*\"data_rr\" + 0.016*\"pypi\" + 0.012*\"lasso\" + 0.010*\"loss\" + 0.009*\"multiple_linear\" + 0.008*\"data\" + 0.008*\"python_package\" + 0.008*\"upload\" + 0.008*\"model\"'),\n",
       " (12,\n",
       "  '0.030*\"data\" + 0.007*\"information\" + 0.006*\"use\" + 0.006*\"privacy\" + 0.005*\"key\" + 0.005*\"system\" + 0.005*\"block\" + 0.005*\"web\" + 0.005*\"record\" + 0.004*\"security\"'),\n",
       " (13,\n",
       "  '0.028*\"data\" + 0.013*\"hyperplane\" + 0.012*\"classifier\" + 0.012*\"stream\" + 0.011*\"observation\" + 0.010*\"time\" + 0.009*\"margin\" + 0.008*\"maximal_margin\" + 0.008*\"query\" + 0.007*\"support_vector\"'),\n",
       " (14,\n",
       "  '0.017*\"layer\" + 0.011*\"network\" + 0.010*\"input\" + 0.010*\"factor\" + 0.010*\"block\" + 0.009*\"design\" + 0.008*\"effect\" + 0.007*\"output\" + 0.007*\"image\" + 0.007*\"task\"')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_lda_model = LdaModel(doc_term_matrix, num_topics = NUM_TOPICS, id2word = dictionary, alpha = 'auto', eta = 'auto', passes = 50)\n",
    "\n",
    "LDAvis_prepared = gensim_vis.prepare(final_lda_model, doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, f'Lectures_Final_{NUM_TOPICS}.html')\n",
    "\n",
    "final_lda_model.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.025*\"data\" + 0.025*\"value\" + 0.021*\"function\" + 0.013*\"use\" + 0.010*\"list\" + 0.010*\"column\" + 0.008*\"create\" + 0.008*\"string\" + 0.008*\"object\" + 0.008*\"output\"'),\n",
       " (1,\n",
       "  '0.030*\"data\" + 0.015*\"rdd\" + 0.011*\"privacy\" + 0.010*\"probability\" + 0.009*\"record\" + 0.008*\"information\" + 0.008*\"brief_history\" + 0.007*\"motivating_review\" + 0.005*\"spark\" + 0.005*\"individual\"'),\n",
       " (2,\n",
       "  '0.032*\"data\" + 0.025*\"model\" + 0.010*\"estimate\" + 0.009*\"regression\" + 0.009*\"value\" + 0.008*\"set\" + 0.008*\"observation\" + 0.008*\"variable\" + 0.008*\"fit\" + 0.008*\"predictor\"'),\n",
       " (3,\n",
       "  '0.019*\"audience\" + 0.015*\"mixture_models\" + 0.014*\"email\" + 0.013*\"proposal\" + 0.012*\"default\" + 0.011*\"student\" + 0.011*\"write\" + 0.010*\"communication\" + 0.010*\"use\" + 0.010*\"presentation\"'),\n",
       " (4,\n",
       "  '0.021*\"likelihood_prior\" + 0.020*\"posterior_exchangeability\" + 0.019*\"prior\" + 0.017*\"likelihood\" + 0.017*\"function\" + 0.015*\"posterior\" + 0.013*\"data\" + 0.012*\"condition\" + 0.010*\"probability\" + 0.010*\"distribution\"'),\n",
       " (5,\n",
       "  '0.016*\"data\" + 0.015*\"relationship\" + 0.012*\"attribute\" + 0.012*\"problem\" + 0.010*\"model\" + 0.009*\"relation\" + 0.009*\"type\" + 0.008*\"entity\" + 0.008*\"value\" + 0.008*\"element\"'),\n",
       " (6,\n",
       "  '0.036*\"data\" + 0.028*\"dataframe\" + 0.018*\"column\" + 0.015*\"table\" + 0.015*\"value\" + 0.014*\"stream\" + 0.013*\"count\" + 0.013*\"spark\" + 0.012*\"time\" + 0.011*\"data_carts\"'),\n",
       " (7,\n",
       "  '0.039*\"data\" + 0.012*\"group\" + 0.011*\"image\" + 0.010*\"layer\" + 0.009*\"cell\" + 0.009*\"cluster\" + 0.008*\"convolution\" + 0.008*\"number\" + 0.007*\"column\" + 0.007*\"add\"'),\n",
       " (8,\n",
       "  '0.044*\"test\" + 0.017*\"data\" + 0.016*\"testing\" + 0.012*\"data_rr\" + 0.010*\"cross_validation\" + 0.008*\"case\" + 0.008*\"method\" + 0.008*\"def\" + 0.008*\"lasso\" + 0.008*\"database\"'),\n",
       " (9,\n",
       "  '0.036*\"sample\" + 0.016*\"population\" + 0.014*\"factor\" + 0.012*\"variance\" + 0.011*\"mean\" + 0.011*\"estimate\" + 0.010*\"design\" + 0.010*\"unit\" + 0.009*\"effect\" + 0.008*\"data\"'),\n",
       " (10,\n",
       "  '0.018*\"set\" + 0.011*\"branch\" + 0.010*\"method\" + 0.010*\"let\" + 0.010*\"pc\" + 0.010*\"function\" + 0.009*\"point\" + 0.009*\"minimizer\" + 0.009*\"convex\" + 0.008*\"case\"'),\n",
       " (11,\n",
       "  '0.021*\"data\" + 0.011*\"table\" + 0.009*\"database\" + 0.008*\"user\" + 0.007*\"select\" + 0.007*\"use\" + 0.006*\"system\" + 0.006*\"web\" + 0.006*\"query\" + 0.005*\"create\"'),\n",
       " (12,\n",
       "  '0.034*\"model\" + 0.032*\"prior\" + 0.024*\"posterior\" + 0.016*\"data\" + 0.015*\"distribution\" + 0.011*\"likelihood\" + 0.011*\"sample\" + 0.010*\"beta_binomial\" + 0.010*\"normal_regression\" + 0.009*\"chain\"'),\n",
       " (13,\n",
       "  '0.020*\"data\" + 0.010*\"time\" + 0.009*\"tree\" + 0.008*\"index\" + 0.007*\"key\" + 0.007*\"year\" + 0.007*\"value\" + 0.007*\"level\" + 0.006*\"dataframe\" + 0.006*\"parallel\"'),\n",
       " (14,\n",
       "  '0.016*\"package\" + 0.010*\"code\" + 0.009*\"classifier\" + 0.009*\"support_vector\" + 0.009*\"hyperplane\" + 0.009*\"module\" + 0.007*\"macro\" + 0.007*\"variable\" + 0.006*\"command\" + 0.006*\"find\"'),\n",
       " (15,\n",
       "  '0.021*\"probability\" + 0.021*\"value\" + 0.021*\"function\" + 0.021*\"distribution\" + 0.019*\"model\" + 0.014*\"data\" + 0.014*\"simulate\" + 0.011*\"number\" + 0.010*\"time\" + 0.009*\"normal\"'),\n",
       " (16,\n",
       "  '0.037*\"class\" + 0.019*\"exception\" + 0.017*\"graph\" + 0.013*\"object\" + 0.013*\"vertex\" + 0.013*\"def\" + 0.012*\"edge\" + 0.011*\"error\" + 0.009*\"data\" + 0.009*\"attribute\"')]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_lda_model = LdaModel(doc_term_matrix, num_topics = NUM_TOPICS, id2word = dictionary, alpha = 'auto', eta = 'auto', passes = 75)\n",
    "\n",
    "LDAvis_prepared = gensim_vis.prepare(final_lda_model, doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, f'Lectures_Final_{NUM_TOPICS}.html')\n",
    "\n",
    "final_lda_model.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.014*\"address\" + 0.014*\"network\" + 0.010*\"server\" + 0.009*\"command\" + 0.008*\"client\" + 0.008*\"internet\" + 0.008*\"loop\" + 0.007*\"data\" + 0.007*\"ip_address\" + 0.007*\"host\"'),\n",
       " (1,\n",
       "  '0.028*\"data\" + 0.023*\"value\" + 0.022*\"function\" + 0.018*\"model\" + 0.013*\"distribution\" + 0.012*\"time\" + 0.011*\"probability\" + 0.010*\"simulate\" + 0.009*\"mean\" + 0.009*\"number\"'),\n",
       " (2,\n",
       "  '0.012*\"branch\" + 0.010*\"function\" + 0.009*\"solution\" + 0.008*\"problem\" + 0.008*\"point\" + 0.008*\"let\" + 0.008*\"constraint\" + 0.008*\"support_vector\" + 0.007*\"graph\" + 0.007*\"hyperplane\"'),\n",
       " (3,\n",
       "  '0.046*\"data\" + 0.008*\"web\" + 0.008*\"group\" + 0.007*\"cluster\" + 0.007*\"use\" + 0.007*\"element\" + 0.006*\"set\" + 0.006*\"open\" + 0.005*\"graph\" + 0.005*\"software\"'),\n",
       " (4,\n",
       "  '0.021*\"model\" + 0.019*\"data\" + 0.011*\"estimate\" + 0.010*\"layer\" + 0.009*\"class\" + 0.008*\"set\" + 0.008*\"training\" + 0.007*\"value\" + 0.007*\"classification\" + 0.007*\"network\"'),\n",
       " (5,\n",
       "  '0.030*\"state\" + 0.028*\"markov_chain\" + 0.028*\"probability\" + 0.020*\"model\" + 0.019*\"transition\" + 0.018*\"data\" + 0.014*\"simulate\" + 0.013*\"transition_matrix\" + 0.011*\"function\" + 0.011*\"class\"'),\n",
       " (6,\n",
       "  '0.024*\"set\" + 0.014*\"pc\" + 0.011*\"case\" + 0.011*\"method\" + 0.009*\"give\" + 0.009*\"value\" + 0.009*\"projection\" + 0.009*\"sign\" + 0.009*\"point\" + 0.008*\"selection\"'),\n",
       " (7,\n",
       "  '0.034*\"data\" + 0.018*\"column\" + 0.016*\"cell\" + 0.015*\"value\" + 0.011*\"select\" + 0.008*\"row\" + 0.007*\"formula\" + 0.007*\"cassandra\" + 0.007*\"function\" + 0.007*\"table\"'),\n",
       " (8,\n",
       "  '0.024*\"data\" + 0.007*\"use\" + 0.007*\"array\" + 0.007*\"value\" + 0.006*\"index\" + 0.006*\"problem\" + 0.006*\"search\" + 0.006*\"output\" + 0.005*\"time\" + 0.005*\"need\"'),\n",
       " (9,\n",
       "  '0.023*\"function\" + 0.019*\"value\" + 0.016*\"test\" + 0.012*\"code\" + 0.011*\"use\" + 0.011*\"package\" + 0.010*\"error\" + 0.010*\"def\" + 0.008*\"number\" + 0.008*\"create\"'),\n",
       " (10,\n",
       "  '0.021*\"factor\" + 0.018*\"effect\" + 0.017*\"distribution\" + 0.015*\"sample\" + 0.015*\"design\" + 0.015*\"experiment\" + 0.011*\"level\" + 0.011*\"treatment\" + 0.010*\"mean\" + 0.010*\"interaction\"'),\n",
       " (11,\n",
       "  '0.019*\"user\" + 0.019*\"tree\" + 0.017*\"command\" + 0.017*\"data\" + 0.015*\"directory\" + 0.014*\"privilege\" + 0.011*\"command_line\" + 0.009*\"view\" + 0.009*\"system\" + 0.009*\"file\"'),\n",
       " (12,\n",
       "  '0.029*\"table\" + 0.029*\"database\" + 0.018*\"query\" + 0.017*\"data\" + 0.015*\"select\" + 0.013*\"attribute\" + 0.013*\"value\" + 0.012*\"relation\" + 0.011*\"employee\" + 0.011*\"relationship\"'),\n",
       " (13,\n",
       "  '0.021*\"data\" + 0.021*\"probability\" + 0.011*\"likelihood_prior\" + 0.011*\"tree\" + 0.011*\"posterior_exchangeability\" + 0.010*\"distribution\" + 0.009*\"likelihood\" + 0.009*\"prior\" + 0.008*\"value\" + 0.007*\"brief_history\"'),\n",
       " (14,\n",
       "  '0.049*\"sample\" + 0.024*\"prior\" + 0.022*\"population\" + 0.013*\"variance\" + 0.011*\"probability\" + 0.011*\"model\" + 0.011*\"distribution\" + 0.010*\"unit\" + 0.010*\"estimate\" + 0.010*\"likelihood\"'),\n",
       " (15,\n",
       "  '0.029*\"data\" + 0.018*\"class\" + 0.009*\"key\" + 0.008*\"privacy\" + 0.008*\"object\" + 0.008*\"use\" + 0.007*\"index\" + 0.007*\"attribute\" + 0.006*\"information\" + 0.006*\"security\"'),\n",
       " (16,\n",
       "  '0.041*\"data\" + 0.038*\"model\" + 0.024*\"regression\" + 0.012*\"variable\" + 0.010*\"value\" + 0.009*\"simple_linear\" + 0.009*\"posterior\" + 0.008*\"normal_regression\" + 0.008*\"prior\" + 0.008*\"fit\"'),\n",
       " (17,\n",
       "  '0.020*\"data\" + 0.014*\"dataframe\" + 0.012*\"rdd\" + 0.011*\"string\" + 0.011*\"spark\" + 0.010*\"value\" + 0.010*\"list\" + 0.009*\"use\" + 0.009*\"time\" + 0.008*\"operation\"')]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_lda_model = LdaModel(doc_term_matrix, num_topics = NUM_TOPICS, id2word = dictionary, alpha = 'auto', eta = 'auto', passes = 75)\n",
    "\n",
    "LDAvis_prepared = gensim_vis.prepare(final_lda_model, doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, f'Lectures_Final_Chosen_{NUM_TOPICS}.html')\n",
    "\n",
    "final_lda_model.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared = gensim_vis.prepare(final_lda_model, doc_term_matrix, dictionary, mds='mmds')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, f'Lectures_Final_Chosen_{NUM_TOPICS}.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lda_model.save(datapath('Lectures_Chosen_18'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words = []\n",
    "for i in range(1,NUM_TOPICS+1): \n",
    "    topic = {}   \n",
    "    topic[i] = LDAvis_prepared.sorted_terms(topic = i, _lambda = 0.67)[:30][\"Term\"].tolist()\n",
    "    topic_words.append(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: ['model', 'layer', 'training', 'estimate', 'data', 'classification', 'network', 'class', 'observation', 'prediction', 'set', 'convolution', 'block', 'input', 'method', 'image', 'response', 'fit', 'predictor', 'error', 'parameter', 'predict', 'variable', 'task', 'weight', 'bootstrap', 'vector', 'matrix', 'neural_networks', 'output']}\n",
      "{2: ['data', 'array', 'index', 'audience', 'search', 'macro', 'problem', 'hash', 'algorithm', 'item', 'output', 'use', 'structure', 'information', 'queue', 'stack', 'need', 'email', 'record', 'pypi', 'proposal', 'list', 'sort', 'communication', 'time', 'object', 'purpose', 'value', 'business', 'presentation']}\n",
      "{3: ['function', 'test', 'code', 'package', 'value', 'def', 'exception', 'dash', 'module', 'error', 'use', 'app', 'plot', 'object', 'create', 'python', 'argument', 'try', 'widget', 'output', 'number', 'add', 'pass', 'raise', 'testing', 'column', 'write', 'list', 'run', 'install']}\n",
      "{4: ['function', 'value', 'data', 'simulate', 'distribution', 'model', 'probability', 'poisson', 'time', 'random_variable', 'measurement', 'independent', 'distance', 'random_variables', 'log_likelihood', 'expected_value', 'moving_linearity', 'log', 'mean', 'normal', 'success', 'probability_density', 'variable', 'number', 'time_series', 'event', 'count', 'missing_values', 'exponential', 'link_function']}\n",
      "{5: ['data', 'web', 'cluster', 'open', 'group', 'element', 'computing_environments', 'document', 'api', 'graph', 'software', 'predictive_modelling', 'wage_genes', 'request', 'graphframe', 'path', 'step_examples', 'step_stan', 'edge', 'server', 'xml', 'open_source', 'convex_analysis', 'distance', 'computer', 'application', 'use', 'notation', 'set', 'tag']}\n",
      "{6: ['rdd', 'spark', 'dataframe', 'string', 'operation', 'data', 'memory', 'stream', 'list', 'distribute', 'parallel', 'hadoop', 'compartment', 'disk', 'time', 'item', 'value', 'element', 'line', 'create', 'process', 'partition', 'use', 'markdown', 'word', 'output', 'program', 'count', 'thread', 'format']}\n",
      "{7: ['model', 'regression', 'data', 'simple_linear', 'normal_regression', 'multiple_linear', 'intercept', 'variable', 'posterior', 'data_carts', 'residual', 'fit', 'model_coding', 'prior', 'plot', 'cross_validation', 'linear_regression', 'coefficient', 'degrees_freedom', 'simulation', 'motivate', 'predictor', 'error', 'chain', 'data_rf', 'estimate', 'prior_diagnostics', 'value', 'tree', 'sample']}\n",
      "{8: ['class', 'data', 'privacy', 'key', 'security', 'index', 'attribute', 'object', 'public_key', 'encrypt', 'nan_nan', 'age', 'year', 'private_key', 'information', 'consent', 'protection', 'inheritance', 'ethic', 'child', 'right', 'risk', 'attack', 'use', 'encryption', 'person', 'public', 'gdpr', 'private', 'dataframe']}\n",
      "{9: ['branch', 'hyperplane', 'support_vector', 'classifier', 'solution', 'constraint', 'minimizer', 'graph', 'convex', 'vertex', 'maximal_margin', 'margin', 'problem', 'let', 'point', 'repository', 'commit', 'git', 'dual', 'find', 'solve', 'ax', 'change', 'function', 'git_branch', 'problem_minimize', 'new', 'git_push', 'objective_function', 'visit']}\n",
      "{10: ['database', 'table', 'query', 'relation', 'employee', 'attribute', 'select', 'sql', 'emp', 'relationship', 'relational', 'entity', 'title', 'salary', 'primary_key', 'clause', 'join', 'title_salary', 'select_ename', 'project', 'foreign_key', 'row', 'dept', 'manager', 'null', 'update', 'statement', 'engineer', 'eno_ename', 'budget']}\n",
      "{11: ['sample', 'population', 'prior', 'variance', 'beta_binomial', 'stratum', 'sample_size', 'strata', 'srswor', 'likelihood', 'unit', 'normal_normal', 'student', 'data_rr', 'probability', 'conjugate_prior', 'beta_prior', 'posterior', 'distribution', 'result_posterior', 'stratified_sampling', 'model_specifying', 'ui_non', 'informative_prior', 'estimate', 'estimator', 'parameters_plotting', 'mean', 'beta', 'random_sampling']}\n",
      "{12: ['probability', 'posterior_exchangeability', 'likelihood_prior', 'motivating_review', 'brief_history', 'mixture_models', 'tree', 'likelihood', 'data_nmf', 'coin', 'bayesian', 'data', 'frequentist', 'coin_toss', 'prior', 'distribution', 'node', 'head', 'mixture', 'pmf', 'posterior', 'subtree', 'binary_tree', 'bin', 'score', 'test_screening', 'outlier', 'pca', 'event', 'bayesian_inference']}\n",
      "{13: ['cell', 'column', 'data', 'cassandra', 'semi_supervised', 'review_view', 'formula', 'revenue', 'developer_team', 'select', 'spreadsheet', 'home_insert', 'ready', 'aggregate_functions', 'layout_formulas', 'tell_sign', 'value', 'chocolate_ball', 'chart', 'row', 'sales_excel', 'ball_chocolate', 'tag', 'format', 'sort_filter', 'filter', 'price_cost', 'subquery', 'na_na', 'share']}\n",
      "{14: ['factor', 'effect', 'experiment', 'treatment', 'metropolis_algorithm', 'design', 'mcmc', 'diagnostic', 'distribution', 'experimental', 'interaction', 'level', 'markov_chains', 'metropolis_hastings', 'sample', 'difference', 'factorial', 'average', 'study', 'posterior', 'burn_mcmc', 'crd', 'trace_plots', 'drug', 'plot', 'mean', 'stationary_distribution', 'response', 'chain', 'contrast']}\n",
      "{15: ['privilege', 'directory', 'command', 'user', 'tree', 'command_line', 'grant', 'boost', 'view', 'file', 'window', 'transaction', 'itemset', 'system', 'association_rules', 'revoke', 'rlawrenc', 'vegetable', 'course', 'data', 'milk', 'command_prompt', 'fruit', 'master', 'mac_os', 'material', 'navigate', 'grocery', 'relative_path', 'change']}\n",
      "{16: ['pc', 'set', 'projected_subgradient', 'descent', 'suppose_convex', 'projection', 'stochastic_gradient', 'sign', 'bound', 'set_minimizers', 'subgradient', 'chosen_randomly', 'recall', 'stochastic_projected', 'descent_directions', 'convex_subset', 'convergence', 'spgm', 'closed_convex', 'convex', 'regular', 'alternating_projections', 'necessarily_differentiable', 'basic_subgradient', 'convex_nonsmooth', 'optimal_value', 'nonempty_closed', 'plant', 'recall_maximum', 'convex_feasibility']}\n",
      "{17: ['address', 'network', 'packet', 'ip_address', 'client', 'server', 'loop', 'internet', 'host', 'slider', 'mac_address', 'port', 'shell', 'echo', 'switch', 'command', 'send', 'router', 'bit', 'script', 'bind', 'legend', 'secure', 'message', 'genre', 'ip', 'ethernet', 'service', 'connect', 'non_null']}\n",
      "{18: ['markov_chain', 'transition', 'state', 'transition_matrix', 'probability', 'emission', 'hidden_markov', 'def_compute', 'hmm', 'simulate', 'circle_circle', 'circle_radius', 'model', 'hidden_states', 'windspee', 'initial', 'circle', 'size_prob', 'matrix', 'sequence', 'class', 'radius_def', 'centre', 'data', 'dict', 'noon', 'observe', 'viterbi', 'infile', 'midnight']}\n"
     ]
    }
   ],
   "source": [
    "for i in range(NUM_TOPICS):\n",
    "    print(topic_words[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.052*\"sample\" + 0.043*\"error\" + 0.036*\"exception\" + 0.021*\"unit\" + 0.018*\"population\" + 0.017*\"try\" + 0.014*\"raise\" + 0.012*\"student\" + 0.011*\"def\" + 0.011*\"program\"'),\n",
       " (1,\n",
       "  '0.034*\"data\" + 0.024*\"table\" + 0.020*\"column\" + 0.017*\"dataframe\" + 0.016*\"value\" + 0.015*\"create\" + 0.014*\"select\" + 0.013*\"row\" + 0.012*\"query\" + 0.011*\"attribute\"'),\n",
       " (2,\n",
       "  '0.020*\"factor\" + 0.015*\"effect\" + 0.013*\"block\" + 0.012*\"mean\" + 0.012*\"design\" + 0.011*\"group\" + 0.009*\"variable\" + 0.009*\"response\" + 0.009*\"model\" + 0.008*\"level\"'),\n",
       " (3,\n",
       "  '0.035*\"probability\" + 0.020*\"hyperplane\" + 0.019*\"classifier\" + 0.017*\"observation\" + 0.015*\"margin\" + 0.015*\"brief_history\" + 0.014*\"motivating_review\" + 0.012*\"maximal_margin\" + 0.011*\"data\" + 0.011*\"bayesian\"'),\n",
       " (4,\n",
       "  '0.048*\"class\" + 0.034*\"classification\" + 0.020*\"data_carts\" + 0.017*\"tree\" + 0.016*\"regression\" + 0.015*\"support_vector\" + 0.014*\"motivate\" + 0.012*\"classifier\" + 0.012*\"observation\" + 0.011*\"linear\"'),\n",
       " (5,\n",
       "  '0.014*\"macro\" + 0.012*\"diagnostic\" + 0.011*\"metropolis_algorithm\" + 0.009*\"time_step\" + 0.009*\"input\" + 0.008*\"use\" + 0.008*\"value\" + 0.007*\"mcmc\" + 0.007*\"hidden_state\" + 0.007*\"posterior\"'),\n",
       " (6,\n",
       "  '0.028*\"prior\" + 0.025*\"sample\" + 0.014*\"population\" + 0.014*\"variance\" + 0.014*\"model\" + 0.014*\"distribution\" + 0.013*\"likelihood\" + 0.013*\"posterior\" + 0.010*\"estimate\" + 0.009*\"mean\"'),\n",
       " (7,\n",
       "  '0.021*\"function\" + 0.012*\"value\" + 0.012*\"layer\" + 0.010*\"number\" + 0.010*\"use\" + 0.009*\"network\" + 0.008*\"input\" + 0.006*\"relationship\" + 0.006*\"output\" + 0.006*\"code\"'),\n",
       " (8,\n",
       "  '0.030*\"tree\" + 0.015*\"module\" + 0.013*\"package\" + 0.012*\"boost\" + 0.010*\"variable\" + 0.008*\"problem\" + 0.008*\"region\" + 0.007*\"step\" + 0.007*\"split\" + 0.007*\"value\"'),\n",
       " (9,\n",
       "  '0.036*\"data\" + 0.011*\"group\" + 0.010*\"cluster\" + 0.010*\"use\" + 0.010*\"index\" + 0.009*\"item\" + 0.007*\"list\" + 0.006*\"year\" + 0.006*\"open\" + 0.006*\"time\"'),\n",
       " (10,\n",
       "  '0.019*\"set\" + 0.016*\"point\" + 0.014*\"function\" + 0.013*\"problem\" + 0.011*\"solution\" + 0.011*\"pc\" + 0.010*\"method\" + 0.010*\"let\" + 0.010*\"minimizer\" + 0.010*\"convex\"'),\n",
       " (11,\n",
       "  '0.030*\"class\" + 0.018*\"def\" + 0.017*\"value\" + 0.015*\"plot\" + 0.014*\"dash\" + 0.012*\"app\" + 0.011*\"data\" + 0.011*\"column\" + 0.010*\"posterior_exchangeability\" + 0.010*\"function\"'),\n",
       " (12,\n",
       "  '0.024*\"model\" + 0.020*\"function\" + 0.017*\"value\" + 0.016*\"distribution\" + 0.015*\"data\" + 0.010*\"probability\" + 0.010*\"sample\" + 0.009*\"simulate\" + 0.009*\"time\" + 0.008*\"number\"'),\n",
       " (13,\n",
       "  '0.061*\"moving_linearity\" + 0.040*\"data\" + 0.018*\"weight\" + 0.014*\"height\" + 0.012*\"intercept\" + 0.012*\"value\" + 0.011*\"age\" + 0.011*\"bidder\" + 0.011*\"model\" + 0.009*\"variable\"'),\n",
       " (14,\n",
       "  '0.042*\"data\" + 0.028*\"value\" + 0.019*\"cell\" + 0.017*\"column\" + 0.010*\"select\" + 0.009*\"row\" + 0.009*\"missing_values\" + 0.009*\"formula\" + 0.008*\"format\" + 0.008*\"review_view\"'),\n",
       " (15,\n",
       "  '0.050*\"data\" + 0.030*\"model\" + 0.015*\"estimate\" + 0.013*\"regression\" + 0.011*\"set\" + 0.010*\"value\" + 0.009*\"observation\" + 0.009*\"predictor\" + 0.009*\"fit\" + 0.009*\"variable\"'),\n",
       " (16,\n",
       "  '0.021*\"data\" + 0.013*\"graph\" + 0.010*\"time\" + 0.008*\"tree\" + 0.008*\"vertex\" + 0.007*\"edge\" + 0.007*\"search\" + 0.007*\"web\" + 0.006*\"structure\" + 0.006*\"stream\"'),\n",
       " (17,\n",
       "  '0.018*\"database\" + 0.016*\"data\" + 0.010*\"string\" + 0.009*\"document\" + 0.009*\"value\" + 0.007*\"write\" + 0.007*\"key\" + 0.007*\"audience\" + 0.007*\"element\" + 0.007*\"use\"'),\n",
       " (18,\n",
       "  '0.023*\"data\" + 0.007*\"use\" + 0.007*\"system\" + 0.006*\"command\" + 0.006*\"create\" + 0.006*\"code\" + 0.005*\"key\" + 0.005*\"change\" + 0.005*\"branch\" + 0.005*\"program\"')]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_lda_model = LdaModel(doc_term_matrix, num_topics = NUM_TOPICS, id2word = dictionary, alpha = 'auto', eta = 'auto', passes = 50)\n",
    "\n",
    "LDAvis_prepared = gensim_vis.prepare(final_lda_model, doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, f'Lectures_Final_{NUM_TOPICS}.html')\n",
    "\n",
    "final_lda_model.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.024*\"web\" + 0.013*\"data\" + 0.010*\"internet\" + 0.010*\"request\" + 0.007*\"new\" + 0.007*\"network\" + 0.007*\"application\" + 0.007*\"convex_analysis\" + 0.006*\"packet\" + 0.006*\"port\"'),\n",
       " (1,\n",
       "  '0.029*\"layer\" + 0.026*\"network\" + 0.017*\"block\" + 0.014*\"address\" + 0.013*\"number\" + 0.010*\"class\" + 0.009*\"increase\" + 0.009*\"key\" + 0.008*\"convolution\" + 0.007*\"bit\"'),\n",
       " (2,\n",
       "  '0.030*\"tree\" + 0.014*\"search\" + 0.012*\"problem\" + 0.012*\"graph\" + 0.009*\"node\" + 0.009*\"algorithm\" + 0.009*\"order\" + 0.008*\"data\" + 0.008*\"sort\" + 0.007*\"structure\"'),\n",
       " (3,\n",
       "  '0.044*\"data\" + 0.031*\"model\" + 0.014*\"value\" + 0.013*\"regression\" + 0.011*\"estimate\" + 0.011*\"function\" + 0.010*\"variable\" + 0.008*\"set\" + 0.008*\"fit\" + 0.007*\"probability\"'),\n",
       " (4,\n",
       "  '0.051*\"state\" + 0.048*\"markov_chain\" + 0.033*\"distribution\" + 0.033*\"probability\" + 0.028*\"transition\" + 0.021*\"simulate\" + 0.019*\"transition_matrix\" + 0.018*\"stationary_distribution\" + 0.013*\"time\" + 0.012*\"model\"'),\n",
       " (5,\n",
       "  '0.022*\"data\" + 0.018*\"value\" + 0.015*\"use\" + 0.014*\"object\" + 0.013*\"function\" + 0.012*\"list\" + 0.010*\"element\" + 0.009*\"string\" + 0.009*\"type\" + 0.008*\"command\"'),\n",
       " (6,\n",
       "  '0.000*\"data\" + 0.000*\"number\" + 0.000*\"value\" + 0.000*\"use\" + 0.000*\"model\" + 0.000*\"function\" + 0.000*\"sample\" + 0.000*\"code\" + 0.000*\"list\" + 0.000*\"set\"'),\n",
       " (7,\n",
       "  '0.030*\"class\" + 0.021*\"classification\" + 0.021*\"observation\" + 0.017*\"classifier\" + 0.015*\"image\" + 0.013*\"support_vector\" + 0.013*\"training\" + 0.010*\"cross_validation\" + 0.009*\"convolution\" + 0.009*\"data\"'),\n",
       " (8,\n",
       "  '0.018*\"data\" + 0.017*\"class\" + 0.015*\"value\" + 0.013*\"dataframe\" + 0.011*\"function\" + 0.010*\"create\" + 0.009*\"time\" + 0.009*\"output\" + 0.009*\"plot\" + 0.009*\"object\"'),\n",
       " (9,\n",
       "  '0.021*\"data\" + 0.009*\"use\" + 0.008*\"hyperplane\" + 0.007*\"information\" + 0.006*\"need\" + 0.006*\"key\" + 0.005*\"margin\" + 0.005*\"want\" + 0.005*\"know\" + 0.005*\"maximal_margin\"'),\n",
       " (10,\n",
       "  '0.034*\"test\" + 0.022*\"function\" + 0.019*\"error\" + 0.016*\"code\" + 0.012*\"def\" + 0.011*\"package\" + 0.009*\"case\" + 0.008*\"write\" + 0.008*\"argument\" + 0.008*\"testing\"'),\n",
       " (11,\n",
       "  '0.029*\"prior\" + 0.023*\"model\" + 0.021*\"posterior\" + 0.020*\"distribution\" + 0.014*\"sample\" + 0.014*\"data\" + 0.013*\"likelihood\" + 0.012*\"probability\" + 0.010*\"normal\" + 0.009*\"mean\"'),\n",
       " (12,\n",
       "  '0.078*\"database\" + 0.043*\"user\" + 0.031*\"privilege\" + 0.029*\"update\" + 0.024*\"view\" + 0.021*\"delete\" + 0.021*\"table\" + 0.019*\"create\" + 0.019*\"query\" + 0.017*\"select\"'),\n",
       " (13,\n",
       "  '0.032*\"data\" + 0.008*\"use\" + 0.008*\"system\" + 0.008*\"create\" + 0.006*\"change\" + 0.006*\"distribute\" + 0.006*\"program\" + 0.006*\"software\" + 0.006*\"branch\" + 0.005*\"code\"'),\n",
       " (14,\n",
       "  '0.027*\"data\" + 0.018*\"privacy\" + 0.015*\"record\" + 0.012*\"information\" + 0.011*\"module\" + 0.009*\"individual\" + 0.007*\"group\" + 0.007*\"attribute\" + 0.007*\"research\" + 0.006*\"collection\"'),\n",
       " (15,\n",
       "  '0.023*\"cell\" + 0.018*\"value\" + 0.016*\"number\" + 0.011*\"formula\" + 0.011*\"data\" + 0.011*\"function\" + 0.009*\"format\" + 0.009*\"time_step\" + 0.008*\"select\" + 0.007*\"column\"'),\n",
       " (16,\n",
       "  '0.042*\"data\" + 0.029*\"value\" + 0.017*\"dataframe\" + 0.017*\"column\" + 0.015*\"output\" + 0.014*\"array\" + 0.012*\"index\" + 0.010*\"missing_values\" + 0.009*\"condition\" + 0.009*\"function\"'),\n",
       " (17,\n",
       "  '0.037*\"data\" + 0.016*\"index\" + 0.016*\"column\" + 0.012*\"dataframe\" + 0.011*\"use\" + 0.011*\"year\" + 0.011*\"default\" + 0.011*\"chart\" + 0.011*\"revenue\" + 0.010*\"nan_nan\"'),\n",
       " (18,\n",
       "  '0.014*\"factor\" + 0.012*\"set\" + 0.011*\"effect\" + 0.010*\"design\" + 0.010*\"block\" + 0.010*\"experiment\" + 0.007*\"pc\" + 0.007*\"response\" + 0.007*\"treatment\" + 0.007*\"level\"'),\n",
       " (19,\n",
       "  '0.051*\"layer\" + 0.050*\"input\" + 0.040*\"network\" + 0.028*\"output\" + 0.024*\"loss\" + 0.022*\"neural_networks\" + 0.021*\"learn\" + 0.020*\"task\" + 0.017*\"neural_network\" + 0.016*\"hidden_layer\"'),\n",
       " (20,\n",
       "  '0.026*\"point\" + 0.025*\"function\" + 0.016*\"let\" + 0.014*\"minimizer\" + 0.014*\"find\" + 0.013*\"convex\" + 0.011*\"time\" + 0.011*\"set\" + 0.011*\"give\" + 0.010*\"suppose\"'),\n",
       " (21,\n",
       "  '0.047*\"mixture_models\" + 0.040*\"semi_supervised\" + 0.033*\"data\" + 0.030*\"group\" + 0.026*\"modelling\" + 0.022*\"supervise\" + 0.021*\"estimate\" + 0.017*\"mean\" + 0.015*\"model\" + 0.012*\"unsupervise\"'),\n",
       " (22,\n",
       "  '0.031*\"table\" + 0.018*\"attribute\" + 0.018*\"query\" + 0.017*\"value\" + 0.017*\"select\" + 0.017*\"column\" + 0.015*\"relationship\" + 0.014*\"relation\" + 0.013*\"employee\" + 0.012*\"row\"'),\n",
       " (23,\n",
       "  '0.045*\"sample\" + 0.021*\"population\" + 0.012*\"unit\" + 0.010*\"estimate\" + 0.009*\"weight\" + 0.009*\"probability\" + 0.009*\"value\" + 0.008*\"error\" + 0.007*\"stratum\" + 0.007*\"variance\"'),\n",
       " (24,\n",
       "  '0.019*\"data_carts\" + 0.014*\"add\" + 0.013*\"markdown\" + 0.012*\"tag\" + 0.011*\"motivate\" + 0.011*\"create\" + 0.009*\"need\" + 0.009*\"use\" + 0.009*\"word\" + 0.009*\"item\"')]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_lda_model = LdaModel(doc_term_matrix, num_topics = NUM_TOPICS, id2word = dictionary, alpha = 'auto', eta = 'auto', passes = 50)\n",
    "\n",
    "LDAvis_prepared = gensim_vis.prepare(final_lda_model, doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, f'Lectures_Final_{NUM_TOPICS}.html')\n",
    "\n",
    "final_lda_model.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing a topic model where the document term matrix is TF-IDF weighted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "# Duplicating these to avoid modifying the originals\n",
    "tf_corpus = doc_term_matrix\n",
    "tf_dictionary = dictionary\n",
    "\n",
    "tfidf = TfidfModel(corpus=tf_corpus, id2word=tf_dictionary)\n",
    "\n",
    "low_value = 0.03\n",
    "words  = []\n",
    "words_missing_in_tfidf = []\n",
    "for i in range(0, len(tf_corpus)):\n",
    "    bow = tf_corpus[i]\n",
    "    low_value_words = [] #reinitialize to be safe. You can skip this.\n",
    "    tfidf_ids = [id for id, value in tfidf[bow]]\n",
    "    bow_ids = [id for id, value in bow]\n",
    "    low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "    drops = low_value_words+words_missing_in_tfidf\n",
    "    for item in drops:\n",
    "        words.append(tf_dictionary[item])\n",
    "    words_missing_in_tfidf = [id for id in bow_ids if id not in tfidf_ids] # The words with tf-idf score 0 will be missing\n",
    "\n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words and b[0] not in words_missing_in_tfidf]\n",
    "    tf_corpus[i] = new_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.101*\"prior\" + 0.071*\"posterior\" + 0.046*\"likelihood\" + 0.045*\"introduction\" + 0.038*\"normal\" + 0.032*\"probability\" + 0.028*\"beta\" + 0.024*\"chain\" + 0.024*\"bayesian\" + 0.022*\"regression\"'),\n",
       " (1,\n",
       "  '0.036*\"def\" + 0.032*\"package\" + 0.030*\"git\" + 0.029*\"python\" + 0.025*\"branch\" + 0.022*\"exception\" + 0.022*\"module\" + 0.019*\"testing\" + 0.017*\"repository\" + 0.017*\"object\"'),\n",
       " (2,\n",
       "  '0.026*\"command\" + 0.018*\"git\" + 0.017*\"group\" + 0.017*\"open\" + 0.017*\"line\" + 0.016*\"distance\" + 0.014*\"cluster\" + 0.013*\"echo\" + 0.011*\"key\" + 0.011*\"mixture\"'),\n",
       " (3,\n",
       "  '0.024*\"emp\" + 0.023*\"random\" + 0.023*\"database\" + 0.018*\"query\" + 0.017*\"key\" + 0.016*\"eno\" + 0.016*\"sql\" + 0.015*\"probability\" + 0.013*\"density\" + 0.013*\"dno\"'),\n",
       " (4,\n",
       "  '0.030*\"distribution\" + 0.022*\"return\" + 0.019*\"probability\" + 0.016*\"markov\" + 0.016*\"likelihood\" + 0.015*\"list\" + 0.015*\"string\" + 0.014*\"state\" + 0.014*\"chain\" + 0.013*\"regression\"'),\n",
       " (5,\n",
       "  '0.032*\"linear\" + 0.024*\"regression\" + 0.023*\"cell\" + 0.021*\"mar\" + 0.017*\"linear_regression\" + 0.016*\"macro\" + 0.015*\"excel\" + 0.015*\"food\" + 0.014*\"toy\" + 0.013*\"predictor\"'),\n",
       " (6,\n",
       "  '0.071*\"sample\" + 0.040*\"treatment\" + 0.035*\"population\" + 0.027*\"unit\" + 0.025*\"factor\" + 0.025*\"design\" + 0.020*\"block\" + 0.018*\"estimate\" + 0.018*\"sampling\" + 0.018*\"effect\"'),\n",
       " (7,\n",
       "  '0.000*\"layer\" + 0.000*\"sample\" + 0.000*\"column\" + 0.000*\"network\" + 0.000*\"array\" + 0.000*\"nan\" + 0.000*\"dataframe\" + 0.000*\"neural\" + 0.000*\"list\" + 0.000*\"treatment\"'),\n",
       " (8,\n",
       "  '0.032*\"column\" + 0.029*\"dataframe\" + 0.027*\"nan\" + 0.022*\"index\" + 0.021*\"app\" + 0.019*\"dash\" + 0.019*\"array\" + 0.016*\"cont\" + 0.015*\"col\" + 0.015*\"panda\"'),\n",
       " (9,\n",
       "  '0.025*\"tree\" + 0.022*\"layer\" + 0.021*\"network\" + 0.018*\"observation\" + 0.015*\"estimate\" + 0.015*\"classification\" + 0.015*\"neural\" + 0.014*\"training\" + 0.013*\"regression\" + 0.013*\"vector\"')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_lda_model = LdaModel(corpus=tf_corpus, id2word=tf_dictionary, num_topics=10, random_state=448, passes=20, alpha=\"auto\", eta = \"auto\")\n",
    "idf_lda_model.show_topics(num_topics = -1, num_words = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAvis_prepared = gensim_vis.prepare(idf_lda_model, doc_term_matrix, dictionary, mds='pcoa')\n",
    "pyLDAvis.display(LDAvis_prepared)\n",
    "pyLDAvis.save_html(LDAvis_prepared, 'IDF_auto_run_LDA_10.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting but removes too many words, which means it destroys the metric of importance of each topic (calculated by % of tokens that belong to each topic), which is the main reason that we care about using LDA for this. \n",
    "\n",
    "Could also try other ways of doing this with: Dictionary.filter_extremes \n",
    "\n",
    "Should probably retry these two with corpus consisting of documents that are individual pdfs/lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, we go with the 18-topic model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some interesting words that we could look for in the jobs dataset dictionary could be: git, training, simulate, probability, regression, database, experiment, excel, time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding word frequencies for words from jobs dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "\n",
    "for item in dictionary.items():\n",
    "    vocab.append(item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abbreviation', 'add', 'address', 'advanced', 'allow', 'analog', 'analysis', 'analyst', 'analytic', 'app']\n",
      "9922\n"
     ]
    }
   ],
   "source": [
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deploy: []\n",
      "pipeline: []\n",
      "etl: not found\n",
      "llm: not found\n",
      "power_bi: not found\n",
      "generative_ai: not found\n",
      "gcp: not found\n",
      "spark: [(17, 0.010765541)]\n",
      "hadoop: [(17, 0.0035910984)]\n",
      "git: [(2, 0.0033630973)]\n",
      "training: [(4, 0.008145138)]\n",
      "simulate: [(1, 0.0097651975), (5, 0.013888511), (16, 0.002324933)]\n",
      "probability: [(1, 0.011176379), (4, 0.0027122817), (5, 0.027418612), (10, 0.002061787), (13, 0.021222476), (14, 0.01113725)]\n",
      "regression: [(1, 0.0037703142), (4, 0.0029165489), (7, 0.0029271643), (16, 0.02352627)]\n",
      "database: [(12, 0.028981334), (15, 0.001560567), (17, 0.0012988469)]\n",
      "experiment: [(10, 0.014843564), (13, 0.0021932065)]\n",
      "excel: [(7, 0.0020061985), (8, 0.0017035467)]\n",
      "time_series: [(1, 0.0034515331), (14, 0.0011038793)]\n",
      "deep_learning: [(4, 0.0013561486)]\n",
      "visualization: [(15, 0.0011943558)]\n",
      "data: [(0, 0.0070041586), (1, 0.028142318), (2, 0.0014796851), (3, 0.045594253), (4, 0.018646404), (5, 0.01824003), (6, 0.002973792), (7, 0.033636827), (8, 0.023922192), (9, 0.0048884694), (10, 0.0012756683), (11, 0.016456641), (12, 0.016747829), (13, 0.021416204), (14, 0.0038854808), (15, 0.029397713), (16, 0.040757045), (17, 0.020150548)]\n",
      "markov_chain: [(5, 0.028303757), (10, 0.0027419485), (16, 0.001478377)]\n",
      "neural_network: [(4, 0.0018532253)]\n",
      "pytorch: []\n"
     ]
    }
   ],
   "source": [
    "words_to_find = [\"deploy\", \"pipeline\", \"etl\", \"llm\", \"power_bi\", \"generative_ai\", \"gcp\", \"spark\", \"hadoop\", \"git\", \"training\", \"simulate\", \"probability\", \"regression\", \"database\", \"experiment\", \"excel\", \"time_series\", \"deep_learning\", \"visualization\", \"data\", \"markov_chain\", \"neural_network\", \"pytorch\", \"tensorflow\", \"kafka\", \"nlp\"]\n",
    "for word in words_to_find:\n",
    "    print(word, end=\": \")\n",
    "    if word in vocab:\n",
    "        print(final_lda_model.get_term_topics(word, minimum_probability=0.001))\n",
    "    else:\n",
    "        print(\"not found\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The below section is experimental:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: add an explanation of what an embedding is, how they are learned, sentence vs word level embeddings (and the fact that we use word level). Also describe each approach, what worked and what didn't.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to assign a label to a topic using word embeddings of the top 20 words in a topic sorted by relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# Top 20 words for mmds visualization of LDA model with 5 topics\n",
    "top_words = [\"model\", \"data\", \"value\", \"function\", \"distribution\", \"example\", \"probability\", \"number\", \"using\", \"use\", \"simulate\", \"sample\", \"independent\", \"average\", \"mean\", \"figure\", \"estimate\", \"variable\", \"measurement\", \"plot\"]\n",
    "print(len(top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\syeda/gensim-data\\fasttext-wiki-news-subwords-300\\fasttext-wiki-news-subwords-300.gz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import gensim.downloader as api\n",
    "# model_location = api.load(\"fasttext-wiki-news-subwords-300\", return_path=True)\n",
    "# print(model_location)\n",
    "# Stored at C:\\Users\\syeda/gensim-data\\fasttext-wiki-news-subwords-300\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import load_facebook_model\n",
    "\n",
    "model_location = datapath(\"C:/Users/syeda/OneDrive/Desktop/4th Year/DATA448/cc.en.300.bin\")\n",
    "pretrained_model = load_facebook_model(model_location)\n",
    "finetuned_model = load_facebook_model(model_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('calculate', 0.6181637048721313), ('use', 0.6073808670043945), ('extrapolate', 0.5911571383476257), ('calculation', 0.5882555842399597), ('estimate', 0.5864962935447693)]\n",
      "Representative word for the topic: calculate\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "word_embeddings = [pretrained_model.wv[word] for word in top_words]\n",
    "mean_vector = np.mean(word_embeddings, axis=0)\n",
    "\n",
    "pt_similar_words = pretrained_model.wv.similar_by_vector(mean_vector, topn=5)\n",
    "print(pt_similar_words)\n",
    "\n",
    "topic_label = pt_similar_words[0][0]\n",
    "print(f\"Representative word for the topic: {topic_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57851, 291540)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finetuned_model.build_vocab(corpus_with_bigrams_trigrams, update=True)  # Add the new words to the vocabulary\n",
    "finetuned_model.train(corpus_with_bigrams_trigrams, total_examples=len(corpus_with_bigrams_trigrams), epochs=10)  # Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('variation', 0.9998103976249695), ('calculation', 0.9998043179512024), ('estimation', 0.9997916221618652), ('computer-simulation', 0.9997856020927429), ('correlation', 0.9997814893722534)]\n",
      "Representative word for the topic: variation\n"
     ]
    }
   ],
   "source": [
    "# Now you can use the updated model with embeddings that include domain-specific words\n",
    "ft_word_embeddings = [finetuned_model.wv[word] for word in top_words]\n",
    "ft_mean_vector = np.mean(ft_word_embeddings, axis=0)\n",
    "\n",
    "ft_similar_words = finetuned_model.wv.similar_by_vector(ft_mean_vector, topn=5)\n",
    "print(ft_similar_words)\n",
    "\n",
    "ft_topic_label = ft_similar_words[0][0]\n",
    "print(f\"Representative word for the topic: {ft_topic_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(mean_vector, ft_mean_vector, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "custom_model = FastText(vector_size=100, window=3, min_count=1, sentences=corpus_with_bigrams_trigrams, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('distancetraveled', 0.999996542930603), ('projected', 0.9999964833259583), ('example_consider', 0.9999963641166687), ('thersystemanintroductionandoverview', 0.9999961853027344), ('mentioned', 0.9999961256980896)]\n",
      "Representative word for the topic: distancetraveled\n"
     ]
    }
   ],
   "source": [
    "custom_embeddings = [custom_model.wv[word] for word in top_words]\n",
    "custom_mean_vector = np.mean(custom_embeddings, axis=0)\n",
    "\n",
    "similar_words = custom_model.wv.similar_by_vector(custom_mean_vector, topn=5)\n",
    "print(similar_words)\n",
    "\n",
    "custom_topic_label = similar_words[0][0]\n",
    "print(f\"Representative word for the topic: {custom_topic_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to assign a label to a topic using a pre-trained transformer by encoding the top 20 words in a topic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuned T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"Michau/t5-base-en-generate-headline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "# Top 20 words for mmds visualization of LDA model with 5 topics\n",
    "top_words = [\"value\",\"function\",\"datum\",\"random\",\"use\",\"model\",\"variable\", \"time\",\"figure\",\"example\",\"number\",\"plot\",\"estimate\",\"random_variable\",\"lag\",\"histogram\",\"probability\",\"series\",\"mean\",\"simulate\",\"standard\",\"sample\",\"r\",\"regression\",\"follow\",\"level\",\"distribution\",\"variance\",\"x\",\"pseudorandom_number\"]\n",
    "print(len(top_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate a one-word topic label from a list of words\n",
    "def generate_topic_label(top_words: list) -> str:\n",
    "    \n",
    "    input_string = \"label these topics: \" + \" \".join(top_words)\n",
    "    print(input_string)\n",
    "    \n",
    "    # Tokenize the input string\n",
    "    encoding = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate the label using the model\n",
    "    output = model.generate(encoding, max_length=5, num_beams=4, early_stopping=True)\n",
    "    \n",
    "    # Decode the output to get the label\n",
    "    label = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label these topics: value function datum random use model variable time figure example number plot estimate random_variable lag histogram probability series mean simulate standard sample r regression follow level distribution variance x pseudorandom_number\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages\\transformers\\generation\\utils.py:1244: UserWarning: Unfeasible length constraints: `min_length` (12) is larger than the maximum possible length (5). Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated topic label: \n"
     ]
    }
   ],
   "source": [
    "topic_label = generate_topic_label(top_words)\n",
    "print(f\"Generated topic label: {topic_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuned BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "mname = \"cristian-popa/bart-tl-all\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(mname)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(mname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topic_label_with_BART(top_words: list[str]) -> str:\n",
    "    enc = tokenizer(top_words, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "    outputs = model.generate(\n",
    "        input_ids=enc.input_ids,\n",
    "        attention_mask=enc.attention_mask,\n",
    "        max_length=15,\n",
    "        min_length=1,\n",
    "        do_sample=False,\n",
    "        num_beams=25,\n",
    "        length_penalty=1.0,\n",
    "        repetition_penalty=1.5\n",
    "    )\n",
    "\n",
    "    label = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 23 13:29:08 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 552.44                 Driver Version: 552.44         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3060 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   66C    P8             11W /   95W |      73MiB /   6144MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      3512    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "|    0   N/A  N/A     10876    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     18776    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     25660    C+G   ...on\\129.0.2792.89\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     27128      C   ...ta\\Local\\Programs\\Ollama\\ollama.exe      N/A      |\n",
      "|    0   N/A  N/A     29020    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated topic label: rate of return\n"
     ]
    }
   ],
   "source": [
    "topic_label = generate_topic_label_with_BART(top_words)\n",
    "print(f\"Generated topic label: {topic_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying BERTopic to get topic info for the entire corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tensorflow<2.18,>=2.17 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from tf-keras) (2.17.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from tensorflow<2.18,>=2.17->tf-keras) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.66.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.5.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (13.9.1)\n",
      "Requirement already satisfied: namex in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow<2.18,>=2.17->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.17.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 0.5/1.7 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 4.1 MB/s eta 0:00:00\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.17.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install BERTopic\n",
    "# !pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from umap import UMAP\n",
    "\n",
    "# Assuming `corpus` is a list of lists of strings\n",
    "# Convert the list of lists into a list of strings (documents)\n",
    "split_corpus = []\n",
    "\n",
    "# Split each string into 5 parts\n",
    "for string in corpus:\n",
    "    # Calculate the length of each part\n",
    "    part_length = max(1, len(string) // 5)  # Ensure at least one character per part\n",
    "    parts = [string[i:i + part_length] for i in range(0, len(string), part_length)]\n",
    "    \n",
    "    # If there are more than 5 parts, combine excess parts\n",
    "    while len(parts) > 5:\n",
    "        last_part = parts.pop()\n",
    "        parts[-1] += last_part  # Combine excess into the last part\n",
    "    \n",
    "    # Add the parts to the split_corpus\n",
    "    split_corpus.extend(parts)\n",
    "\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 3), stop_words='english')\n",
    "\n",
    "# Initialize BERTopic model\n",
    "topic_model = BERTopic(vectorizer_model=vectorizer_model)\n",
    "\n",
    "\n",
    "# Fit the BERTopic model on your corpus and extract topics\n",
    "topics, probabilities = topic_model.fit_transform(split_corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Topic  Count                                  Name  \\\n",
      "0     -1     40  -1_function_random_data_distribution   \n",
      "\n",
      "                                      Representation  \\\n",
      "0  [function, random, data, distribution, example...   \n",
      "\n",
      "                                 Representative_Docs  \n",
      "0  [ying this in the inverse CDF method runs as f...  \n"
     ]
    }
   ],
   "source": [
    "print(topic_model.get_topic_info())\n",
    "# topic_model.visualize_topics() does not work because only one topic lol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Less than ideal results: \n",
    "\n",
    "- BERTopic does not work out of the box with a corpus of 8 documents (in this case, each chapter is one document as a string so the corpus is a list of 8 strings), so we need to split the 8 documents into 40 documents (by evenly splitting each doc into 5 docs).\n",
    "- The output is only one \"topic\" with index -1. According to BERTopic documentation, topic ID -1 is for documents that \"do not fit into any topics\". All of our documents are assigned to this topic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trying BERTopic on complete modules corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\syeda\\miniconda3\\envs\\dir-st\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "topic_model = BERTopic()        # Default arguments as used on the website: https://maartengr.github.io/BERTopic/getting_started/quickstart/quickstart.html\n",
    "topics, probabilities = topic_model.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>18</td>\n",
       "      <td>-1_the_of_to_is</td>\n",
       "      <td>[the, of, to, is, and, in, data, for, we, that]</td>\n",
       "      <td>[         DATA 580\\n\\nModeling and Simulation ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count             Name  \\\n",
       "0     -1     18  -1_the_of_to_is   \n",
       "\n",
       "                                    Representation  \\\n",
       "0  [the, of, to, is, and, in, data, for, we, that]   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [         DATA 580\\n\\nModeling and Simulation ...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "      <th>Top_n_words</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Representative_document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UC\\nDa...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_the_of_to_is</td>\n",
       "      <td>[the, of, to, is, and, in, data, for, we, that]</td>\n",
       "      <td>[            DATA 581\\n\\nModeling and Simulati...</td>\n",
       "      <td>the - of - to - is - and - in - data - for - w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lecture 7: Functional-style programming and\\nH...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_the_of_to_is</td>\n",
       "      <td>[the, of, to, is, and, in, data, for, we, that]</td>\n",
       "      <td>[            DATA 581\\n\\nModeling and Simulati...</td>\n",
       "      <td>the - of - to - is - and - in - data - for - w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Structures and\\nAlgorithms\\n\\nUBCO Master...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_the_of_to_is</td>\n",
       "      <td>[the, of, to, is, and, in, data, for, we, that]</td>\n",
       "      <td>[            DATA 581\\n\\nModeling and Simulati...</td>\n",
       "      <td>the - of - to - is - and - in - data - for - w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UC\\nPy...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_the_of_to_is</td>\n",
       "      <td>[the, of, to, is, and, in, data, for, we, that]</td>\n",
       "      <td>[            DATA 581\\n\\nModeling and Simulati...</td>\n",
       "      <td>the - of - to - is - and - in - data - for - w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UC\\nSQ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_the_of_to_is</td>\n",
       "      <td>[the, of, to, is, and, in, data, for, we, that]</td>\n",
       "      <td>[            DATA 581\\n\\nModeling and Simulati...</td>\n",
       "      <td>the - of - to - is - and - in - data - for - w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Version Control\\n\\nUBCO Master of Data Science...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_the_of_to_is</td>\n",
       "      <td>[the, of, to, is, and, in, data, for, we, that]</td>\n",
       "      <td>[            DATA 581\\n\\nModeling and Simulati...</td>\n",
       "      <td>the - of - to - is - and - in - data - for - w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Profiling and\\nCleaning\\nHandling Missing...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_the_of_to_is</td>\n",
       "      <td>[the, of, to, is, and, in, data, for, we, that]</td>\n",
       "      <td>[            DATA 581\\n\\nModeling and Simulati...</td>\n",
       "      <td>the - of - to - is - and - in - data - for - w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Completely Randomized Designs (CRD)\\n         ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_the_of_to_is</td>\n",
       "      <td>[the, of, to, is, and, in, data, for, we, that]</td>\n",
       "      <td>[            DATA 581\\n\\nModeling and Simulati...</td>\n",
       "      <td>the - of - to - is - and - in - data - for - w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>551 Lec 5 - Tables, styling, performance\\nYou ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_the_of_to_is</td>\n",
       "      <td>[the, of, to, is, and, in, data, for, we, that]</td>\n",
       "      <td>[            DATA 581\\n\\nModeling and Simulati...</td>\n",
       "      <td>the - of - to - is - and - in - data - for - w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Moving beyond linearity in response\\n\\n       ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_the_of_to_is</td>\n",
       "      <td>[the, of, to, is, and, in, data, for, we, that]</td>\n",
       "      <td>[            DATA 581\\n\\nModeling and Simulati...</td>\n",
       "      <td>the - of - to - is - and - in - data - for - w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Classification and Regression Trees\\n\\n       ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_the_of_to_is</td>\n",
       "      <td>[the, of, to, is, and, in, data, for, we, that]</td>\n",
       "      <td>[            DATA 581\\n\\nModeling and Simulati...</td>\n",
       "      <td>the - of - to - is - and - in - data - for - w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>DATA 572: Supervised Learning\\n\\n             ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_the_of_to_is</td>\n",
       "      <td>[the, of, to, is, and, in, data, for, we, that]</td>\n",
       "      <td>[            DATA 581\\n\\nModeling and Simulati...</td>\n",
       "      <td>the - of - to - is - and - in - data - for - w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Non-negative Matrix Factorization\\n\\n         ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_the_of_to_is</td>\n",
       "      <td>[the, of, to, is, and, in, data, for, we, that]</td>\n",
       "      <td>[            DATA 581\\n\\nModeling and Simulati...</td>\n",
       "      <td>the - of - to - is - and - in - data - for - w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>DATA 580\\n\\nModeling and Simulation I...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_the_of_to_is</td>\n",
       "      <td>[the, of, to, is, and, in, data, for, we, that]</td>\n",
       "      <td>[            DATA 581\\n\\nModeling and Simulati...</td>\n",
       "      <td>the - of - to - is - and - in - data - for - w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>DATA 581\\n\\nModeling and Simulatio...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_the_of_to_is</td>\n",
       "      <td>[the, of, to, is, and, in, data, for, we, that]</td>\n",
       "      <td>[            DATA 581\\n\\nModeling and Simulati...</td>\n",
       "      <td>the - of - to - is - and - in - data - for - w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>DATA 582: Bayesian Inference\\n    Lecture 3: B...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_the_of_to_is</td>\n",
       "      <td>[the, of, to, is, and, in, data, for, we, that]</td>\n",
       "      <td>[            DATA 581\\n\\nModeling and Simulati...</td>\n",
       "      <td>the - of - to - is - and - in - data - for - w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Lecture 7\\n     Generalized Additive M...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_the_of_to_is</td>\n",
       "      <td>[the, of, to, is, and, in, data, for, we, that]</td>\n",
       "      <td>[            DATA 581\\n\\nModeling and Simulati...</td>\n",
       "      <td>the - of - to - is - and - in - data - for - w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>DATA 586: Advanced Machine\\n             Learn...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_the_of_to_is</td>\n",
       "      <td>[the, of, to, is, and, in, data, for, we, that]</td>\n",
       "      <td>[            DATA 581\\n\\nModeling and Simulati...</td>\n",
       "      <td>the - of - to - is - and - in - data - for - w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Document  Topic             Name  \\\n",
       "0                                           UC\\nDa...     -1  -1_the_of_to_is   \n",
       "1   Lecture 7: Functional-style programming and\\nH...     -1  -1_the_of_to_is   \n",
       "2   Data Structures and\\nAlgorithms\\n\\nUBCO Master...     -1  -1_the_of_to_is   \n",
       "3                                           UC\\nPy...     -1  -1_the_of_to_is   \n",
       "4                                           UC\\nSQ...     -1  -1_the_of_to_is   \n",
       "5   Version Control\\n\\nUBCO Master of Data Science...     -1  -1_the_of_to_is   \n",
       "6   Data Profiling and\\nCleaning\\nHandling Missing...     -1  -1_the_of_to_is   \n",
       "7   Completely Randomized Designs (CRD)\\n         ...     -1  -1_the_of_to_is   \n",
       "8   551 Lec 5 - Tables, styling, performance\\nYou ...     -1  -1_the_of_to_is   \n",
       "9   Moving beyond linearity in response\\n\\n       ...     -1  -1_the_of_to_is   \n",
       "10  Classification and Regression Trees\\n\\n       ...     -1  -1_the_of_to_is   \n",
       "11  DATA 572: Supervised Learning\\n\\n             ...     -1  -1_the_of_to_is   \n",
       "12  Non-negative Matrix Factorization\\n\\n         ...     -1  -1_the_of_to_is   \n",
       "13           DATA 580\\n\\nModeling and Simulation I...     -1  -1_the_of_to_is   \n",
       "14              DATA 581\\n\\nModeling and Simulatio...     -1  -1_the_of_to_is   \n",
       "15  DATA 582: Bayesian Inference\\n    Lecture 3: B...     -1  -1_the_of_to_is   \n",
       "16          Lecture 7\\n     Generalized Additive M...     -1  -1_the_of_to_is   \n",
       "17  DATA 586: Advanced Machine\\n             Learn...     -1  -1_the_of_to_is   \n",
       "\n",
       "                                     Representation  \\\n",
       "0   [the, of, to, is, and, in, data, for, we, that]   \n",
       "1   [the, of, to, is, and, in, data, for, we, that]   \n",
       "2   [the, of, to, is, and, in, data, for, we, that]   \n",
       "3   [the, of, to, is, and, in, data, for, we, that]   \n",
       "4   [the, of, to, is, and, in, data, for, we, that]   \n",
       "5   [the, of, to, is, and, in, data, for, we, that]   \n",
       "6   [the, of, to, is, and, in, data, for, we, that]   \n",
       "7   [the, of, to, is, and, in, data, for, we, that]   \n",
       "8   [the, of, to, is, and, in, data, for, we, that]   \n",
       "9   [the, of, to, is, and, in, data, for, we, that]   \n",
       "10  [the, of, to, is, and, in, data, for, we, that]   \n",
       "11  [the, of, to, is, and, in, data, for, we, that]   \n",
       "12  [the, of, to, is, and, in, data, for, we, that]   \n",
       "13  [the, of, to, is, and, in, data, for, we, that]   \n",
       "14  [the, of, to, is, and, in, data, for, we, that]   \n",
       "15  [the, of, to, is, and, in, data, for, we, that]   \n",
       "16  [the, of, to, is, and, in, data, for, we, that]   \n",
       "17  [the, of, to, is, and, in, data, for, we, that]   \n",
       "\n",
       "                                  Representative_Docs  \\\n",
       "0   [            DATA 581\\n\\nModeling and Simulati...   \n",
       "1   [            DATA 581\\n\\nModeling and Simulati...   \n",
       "2   [            DATA 581\\n\\nModeling and Simulati...   \n",
       "3   [            DATA 581\\n\\nModeling and Simulati...   \n",
       "4   [            DATA 581\\n\\nModeling and Simulati...   \n",
       "5   [            DATA 581\\n\\nModeling and Simulati...   \n",
       "6   [            DATA 581\\n\\nModeling and Simulati...   \n",
       "7   [            DATA 581\\n\\nModeling and Simulati...   \n",
       "8   [            DATA 581\\n\\nModeling and Simulati...   \n",
       "9   [            DATA 581\\n\\nModeling and Simulati...   \n",
       "10  [            DATA 581\\n\\nModeling and Simulati...   \n",
       "11  [            DATA 581\\n\\nModeling and Simulati...   \n",
       "12  [            DATA 581\\n\\nModeling and Simulati...   \n",
       "13  [            DATA 581\\n\\nModeling and Simulati...   \n",
       "14  [            DATA 581\\n\\nModeling and Simulati...   \n",
       "15  [            DATA 581\\n\\nModeling and Simulati...   \n",
       "16  [            DATA 581\\n\\nModeling and Simulati...   \n",
       "17  [            DATA 581\\n\\nModeling and Simulati...   \n",
       "\n",
       "                                          Top_n_words  Probability  \\\n",
       "0   the - of - to - is - and - in - data - for - w...          0.0   \n",
       "1   the - of - to - is - and - in - data - for - w...          0.0   \n",
       "2   the - of - to - is - and - in - data - for - w...          0.0   \n",
       "3   the - of - to - is - and - in - data - for - w...          0.0   \n",
       "4   the - of - to - is - and - in - data - for - w...          0.0   \n",
       "5   the - of - to - is - and - in - data - for - w...          0.0   \n",
       "6   the - of - to - is - and - in - data - for - w...          0.0   \n",
       "7   the - of - to - is - and - in - data - for - w...          0.0   \n",
       "8   the - of - to - is - and - in - data - for - w...          0.0   \n",
       "9   the - of - to - is - and - in - data - for - w...          0.0   \n",
       "10  the - of - to - is - and - in - data - for - w...          0.0   \n",
       "11  the - of - to - is - and - in - data - for - w...          0.0   \n",
       "12  the - of - to - is - and - in - data - for - w...          0.0   \n",
       "13  the - of - to - is - and - in - data - for - w...          0.0   \n",
       "14  the - of - to - is - and - in - data - for - w...          0.0   \n",
       "15  the - of - to - is - and - in - data - for - w...          0.0   \n",
       "16  the - of - to - is - and - in - data - for - w...          0.0   \n",
       "17  the - of - to - is - and - in - data - for - w...          0.0   \n",
       "\n",
       "    Representative_document  \n",
       "0                     False  \n",
       "1                     False  \n",
       "2                     False  \n",
       "3                     False  \n",
       "4                     False  \n",
       "5                     False  \n",
       "6                     False  \n",
       "7                     False  \n",
       "8                     False  \n",
       "9                     False  \n",
       "10                    False  \n",
       "11                     True  \n",
       "12                    False  \n",
       "13                    False  \n",
       "14                     True  \n",
       "15                    False  \n",
       "16                    False  \n",
       "17                     True  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_document_info(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still terrible results, not sure what I'm doing wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking if it works in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18846\n",
      "<class 'list'>\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about \n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))\n",
    "print(type(docs))\n",
    "print(docs[0][:100])\n",
    "print(type(docs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712\n"
     ]
    }
   ],
   "source": [
    "print(len(docs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, probs = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>6596</td>\n",
       "      <td>-1_to_the_of_and</td>\n",
       "      <td>[to, the, of, and, is, for, in, you, it, that]</td>\n",
       "      <td>[\\nProbably because it IS rape.\\n\\n\\nSo nothin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1832</td>\n",
       "      <td>0_game_team_games_he</td>\n",
       "      <td>[game, team, games, he, players, season, hocke...</td>\n",
       "      <td>[\\nWales Conference, Adams Division, Semifinal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>616</td>\n",
       "      <td>1_key_clipper_chip_encryption</td>\n",
       "      <td>[key, clipper, chip, encryption, keys, escrow,...</td>\n",
       "      <td>[The following document summarizes the Clipper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>464</td>\n",
       "      <td>2_israel_israeli_jews_arab</td>\n",
       "      <td>[israel, israeli, jews, arab, jewish, arabs, p...</td>\n",
       "      <td>[\\n\\n\"Assuming\"? Also: come on, Brad. If we ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>451</td>\n",
       "      <td>3_ites_cheek_yep_huh</td>\n",
       "      <td>[ites, cheek, yep, huh, ken, , , , , ]</td>\n",
       "      <td>[Ken\\n, \\nYep.\\n, ites:]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>210</td>\n",
       "      <td>10</td>\n",
       "      <td>210_oil_lights_indicators_service</td>\n",
       "      <td>[oil, lights, indicators, service, reset, indi...</td>\n",
       "      <td>[Derek....\\n\\nThere is a tool available to res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>211</td>\n",
       "      <td>10</td>\n",
       "      <td>211_needles_acupuncture_needle_syringe</td>\n",
       "      <td>[needles, acupuncture, needle, syringe, hypode...</td>\n",
       "      <td>[\\nIt is illegal to perform acupuncture with u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>212</td>\n",
       "      <td>10</td>\n",
       "      <td>212_alarm_sensor_alarms_shock</td>\n",
       "      <td>[alarm, sensor, alarms, shock, car, viper, alp...</td>\n",
       "      <td>[Just found a great deal on a Clifford Delta c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>213</td>\n",
       "      <td>10</td>\n",
       "      <td>213_religion_supreme_arf_definition</td>\n",
       "      <td>[religion, supreme, arf, definition, belief, l...</td>\n",
       "      <td>[\\n  .\\n           It's my understanding that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>214</td>\n",
       "      <td>10</td>\n",
       "      <td>214_pgp_keyrings_vectors_code</td>\n",
       "      <td>[pgp, keyrings, vectors, code, program, pgppat...</td>\n",
       "      <td>[: I am a new reader of sci.crypt I would like...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>216 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic  Count                                    Name  \\\n",
       "0       -1   6596                        -1_to_the_of_and   \n",
       "1        0   1832                    0_game_team_games_he   \n",
       "2        1    616           1_key_clipper_chip_encryption   \n",
       "3        2    464              2_israel_israeli_jews_arab   \n",
       "4        3    451                    3_ites_cheek_yep_huh   \n",
       "..     ...    ...                                     ...   \n",
       "211    210     10       210_oil_lights_indicators_service   \n",
       "212    211     10  211_needles_acupuncture_needle_syringe   \n",
       "213    212     10           212_alarm_sensor_alarms_shock   \n",
       "214    213     10     213_religion_supreme_arf_definition   \n",
       "215    214     10           214_pgp_keyrings_vectors_code   \n",
       "\n",
       "                                        Representation  \\\n",
       "0       [to, the, of, and, is, for, in, you, it, that]   \n",
       "1    [game, team, games, he, players, season, hocke...   \n",
       "2    [key, clipper, chip, encryption, keys, escrow,...   \n",
       "3    [israel, israeli, jews, arab, jewish, arabs, p...   \n",
       "4               [ites, cheek, yep, huh, ken, , , , , ]   \n",
       "..                                                 ...   \n",
       "211  [oil, lights, indicators, service, reset, indi...   \n",
       "212  [needles, acupuncture, needle, syringe, hypode...   \n",
       "213  [alarm, sensor, alarms, shock, car, viper, alp...   \n",
       "214  [religion, supreme, arf, definition, belief, l...   \n",
       "215  [pgp, keyrings, vectors, code, program, pgppat...   \n",
       "\n",
       "                                   Representative_Docs  \n",
       "0    [\\nProbably because it IS rape.\\n\\n\\nSo nothin...  \n",
       "1    [\\nWales Conference, Adams Division, Semifinal...  \n",
       "2    [The following document summarizes the Clipper...  \n",
       "3    [\\n\\n\"Assuming\"? Also: come on, Brad. If we ar...  \n",
       "4                             [Ken\\n, \\nYep.\\n, ites:]  \n",
       "..                                                 ...  \n",
       "211  [Derek....\\n\\nThere is a tool available to res...  \n",
       "212  [\\nIt is illegal to perform acupuncture with u...  \n",
       "213  [Just found a great deal on a Clifford Delta c...  \n",
       "214  [\\n  .\\n           It's my understanding that ...  \n",
       "215  [: I am a new reader of sci.crypt I would like...  \n",
       "\n",
       "[216 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic_info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dir-st",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
